{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lJjyibVan5fD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential\n",
        "from torch.nn import functional as F\n",
        "import logging\n",
        "import pandas as pd\n",
        "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import sdgym\n",
        "from sdv.metadata.single_table import SingleTableMetadata\n",
        "from sdgym.datasets import load_dataset\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ix_behhIn5fF"
      },
      "outputs": [],
      "source": [
        "LOGGER = logging.getLogger(__name__)\n",
        "class BaseSynthesizer:\n",
        "    \"\"\"Base class for all default synthesizers of ``SDGym``.\"\"\"\n",
        "\n",
        "    def fit(self, data, categorical_columns=tuple(), ordinal_columns=tuple()):\n",
        "        pass\n",
        "\n",
        "    def sample(self, samples):\n",
        "        pass\n",
        "\n",
        "    def fit_sample(self, data, categorical_columns=tuple(), ordinal_columns=tuple()):\n",
        "        LOGGER.info(\"Fitting %s\", self.__class__.__name__)\n",
        "        self.fit(data, categorical_columns, ordinal_columns)\n",
        "\n",
        "        LOGGER.info(\"Sampling %s\", self.__class__.__name__)\n",
        "        return self.sample(data.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GdMU1DE0n5fG"
      },
      "outputs": [],
      "source": [
        "CATEGORICAL = \"categorical\"\n",
        "CONTINUOUS = \"continuous\"\n",
        "ORDINAL = \"ordinal\"\n",
        "class Transformer:\n",
        "\n",
        "    @staticmethod\n",
        "    def get_metadata(data, categorical_columns=tuple(), ordinal_columns=tuple()):\n",
        "        meta = []\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        for index in df:\n",
        "            column = df[index]\n",
        "\n",
        "            if index in categorical_columns:\n",
        "                mapper = column.value_counts().index.tolist()\n",
        "                meta.append({\n",
        "                    \"name\": index,\n",
        "                    \"type\": CATEGORICAL,\n",
        "                    \"size\": len(mapper),\n",
        "                    \"i2s\": mapper\n",
        "                })\n",
        "            elif index in ordinal_columns:\n",
        "                value_count = list(dict(column.value_counts()).items())\n",
        "                value_count = sorted(value_count, key=lambda x: -x[1])\n",
        "                mapper = list(map(lambda x: x[0], value_count))\n",
        "                meta.append({\n",
        "                    \"name\": index,\n",
        "                    \"type\": ORDINAL,\n",
        "                    \"size\": len(mapper),\n",
        "                    \"i2s\": mapper\n",
        "                })\n",
        "            else:\n",
        "                meta.append({\n",
        "                    \"name\": index,\n",
        "                    \"type\": CONTINUOUS,\n",
        "                    \"min\": column.min(),\n",
        "                    \"max\": column.max(),\n",
        "                })\n",
        "\n",
        "        return meta\n",
        "\n",
        "    def fit(self, data, categorical_columns=tuple(), ordinal_columns=tuple()):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def transform(self, data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class DiscretizeTransformer(Transformer):\n",
        "    \"\"\"Discretize continuous columns into several bins.\n",
        "\n",
        "    Attributes:\n",
        "        meta\n",
        "        column_index\n",
        "        discretizer(sklearn.preprocessing.KBinsDiscretizer)\n",
        "\n",
        "    Transformation result is a int array.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_bins):\n",
        "        self.n_bins = n_bins\n",
        "        self.meta = None\n",
        "        self.column_index = None\n",
        "        self.discretizer = None\n",
        "\n",
        "    def fit(self, data, categorical_columns=tuple(), ordinal_columns=tuple()):\n",
        "        self.meta = self.get_metadata(data, categorical_columns, ordinal_columns)\n",
        "        self.column_index = [\n",
        "            index for index, info in enumerate(self.meta) if info['type'] == CONTINUOUS]\n",
        "\n",
        "        self.discretizer = KBinsDiscretizer(\n",
        "            n_bins=self.n_bins, encode='ordinal', strategy='uniform')\n",
        "\n",
        "        if not self.column_index:\n",
        "            return\n",
        "\n",
        "        self.discretizer.fit(data[:, self.column_index])\n",
        "\n",
        "    def transform(self, data):\n",
        "        \"\"\"Transform data discretizing continous values.\n",
        "\n",
        "        Args:\n",
        "            data(pandas.DataFrame)\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray\n",
        "\n",
        "        \"\"\"\n",
        "        if self.column_index == []:\n",
        "            return data.astype('int')\n",
        "\n",
        "        data[:, self.column_index] = self.discretizer.transform(data[:, self.column_index])\n",
        "        return data.astype('int')\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        if self.column_index == []:\n",
        "            return data\n",
        "\n",
        "        data = data.astype('float32')\n",
        "        data[:, self.column_index] = self.discretizer.inverse_transform(data[:, self.column_index])\n",
        "        return data\n",
        "\n",
        "\n",
        "class GeneralTransformer(Transformer):\n",
        "    \"\"\"Continuous and ordinal columns are normalized to [0, 1].\n",
        "    Discrete columns are converted to a one-hot vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, act='sigmoid'):\n",
        "        self.act = act\n",
        "        self.meta = None\n",
        "        self.output_dim = None\n",
        "\n",
        "    def fit(self, data, categorical_columns=tuple(), ordinal_columns=tuple()):\n",
        "        self.meta = self.get_metadata(data, categorical_columns, ordinal_columns)\n",
        "        self.output_dim = 0\n",
        "        for info in self.meta:\n",
        "            if info['type'] in [CONTINUOUS, ORDINAL]:\n",
        "                self.output_dim += 1\n",
        "            else:\n",
        "                self.output_dim += info['size']\n",
        "\n",
        "    def transform(self, data):\n",
        "        data_t = []\n",
        "        self.output_info = []\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            col = data[:, id_]\n",
        "            if info['type'] == CONTINUOUS:\n",
        "                col = (col - (info['min'])) / (info['max'] - info['min'])\n",
        "                if self.act == 'tanh':\n",
        "                    col = col * 2 - 1\n",
        "                data_t.append(col.reshape([-1, 1]))\n",
        "                self.output_info.append((1, self.act))\n",
        "\n",
        "            elif info['type'] == ORDINAL:\n",
        "                col = col / info['size']\n",
        "                if self.act == 'tanh':\n",
        "                    col = col * 2 - 1\n",
        "                data_t.append(col.reshape([-1, 1]))\n",
        "                self.output_info.append((1, self.act))\n",
        "\n",
        "            else:\n",
        "                col_t = np.zeros([len(data), info['size']])\n",
        "                idx = list(map(info['i2s'].index, col))\n",
        "                col_t[np.arange(len(data)), idx] = 1\n",
        "                data_t.append(col_t)\n",
        "                self.output_info.append((info['size'], 'softmax'))\n",
        "\n",
        "        return np.concatenate(data_t, axis=1)\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        data_t = np.zeros([len(data), len(self.meta)])\n",
        "\n",
        "        data = data.copy()\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == CONTINUOUS:\n",
        "                current = data[:, 0]\n",
        "                data = data[:, 1:]\n",
        "\n",
        "                if self.act == 'tanh':\n",
        "                    current = (current + 1) / 2\n",
        "\n",
        "                current = np.clip(current, 0, 1)\n",
        "                data_t[:, id_] = current * (info['max'] - info['min']) + info['min']\n",
        "\n",
        "            elif info['type'] == ORDINAL:\n",
        "                current = data[:, 0]\n",
        "                data = data[:, 1:]\n",
        "\n",
        "                if self.act == 'tanh':\n",
        "                    current = (current + 1) / 2\n",
        "\n",
        "                current = current * info['size']\n",
        "                current = np.round(current).clip(0, info['size'] - 1)\n",
        "                data_t[:, id_] = current\n",
        "            else:\n",
        "                current = data[:, :info['size']]\n",
        "                data = data[:, info['size']:]\n",
        "                idx = np.argmax(current, axis=1)\n",
        "                data_t[:, id_] = list(map(info['i2s'].__getitem__, idx))\n",
        "\n",
        "        return data_t\n",
        "\n",
        "\n",
        "class GMMTransformer(Transformer):\n",
        "    \"\"\"\n",
        "    Continuous columns are modeled with a GMM.\n",
        "        and then normalized to a scalor [0, 1] and a n_cluster dimensional vector.\n",
        "\n",
        "    Discrete and ordinal columns are converted to a one-hot vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters=5):\n",
        "        self.meta = None\n",
        "        self.n_clusters = n_clusters\n",
        "\n",
        "    def fit(self, data, categorical_columns=tuple(), ordinal_columns=tuple()):\n",
        "        self.meta = self.get_metadata(data, categorical_columns, ordinal_columns)\n",
        "        model = []\n",
        "\n",
        "        self.output_info = []\n",
        "        self.output_dim = 0\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == CONTINUOUS:\n",
        "                gm = GaussianMixture(self.n_clusters)\n",
        "                gm.fit(data[:, id_].reshape([-1, 1]))\n",
        "                model.append(gm)\n",
        "                self.output_info += [(1, 'tanh'), (self.n_clusters, 'softmax')]\n",
        "                self.output_dim += 1 + self.n_clusters\n",
        "            else:\n",
        "                model.append(None)\n",
        "                self.output_info += [(info['size'], 'softmax')]\n",
        "                self.output_dim += info['size']\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def transform(self, data):\n",
        "        values = []\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            current = data[:, id_]\n",
        "            if info['type'] == CONTINUOUS:\n",
        "                current = current.reshape([-1, 1])\n",
        "\n",
        "                means = self.model[id_].means_.reshape((1, self.n_clusters))\n",
        "                stds = np.sqrt(self.model[id_].covariances_).reshape((1, self.n_clusters))\n",
        "                features = (current - means) / (2 * stds)\n",
        "\n",
        "                probs = self.model[id_].predict_proba(current.reshape([-1, 1]))\n",
        "                argmax = np.argmax(probs, axis=1)\n",
        "                idx = np.arange((len(features)))\n",
        "                features = features[idx, argmax].reshape([-1, 1])\n",
        "\n",
        "                features = np.clip(features, -.99, .99)\n",
        "\n",
        "                values += [features, probs]\n",
        "            else:\n",
        "                col_t = np.zeros([len(data), info['size']])\n",
        "                idx = list(map(info['i2s'].index, current))\n",
        "                col_t[np.arange(len(data)), idx] = 1\n",
        "                values.append(col_t)\n",
        "\n",
        "        return np.concatenate(values, axis=1)\n",
        "\n",
        "    def inverse_transform(self, data, sigmas):\n",
        "        data_t = np.zeros([len(data), len(self.meta)])\n",
        "\n",
        "        st = 0\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == CONTINUOUS:\n",
        "                u = data[:, st]\n",
        "                v = data[:, st + 1:st + 1 + self.n_clusters]\n",
        "                if sigmas is not None:\n",
        "                    sig = sigmas[st]\n",
        "                    u = np.random.normal(u, sig)\n",
        "\n",
        "                u = np.clip(u, -1, 1)\n",
        "                st += 1 + self.n_clusters\n",
        "                means = self.model[id_].means_.reshape([-1])\n",
        "                stds = np.sqrt(self.model[id_].covariances_).reshape([-1])\n",
        "                p_argmax = np.argmax(v, axis=1)\n",
        "                std_t = stds[p_argmax]\n",
        "                mean_t = means[p_argmax]\n",
        "                tmp = u * 2 * std_t + mean_t\n",
        "                data_t[:, id_] = tmp\n",
        "\n",
        "            else:\n",
        "                current = data[:, st:st + info['size']]\n",
        "                st += info['size']\n",
        "                idx = np.argmax(current, axis=1)\n",
        "                data_t[:, id_] = list(map(info['i2s'].__getitem__, idx))\n",
        "\n",
        "        return data_t\n",
        "\n",
        "\n",
        "class BGMTransformer(Transformer):\n",
        "    \"\"\"Model continuous columns with a BayesianGMM and normalized to a scalar [0, 1] and a vector.\n",
        "\n",
        "    Discrete and ordinal columns are converted to a one-hot vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters=10, eps=0.005):\n",
        "        \"\"\"n_cluster is the upper bound of modes.\"\"\"\n",
        "        self.meta = None\n",
        "        self.n_clusters = n_clusters\n",
        "        self.eps = eps\n",
        "\n",
        "    def fit(self, data, categorical_columns=tuple(), ordinal_columns=tuple()):\n",
        "        self.meta = self.get_metadata(data, categorical_columns, ordinal_columns)\n",
        "        model = []\n",
        "\n",
        "        self.output_info = []\n",
        "        self.output_dim = 0\n",
        "        self.components = []\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == CONTINUOUS:\n",
        "                gm = BayesianGaussianMixture(\n",
        "                    n_components=self.n_clusters,\n",
        "                    weight_concentration_prior_type='dirichlet_process',\n",
        "                    weight_concentration_prior=0.001,\n",
        "                    n_init=1)\n",
        "                gm.fit(data.iloc[:, id_].values.reshape([-1, 1]))\n",
        "                model.append(gm)\n",
        "                comp = gm.weights_ > self.eps\n",
        "                self.components.append(comp)\n",
        "\n",
        "                self.output_info += [(1, 'tanh'), (np.sum(comp), 'softmax')]\n",
        "                self.output_dim += 1 + np.sum(comp)\n",
        "            else:\n",
        "                model.append(None)\n",
        "                self.components.append(None)\n",
        "                self.output_info += [(info['size'], 'softmax')]\n",
        "                self.output_dim += info['size']\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def transform(self, data):\n",
        "        values = []\n",
        "        c_model = []\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            current = data.iloc[:, id_]\n",
        "            if info['type'] == CONTINUOUS:\n",
        "                current = current.values.reshape([-1, 1])\n",
        "\n",
        "                means = self.model[id_].means_.reshape((1, self.n_clusters))\n",
        "                stds = np.sqrt(self.model[id_].covariances_).reshape((1, self.n_clusters))\n",
        "                features = (current - means) / (4 * stds)\n",
        "\n",
        "                probs = self.model[id_].predict_proba(current.reshape([-1, 1]))\n",
        "\n",
        "                n_opts = sum(self.components[id_])\n",
        "                features = features[:, self.components[id_]]\n",
        "                probs = probs[:, self.components[id_]]\n",
        "\n",
        "                opt_sel = np.zeros(len(data), dtype='int')\n",
        "                for i in range(len(data)):\n",
        "                    pp = probs[i] + 1e-6\n",
        "                    pp = pp / sum(pp)\n",
        "                    opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
        "\n",
        "                idx = np.arange((len(features)))\n",
        "                features = features[idx, opt_sel].reshape([-1, 1])\n",
        "                features = np.clip(features, -.99, .99)\n",
        "\n",
        "                probs_onehot = np.zeros_like(probs)\n",
        "                probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
        "                values += [features, probs_onehot]\n",
        "                c_model.append(None)\n",
        "            else:\n",
        "                label_encoder = LabelEncoder()\n",
        "                current_encoded = label_encoder.fit_transform(current)\n",
        "                col_t = np.zeros([len(data), info['size']])\n",
        "                idx = list(map(info['i2s'].index, current))\n",
        "                # col_t[np.arange(len(data)), current_encoded] = 1\n",
        "                col_t[np.arange(len(data)), idx] = 1\n",
        "                c_model.append(label_encoder)\n",
        "                values.append(col_t)\n",
        "        self.c_model = c_model\n",
        "        return np.concatenate(values, axis=1)\n",
        "\n",
        "    def inverse_transform(self, data, sigmas):\n",
        "        data_t = np.zeros([len(data), len(self.meta)])\n",
        "        data_t = data_t.astype(object)\n",
        "        st = 0\n",
        "        for id_, info in enumerate(self.meta):\n",
        "            if info['type'] == CONTINUOUS:\n",
        "                u = data[:, st]\n",
        "                v = data[:, st + 1:st + 1 + np.sum(self.components[id_])]\n",
        "\n",
        "                if sigmas is not None:\n",
        "                    sig = sigmas[st]\n",
        "                    u = np.random.normal(u, sig)\n",
        "\n",
        "                u = np.clip(u, -1, 1)\n",
        "                v_t = np.ones((data.shape[0], self.n_clusters)) * -100\n",
        "                v_t[:, self.components[id_]] = v\n",
        "                v = v_t\n",
        "                st += 1 + np.sum(self.components[id_])\n",
        "                means = self.model[id_].means_.reshape([-1])\n",
        "                stds = np.sqrt(self.model[id_].covariances_).reshape([-1])\n",
        "                p_argmax = np.argmax(v, axis=1)\n",
        "                std_t = stds[p_argmax]\n",
        "                mean_t = means[p_argmax]\n",
        "                tmp = u * 4 * std_t + mean_t\n",
        "                data_t[:, id_] = tmp\n",
        "\n",
        "            else:\n",
        "                current = data[:, st:st + info['size']]\n",
        "                st += info['size']\n",
        "                idx = np.argmax(current, axis=1)\n",
        "                data_t[:, id_] = list(map(info['i2s'].__getitem__, idx))\n",
        "\n",
        "        return data_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rTJDM5MTn5fJ"
      },
      "outputs": [],
      "source": [
        "class Discriminator(Module):\n",
        "    def __init__(self, input_dim, dis_dims, pack=10):\n",
        "        super(Discriminator, self).__init__()\n",
        "        dim = input_dim * pack\n",
        "        self.pack = pack\n",
        "        self.packdim = dim\n",
        "        seq = []\n",
        "        for item in list(dis_dims):\n",
        "            seq += [\n",
        "                Linear(dim, item),\n",
        "                LeakyReLU(0.2),\n",
        "                Dropout(0.5)\n",
        "            ]\n",
        "            dim = item\n",
        "        seq += [Linear(dim, 1)]\n",
        "        self.seq = Sequential(*seq)\n",
        "\n",
        "    def forward(self, input):\n",
        "        assert input.size()[0] % self.pack == 0\n",
        "        return self.seq(input.view(-1, self.packdim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s5pnf-wAn5fK"
      },
      "outputs": [],
      "source": [
        "class Residual(Module):\n",
        "    def __init__(self, i, o):\n",
        "        super(Residual, self).__init__()\n",
        "        self.fc = Linear(i, o)\n",
        "        self.bn = BatchNorm1d(o)\n",
        "        self.relu = ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.fc(input)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        return torch.cat([out, input], dim=1)\n",
        "\n",
        "\n",
        "class Generator(Module):\n",
        "    def __init__(self, embedding_dim, gen_dims, data_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        dim = embedding_dim\n",
        "        seq = []\n",
        "        for item in list(gen_dims):\n",
        "            seq += [\n",
        "                Residual(dim, item)\n",
        "            ]\n",
        "            dim += item\n",
        "        seq.append(Linear(dim, data_dim))\n",
        "        self.seq = Sequential(*seq)\n",
        "\n",
        "    def forward(self, input):\n",
        "        data = self.seq(input)\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qyzuoE_9n5fK"
      },
      "outputs": [],
      "source": [
        "def apply_activate(data, output_info):\n",
        "    data_t = []\n",
        "    st = 0\n",
        "    for item in output_info:\n",
        "        if item[1] == 'tanh':\n",
        "            ed = st + item[0]\n",
        "            data_t.append(torch.tanh(data[:, st:ed]))\n",
        "            st = ed\n",
        "        elif item[1] == 'softmax':\n",
        "            ed = st + item[0]\n",
        "            data_t.append(F.gumbel_softmax(data[:, st:ed], tau=0.2))\n",
        "            st = ed\n",
        "        else:\n",
        "            assert 0\n",
        "    return torch.cat(data_t, dim=1)\n",
        "\n",
        "def random_choice_prob_index(a, axis=1):\n",
        "    r = np.expand_dims(np.random.rand(a.shape[1 - axis]), axis=axis)\n",
        "    return (a.cumsum(axis=axis) > r).argmax(axis=axis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "is8BQO6Ln5fL"
      },
      "outputs": [],
      "source": [
        "class Cond(object):\n",
        "    def __init__(self, data, output_info):\n",
        "        # self.n_col = self.n_opt = 0\n",
        "        # return\n",
        "        self.model = []\n",
        "\n",
        "        st = 0\n",
        "        skip = False\n",
        "        max_interval = 0\n",
        "        counter = 0\n",
        "        for item in output_info:\n",
        "            if item[1] == 'tanh':\n",
        "                st += item[0]\n",
        "                skip = True\n",
        "                continue\n",
        "            elif item[1] == 'softmax':\n",
        "                if skip:\n",
        "                    skip = False\n",
        "                    st += item[0]\n",
        "                    continue\n",
        "\n",
        "                ed = st + item[0]\n",
        "                max_interval = max(max_interval, ed - st)\n",
        "                counter += 1\n",
        "                self.model.append(np.argmax(data[:, st:ed], axis=-1))\n",
        "                st = ed\n",
        "            else:\n",
        "                assert 0\n",
        "        assert st == data.shape[1]\n",
        "\n",
        "        self.interval = []\n",
        "        self.n_col = 0\n",
        "        self.n_opt = 0\n",
        "        skip = False\n",
        "        st = 0\n",
        "        self.p = np.zeros((counter, max_interval))\n",
        "        for item in output_info:\n",
        "            if item[1] == 'tanh':\n",
        "                skip = True\n",
        "                st += item[0]\n",
        "                continue\n",
        "            elif item[1] == 'softmax':\n",
        "                if skip:\n",
        "                    st += item[0]\n",
        "                    skip = False\n",
        "                    continue\n",
        "                ed = st + item[0]\n",
        "                tmp = np.sum(data[:, st:ed], axis=0)\n",
        "                tmp = np.log(tmp + 1)\n",
        "                tmp = tmp / np.sum(tmp)\n",
        "                self.p[self.n_col, :item[0]] = tmp\n",
        "                self.interval.append((self.n_opt, item[0]))\n",
        "                self.n_opt += item[0]\n",
        "                self.n_col += 1\n",
        "                st = ed\n",
        "            else:\n",
        "                assert 0\n",
        "        self.interval = np.asarray(self.interval)\n",
        "\n",
        "    def sample(self, batch):\n",
        "        if self.n_col == 0:\n",
        "            return None\n",
        "        batch = batch\n",
        "        idx = np.random.choice(np.arange(self.n_col), batch)\n",
        "\n",
        "        vec1 = np.zeros((batch, self.n_opt), dtype='float32')\n",
        "        mask1 = np.zeros((batch, self.n_col), dtype='float32')\n",
        "        mask1[np.arange(batch), idx] = 1\n",
        "        opt1prime = random_choice_prob_index(self.p[idx])\n",
        "        opt1 = self.interval[idx, 0] + opt1prime\n",
        "        vec1[np.arange(batch), opt1] = 1\n",
        "\n",
        "        return vec1, mask1, idx, opt1prime\n",
        "\n",
        "    def sample_zero(self, batch):\n",
        "        if self.n_col == 0:\n",
        "            return None\n",
        "        vec = np.zeros((batch, self.n_opt), dtype='float32')\n",
        "        idx = np.random.choice(np.arange(self.n_col), batch)\n",
        "        for i in range(batch):\n",
        "            col = idx[i]\n",
        "            pick = int(np.random.choice(self.model[col]))\n",
        "            vec[i, pick + self.interval[col, 0]] = 1\n",
        "        return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3oe-vGyHn5fM"
      },
      "outputs": [],
      "source": [
        "def cond_loss(data, output_info, c, m):\n",
        "    loss = []\n",
        "    st = 0\n",
        "    st_c = 0\n",
        "    skip = False\n",
        "    for item in output_info:\n",
        "        if item[1] == 'tanh':\n",
        "            st += item[0]\n",
        "            skip = True\n",
        "\n",
        "        elif item[1] == 'softmax':\n",
        "            if skip:\n",
        "                skip = False\n",
        "                st += item[0]\n",
        "                continue\n",
        "\n",
        "            ed = st + item[0]\n",
        "            ed_c = st_c + item[0]\n",
        "            tmp = F.cross_entropy(\n",
        "                data[:, st:ed],\n",
        "                torch.argmax(c[:, st_c:ed_c], dim=1),\n",
        "                reduction='none'\n",
        "            )\n",
        "            loss.append(tmp)\n",
        "            st = ed\n",
        "            st_c = ed_c\n",
        "\n",
        "        else:\n",
        "            assert 0\n",
        "    loss = torch.stack(loss, dim=1)\n",
        "\n",
        "    return (loss * m).sum() / data.size()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uM6a2SdNn5fM"
      },
      "outputs": [],
      "source": [
        "class Sampler(object):\n",
        "    \"\"\"docstring for Sampler.\"\"\"\n",
        "\n",
        "    def __init__(self, data, output_info):\n",
        "        super(Sampler, self).__init__()\n",
        "        self.data = data\n",
        "        self.model = []\n",
        "        self.n = len(data)\n",
        "\n",
        "        st = 0\n",
        "        skip = False\n",
        "        for item in output_info:\n",
        "            if item[1] == 'tanh':\n",
        "                st += item[0]\n",
        "                skip = True\n",
        "            elif item[1] == 'softmax':\n",
        "                if skip:\n",
        "                    skip = False\n",
        "                    st += item[0]\n",
        "                    continue\n",
        "                ed = st + item[0]\n",
        "                tmp = []\n",
        "                for j in range(item[0]):\n",
        "                    tmp.append(np.nonzero(data[:, st + j])[0])\n",
        "                self.model.append(tmp)\n",
        "                st = ed\n",
        "            else:\n",
        "                assert 0\n",
        "        assert st == data.shape[1]\n",
        "\n",
        "    def sample(self, n, col, opt):\n",
        "        if col is None:\n",
        "            idx = np.random.choice(np.arange(self.n), n)\n",
        "            return self.data[idx]\n",
        "        idx = []\n",
        "        for c, o in zip(col, opt):\n",
        "            idx.append(np.random.choice(self.model[c][o]))\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VYLfTxMkn5fN"
      },
      "outputs": [],
      "source": [
        "def calc_gradient_penalty(netD, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n",
        "    alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n",
        "    alpha = alpha.repeat(1, pac, real_data.size(1))\n",
        "    alpha = alpha.view(-1, real_data.size(1))\n",
        "\n",
        "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "\n",
        "    # interpolates = torch.Variable(interpolates, requires_grad=True, device=device)\n",
        "\n",
        "    disc_interpolates = netD(interpolates)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=disc_interpolates, inputs=interpolates,\n",
        "        grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n",
        "        create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    gradient_penalty = (\n",
        "        (gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
        "    return gradient_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gR_M1FJsn5fO"
      },
      "outputs": [],
      "source": [
        "class ctGANSynthesizer(BaseSynthesizer):\n",
        "    \"\"\"docstring for IdentitySynthesizer.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                embedding_dim=128,\n",
        "                gen_dim=(256, 256),\n",
        "                dis_dim=(256, 256),\n",
        "                l2scale=1e-6,\n",
        "                batch_size=500,\n",
        "                epochs=300):\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.gen_dim = gen_dim\n",
        "        self.dis_dim = dis_dim\n",
        "\n",
        "        self.l2scale = l2scale\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.column_names = None\n",
        "\n",
        "    def fit(self, train_data, categorical_columns=tuple(), ordinal_columns=tuple()):\n",
        "        self.column_names = list(train_data.columns)\n",
        "        self.transformer = BGMTransformer()\n",
        "        self.transformer.fit(train_data, categorical_columns, ordinal_columns)\n",
        "        train_data = self.transformer.transform(train_data)\n",
        "\n",
        "        data_sampler = Sampler(train_data, self.transformer.output_info)\n",
        "\n",
        "        data_dim = self.transformer.output_dim\n",
        "        self.cond_generator = Cond(train_data, self.transformer.output_info)\n",
        "        e_dim = 128\n",
        "        self.generator = Generator(\n",
        "            e_dim + self.cond_generator.n_opt,\n",
        "            self.gen_dim,\n",
        "            data_dim).to(self.device)\n",
        "\n",
        "        discriminator = Discriminator(\n",
        "            data_dim + self.cond_generator.n_opt,\n",
        "            self.dis_dim).to(self.device)\n",
        "\n",
        "        optimizerG = optim.Adam(\n",
        "            self.generator.parameters(), lr=2e-4, betas=(0.5, 0.9), weight_decay=self.l2scale)\n",
        "        optimizerD = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n",
        "\n",
        "        assert self.batch_size % 2 == 0\n",
        "        # mean = torch.zeros(self.batch_size, self.embedding_dim, device=self.device)\n",
        "        mean = torch.zeros(self.batch_size, e_dim, device=self.device)\n",
        "        std = mean + 1\n",
        "\n",
        "        steps_per_epoch = len(train_data) // self.batch_size\n",
        "        for i in range(self.epochs):\n",
        "            for id_ in range(steps_per_epoch):\n",
        "                fakez = torch.normal(mean=mean, std=std)\n",
        "\n",
        "                condvec = self.cond_generator.sample(self.batch_size)\n",
        "                if condvec is None:\n",
        "                    c1, m1, col, opt = None, None, None, None\n",
        "                    real = data_sampler.sample(self.batch_size, col, opt)\n",
        "                else:\n",
        "                    c1, m1, col, opt = condvec\n",
        "                    c1 = torch.from_numpy(c1).to(self.device)\n",
        "                    m1 = torch.from_numpy(m1).to(self.device)\n",
        "                    fakez = torch.cat([fakez, c1], dim=1)\n",
        "\n",
        "                    perm = np.arange(self.batch_size)\n",
        "                    np.random.shuffle(perm)\n",
        "                    real = data_sampler.sample(self.batch_size, col[perm], opt[perm])\n",
        "                    c2 = c1[perm]\n",
        "\n",
        "                fake = self.generator(fakez)\n",
        "                fakeact = apply_activate(fake, self.transformer.output_info)\n",
        "\n",
        "                real = torch.from_numpy(real.astype('float32')).to(self.device)\n",
        "\n",
        "                if c1 is not None:\n",
        "                    fake_cat = torch.cat([fakeact, c1], dim=1)\n",
        "                    real_cat = torch.cat([real, c2], dim=1)\n",
        "                else:\n",
        "                    real_cat = real\n",
        "                    fake_cat = fake\n",
        "\n",
        "                y_fake = discriminator(fake_cat)\n",
        "                y_real = discriminator(real_cat)\n",
        "\n",
        "                loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n",
        "                pen = calc_gradient_penalty(discriminator, real_cat, fake_cat, self.device)\n",
        "\n",
        "                optimizerD.zero_grad()\n",
        "                pen.backward(retain_graph=True)\n",
        "                loss_d.backward()\n",
        "                optimizerD.step()\n",
        "\n",
        "                fakez = torch.normal(mean=mean, std=std)\n",
        "                condvec = self.cond_generator.sample(self.batch_size)\n",
        "\n",
        "                if condvec is None:\n",
        "                    c1, m1, col, opt = None, None, None, None\n",
        "                else:\n",
        "                    c1, m1, col, opt = condvec\n",
        "                    c1 = torch.from_numpy(c1).to(self.device)\n",
        "                    m1 = torch.from_numpy(m1).to(self.device)\n",
        "                    fakez = torch.cat([fakez, c1], dim=1)\n",
        "\n",
        "                fake = self.generator(fakez)\n",
        "                fakeact = apply_activate(fake, self.transformer.output_info)\n",
        "\n",
        "                if c1 is not None:\n",
        "                    y_fake = discriminator(torch.cat([fakeact, c1], dim=1))\n",
        "                else:\n",
        "                    y_fake = discriminator(fakeact)\n",
        "\n",
        "                if condvec is None:\n",
        "                    cross_entropy = 0\n",
        "                else:\n",
        "                    cross_entropy = cond_loss(fake, self.transformer.output_info, c1, m1)\n",
        "\n",
        "                loss_g = -torch.mean(y_fake) + cross_entropy\n",
        "\n",
        "                optimizerG.zero_grad()\n",
        "                loss_g.backward()\n",
        "                optimizerG.step()\n",
        "\n",
        "    def sample(self, n):\n",
        "        output_info = self.transformer.output_info\n",
        "        steps = n // self.batch_size + 1\n",
        "        data = []\n",
        "        e_dim = 128\n",
        "        for i in range(steps):\n",
        "            mean = torch.zeros(self.batch_size, e_dim)\n",
        "            std = mean + 1\n",
        "            fakez = torch.normal(mean=mean, std=std).to(self.device)\n",
        "\n",
        "            condvec = self.cond_generator.sample_zero(self.batch_size)\n",
        "            if condvec is None:\n",
        "                pass\n",
        "            else:\n",
        "                c1 = condvec\n",
        "                c1 = torch.from_numpy(c1).to(self.device)\n",
        "                fakez = torch.cat([fakez, c1], dim=1)\n",
        "\n",
        "            fake = self.generator(fakez)\n",
        "            fakeact = apply_activate(fake, output_info)\n",
        "            data.append(fakeact.detach().cpu().numpy())\n",
        "\n",
        "        data = np.concatenate(data, axis=0)\n",
        "        data = data[:n]\n",
        "        inv_data = self.transformer.inverse_transform(data, None)\n",
        "        df_inv = pd.DataFrame(inv_data)\n",
        "        df_inv.columns = self.column_names\n",
        "        return df_inv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0iQPOeTSn5fQ"
      },
      "outputs": [],
      "source": [
        "def get_trained_synthesizer(data, metadata):\n",
        "    metadata_obj = SingleTableMetadata.load_from_dict(metadata)\n",
        "    model = ctGANSynthesizer(metadata_obj)\n",
        "    data_dict = metadata_obj.columns\n",
        "    categorical_columns = [column for column, info in data_dict.items() if info.get('sdtype') == 'categorical']\n",
        "    categorical_columns_tuple = tuple(categorical_columns)\n",
        "    ordinal_columns = [column for column, info in data_dict.items() if info.get('sdtype') == 'ordinal']\n",
        "    ordinal_columns_tuple = tuple(ordinal_columns)\n",
        "    model.fit(data, categorical_columns_tuple, ordinal_columns_tuple)\n",
        "    return model\n",
        "\n",
        "def sample_from_synthesizer(synthesizer, n_samples):\n",
        "    return synthesizer.sample(n_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yn4fWvG-n5fQ"
      },
      "outputs": [],
      "source": [
        "from sdgym import create_single_table_synthesizer\n",
        "\n",
        "MyCustomSynthesizerClass = create_single_table_synthesizer(\n",
        "    get_trained_synthesizer_fn=get_trained_synthesizer,\n",
        "    sample_from_synthesizer_fn=sample_from_synthesizer,\n",
        "    display_name='newCTGANSynthesizer'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "UKX_bZcNn5fQ",
        "outputId": "712740eb-26d6-4d01-a0a7-2052bad2f25b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\mixture\\_base.py:268: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  warnings.warn(\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:1152: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:1152: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:1152: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:1152: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:1152: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:1152: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\mixture\\_base.py:268: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Synthesizer</th>\n",
              "      <th>Dataset</th>\n",
              "      <th>Dataset_Size_MB</th>\n",
              "      <th>Train_Time</th>\n",
              "      <th>Peak_Memory_MB</th>\n",
              "      <th>Synthesizer_Size_MB</th>\n",
              "      <th>Sample_Time</th>\n",
              "      <th>Evaluate_Time</th>\n",
              "      <th>Quality_Score</th>\n",
              "      <th>NewRowSynthesis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>UniformSynthesizer</td>\n",
              "      <td>KRK_v1</td>\n",
              "      <td>0.072128</td>\n",
              "      <td>0.131035</td>\n",
              "      <td>0.432352</td>\n",
              "      <td>0.067544</td>\n",
              "      <td>0.031916</td>\n",
              "      <td>18.521102</td>\n",
              "      <td>0.943317</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CTGANSynthesizer</td>\n",
              "      <td>KRK_v1</td>\n",
              "      <td>0.072128</td>\n",
              "      <td>108.601344</td>\n",
              "      <td>6.130727</td>\n",
              "      <td>1.491713</td>\n",
              "      <td>0.926765</td>\n",
              "      <td>19.081308</td>\n",
              "      <td>0.870947</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Custom:newCTGANSynthesizer</td>\n",
              "      <td>KRK_v1</td>\n",
              "      <td>0.072128</td>\n",
              "      <td>96.707541</td>\n",
              "      <td>2.048429</td>\n",
              "      <td>0.730611</td>\n",
              "      <td>0.216148</td>\n",
              "      <td>60.386469</td>\n",
              "      <td>0.658000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Synthesizer Dataset  Dataset_Size_MB  Train_Time  \\\n",
              "0          UniformSynthesizer  KRK_v1         0.072128    0.131035   \n",
              "1            CTGANSynthesizer  KRK_v1         0.072128  108.601344   \n",
              "2  Custom:newCTGANSynthesizer  KRK_v1         0.072128   96.707541   \n",
              "\n",
              "   Peak_Memory_MB  Synthesizer_Size_MB  Sample_Time  Evaluate_Time  \\\n",
              "0        0.432352             0.067544     0.031916      18.521102   \n",
              "1        6.130727             1.491713     0.926765      19.081308   \n",
              "2        2.048429             0.730611     0.216148      60.386469   \n",
              "\n",
              "   Quality_Score  NewRowSynthesis  \n",
              "0       0.943317              1.0  \n",
              "1       0.870947              1.0  \n",
              "2       0.658000              1.0  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_synthesizers = ['UniformSynthesizer', 'CTGANSynthesizer']\n",
        "scores = sdgym.benchmark_single_table(\n",
        "    synthesizers=baseline_synthesizers, custom_synthesizers=[MyCustomSynthesizerClass], sdv_datasets=['KRK_v1'])\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\mixture\\_base.py:268: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  warnings.warn(\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\mixture\\_base.py:268: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  warnings.warn(\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\mixture\\_base.py:268: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  warnings.warn(\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\mixture\\_base.py:268: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  warnings.warn(\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\mixture\\_base.py:268: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  warnings.warn(\n",
            "C:\\Users\\lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\mixture\\_base.py:268: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "data, metadata = load_dataset(dataset='adult', modality='single-table')\n",
        "\n",
        "metadata_obj = SingleTableMetadata.load_from_dict(metadata)\n",
        "data_dict = metadata_obj.columns\n",
        "categorical_columns = [column for column, info in data_dict.items() if info.get('sdtype') == 'categorical']\n",
        "categorical_columns_tuple = tuple(categorical_columns)\n",
        "ordinal_columns = [column for column, info in data_dict.items() if info.get('sdtype') == 'ordinal']\n",
        "ordinal_columns_tuple = tuple(ordinal_columns)\n",
        "\n",
        "synthesizer = ctGANSynthesizer()\n",
        "synthesizer.fit(data, categorical_columns, ordinal_columns)\n",
        "result = synthesizer.sample(100)\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
