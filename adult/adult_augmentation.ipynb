{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sdv.single_table import (\n",
    "    CopulaGANSynthesizer, CTGANSynthesizer, GaussianCopulaSynthesizer, TVAESynthesizer)\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential, functional\n",
    "from torch.nn import functional as F\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import sdgym\n",
    "from sdv.metadata.single_table import SingleTableMetadata\n",
    "from sdgym.datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import rdt\n",
    "from rdt.transformers import AnonymizedFaker, IDGenerator, RegexGenerator, get_default_transformers\n",
    "from copy import deepcopy\n",
    "import inspect\n",
    "import copy\n",
    "import traceback\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from rdt.transformers.pii.anonymization import get_anonymized_transformer\n",
    "import warnings\n",
    "import sys\n",
    "from pandas.core.tools.datetimes import _guess_datetime_format_for_array\n",
    "from pandas.api.types import is_float_dtype, is_integer_dtype\n",
    "import json\n",
    "import datetime\n",
    "import pkg_resources\n",
    "import cloudpickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import uuid\n",
    "import math\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "from tqdm import tqdm as tqdm1\n",
    "from tqdm import tqdm as tqdm2\n",
    "import functools\n",
    "import copulas\n",
    "from ctgan import CTGAN\n",
    "from sdgym import create_single_table_synthesizer\n",
    "from joblib import Parallel, delayed\n",
    "from rdt.transformers import ClusterBasedNormalizer, OneHotEncoder\n",
    "from collections import namedtuple\n",
    "import contextlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from table_evaluator import TableEvaluator\n",
    "from scipy.spatial.distance import cdist\n",
    "from rdt import HyperTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTGAN DataProcessor\n",
    "class DataProcessor:\n",
    "    \"\"\"Single table data processor.\n",
    "\n",
    "    This class handles all pre and post processing that is done to a single table to get it ready\n",
    "    for modeling and finalize sampling. These processes include formatting, transformations,\n",
    "    anonymization and constraint handling.\n",
    "\n",
    "    Args:\n",
    "        metadata (metadata.SingleTableMetadata):\n",
    "            The single table metadata instance that will be used to apply constraints and\n",
    "            transformations to the data.\n",
    "        enforce_rounding (bool):\n",
    "            Define rounding scheme for FloatFormatter. If True, the data returned by\n",
    "            reverse_transform will be rounded to that place. Defaults to True.\n",
    "        enforce_min_max_values (bool):\n",
    "            Specify whether or not to clip the data returned by reverse_transform of the numerical\n",
    "            transformer, FloatFormatter, to the min and max values seen during fit.\n",
    "            Defaults to True.\n",
    "        model_kwargs (dict):\n",
    "            Dictionary specifying the kwargs that need to be used in each tabular\n",
    "            model when working on this table. This dictionary contains as keys the name of the\n",
    "            TabularModel class and as values a dictionary containing the keyword arguments to use.\n",
    "            This argument exists mostly to ensure that the models are fitted using the same\n",
    "            arguments when the same DataProcessor is used to fit different model instances on\n",
    "            different slices of the same table.\n",
    "        table_name (str):\n",
    "            Name of table this processor is for. Optional.\n",
    "        locales (str or list):\n",
    "            Default locales to use for AnonymizedFaker transformers. Optional, defaults to using\n",
    "            Faker's default locale.\n",
    "    \"\"\"\n",
    "\n",
    "    _DTYPE_TO_SDTYPE = {\n",
    "        'i': 'numerical',\n",
    "        'f': 'numerical',\n",
    "        'O': 'categorical',\n",
    "        'b': 'boolean',\n",
    "        'M': 'datetime',\n",
    "    }\n",
    "\n",
    "    _COLUMN_RELATIONSHIP_TO_TRANSFORMER = {\n",
    "        'address': 'RandomLocationGenerator',\n",
    "    }\n",
    "\n",
    "    def _update_numerical_transformer(self, enforce_rounding, enforce_min_max_values):\n",
    "        custom_float_formatter = rdt.transformers.FloatFormatter(\n",
    "            missing_value_replacement='mean',\n",
    "            missing_value_generation='random',\n",
    "            learn_rounding_scheme=enforce_rounding,\n",
    "            enforce_min_max_values=enforce_min_max_values\n",
    "        )\n",
    "        self._transformers_by_sdtype.update({'numerical': custom_float_formatter})\n",
    "\n",
    "    def _detect_multi_column_transformers(self):\n",
    "        \"\"\"Detect if there are any multi column transformers in the metadata.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                A dictionary mapping column names to the multi column transformer.\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        # if self.metadata.column_relationships:\n",
    "        #     for relationship in self.metadata._valid_column_relationships:\n",
    "        #         column_names = tuple(relationship['column_names'])\n",
    "        #         relationship_type = relationship['type']\n",
    "        #         if relationship_type in self._COLUMN_RELATIONSHIP_TO_TRANSFORMER:\n",
    "        #             transformer_name = self._COLUMN_RELATIONSHIP_TO_TRANSFORMER[relationship_type]\n",
    "        #             module = getattr(rdt.transformers, relationship_type)\n",
    "        #             transformer = getattr(module, transformer_name)\n",
    "        #             result[column_names] = transformer(locales=self._locales)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __init__(self, metadata, enforce_rounding=True, enforce_min_max_values=True,\n",
    "                 model_kwargs=None, table_name=None, locales=None):\n",
    "        self.metadata = metadata\n",
    "        self._enforce_rounding = enforce_rounding\n",
    "        self._enforce_min_max_values = enforce_min_max_values\n",
    "        self._model_kwargs = model_kwargs or {}\n",
    "        self._locales = locales\n",
    "        self._constraints_list = []\n",
    "        self._constraints = []\n",
    "        self._constraints_to_reverse = []\n",
    "        self._custom_constraint_classes = {}\n",
    "\n",
    "        self._transformers_by_sdtype = deepcopy(get_default_transformers())\n",
    "        self._transformers_by_sdtype['id'] = rdt.transformers.RegexGenerator()\n",
    "        del self._transformers_by_sdtype['text']\n",
    "        self.grouped_columns_to_transformers = self._detect_multi_column_transformers()\n",
    "\n",
    "        self._update_numerical_transformer(enforce_rounding, enforce_min_max_values)\n",
    "        self._hyper_transformer = rdt.HyperTransformer()\n",
    "        self.table_name = table_name\n",
    "        self._dtypes = None\n",
    "        self.fitted = False\n",
    "        self.formatters = {}\n",
    "        self._primary_key = self.metadata.primary_key\n",
    "        self._prepared_for_fitting = False\n",
    "        self._keys = deepcopy(self.metadata.alternate_keys)\n",
    "        if self._primary_key:\n",
    "            self._keys.append(self._primary_key)\n",
    "        self.columns = None\n",
    "\n",
    "    def _get_grouped_columns(self):\n",
    "        \"\"\"Get the columns that are part of a multi column transformer.\n",
    "\n",
    "        Returns:\n",
    "            list:\n",
    "                A list of columns that are part of a multi column transformer.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            col for col_tuple in self.grouped_columns_to_transformers for col in col_tuple\n",
    "        ]\n",
    "\n",
    "    def _get_columns_in_address_transformer(self):\n",
    "        \"\"\"Get the columns that are part of an address transformer.\n",
    "\n",
    "        Returns:\n",
    "            list:\n",
    "                A list of columns that are part of the address transformers.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            _check_import_address_transformers()\n",
    "            result = []\n",
    "            for col_tuple, transformer in self.grouped_columns_to_transformers.items():\n",
    "                is_randomlocationgenerator = isinstance(\n",
    "                    transformer, rdt.transformers.address.RandomLocationGenerator\n",
    "                )\n",
    "                is_regionalanonymizer = isinstance(\n",
    "                    transformer, rdt.transformers.address.RegionalAnonymizer\n",
    "                )\n",
    "                if is_randomlocationgenerator or is_regionalanonymizer:\n",
    "                    result.extend(list(col_tuple))\n",
    "\n",
    "            return result\n",
    "        except ImportError:\n",
    "            return []\n",
    "\n",
    "    def get_model_kwargs(self, model_name):\n",
    "        \"\"\"Return the required model kwargs for the indicated model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str):\n",
    "                Qualified Name of the model for which model kwargs\n",
    "                are needed.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                Keyword arguments to use on the indicated model.\n",
    "        \"\"\"\n",
    "        return deepcopy(self._model_kwargs.get(model_name))\n",
    "\n",
    "    def set_model_kwargs(self, model_name, model_kwargs):\n",
    "        \"\"\"Set the model kwargs used for the indicated model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str):\n",
    "                Qualified Name of the model for which the kwargs will be set.\n",
    "            model_kwargs (dict):\n",
    "                The key word arguments for the model.\n",
    "        \"\"\"\n",
    "        self._model_kwargs[model_name] = model_kwargs\n",
    "\n",
    "    def get_sdtypes(self, primary_keys=False):\n",
    "        \"\"\"Get a ``dict`` with the ``sdtypes`` for each column of the table.\n",
    "\n",
    "        Args:\n",
    "            primary_keys (bool):\n",
    "                Whether or not to include the primary key fields. Defaults to ``False``.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                Dictionary that contains the column names and ``sdtypes``.\n",
    "        \"\"\"\n",
    "        sdtypes = {}\n",
    "        for name, column_metadata in self.metadata.columns.items():\n",
    "            sdtype = column_metadata['sdtype']\n",
    "\n",
    "            if primary_keys or (name not in self._keys):\n",
    "                sdtypes[name] = sdtype\n",
    "\n",
    "        return sdtypes\n",
    "\n",
    "    def _validate_custom_constraint_name(self, class_name):\n",
    "        reserved_class_names = list(get_subclasses(Constraint))\n",
    "        if class_name in reserved_class_names:\n",
    "            error_message = (\n",
    "                f\"The name '{class_name}' is a reserved constraint name. \"\n",
    "                'Please use a different one for the custom constraint.'\n",
    "            )\n",
    "            raise InvalidConstraintsError(error_message)\n",
    "\n",
    "    def _validate_custom_constraints(self, filepath, class_names, module):\n",
    "        errors = []\n",
    "        for class_name in class_names:\n",
    "            try:\n",
    "                self._validate_custom_constraint_name(class_name)\n",
    "            except InvalidConstraintsError as err:\n",
    "                errors += err.errors\n",
    "\n",
    "            if not hasattr(module, class_name):\n",
    "                errors.append(f\"The constraint '{class_name}' is not defined in '{filepath}'.\")\n",
    "\n",
    "        if errors:\n",
    "            raise InvalidConstraintsError(errors)\n",
    "\n",
    "    def load_custom_constraint_classes(self, filepath, class_names):\n",
    "        \"\"\"Load a custom constraint class for the current synthesizer.\n",
    "\n",
    "        Args:\n",
    "            filepath (str):\n",
    "                String representing the absolute or relative path to the python file where\n",
    "                the custom constraints are declared.\n",
    "            class_names (list):\n",
    "                A list of custom constraint classes to be imported.\n",
    "        \"\"\"\n",
    "        path = Path(filepath)\n",
    "        module = load_module_from_path(path)\n",
    "        self._validate_custom_constraints(filepath, class_names, module)\n",
    "        for class_name in class_names:\n",
    "            constraint_class = getattr(module, class_name)\n",
    "            self._custom_constraint_classes[class_name] = constraint_class\n",
    "\n",
    "    def add_custom_constraint_class(self, class_object, class_name):\n",
    "        \"\"\"Add a custom constraint class for the synthesizer to use.\n",
    "\n",
    "        Args:\n",
    "            class_object (sdv.constraints.Constraint):\n",
    "                A custom constraint class object.\n",
    "            class_name (str):\n",
    "                The name to assign this custom constraint class. This will be the name to use\n",
    "                when writing a constraint dictionary for ``add_constraints``.\n",
    "        \"\"\"\n",
    "        self._validate_custom_constraint_name(class_name)\n",
    "        self._custom_constraint_classes[class_name] = class_object\n",
    "\n",
    "    def _validate_constraint_dict(self, constraint_dict):\n",
    "        \"\"\"Validate a constraint against the single table metadata.\n",
    "\n",
    "        Args:\n",
    "            constraint_dict (dict):\n",
    "                A dictionary containing:\n",
    "                    * ``constraint_class``: Name of the constraint to apply.\n",
    "                    * ``constraint_parameters``: A dictionary with the constraint parameters.\n",
    "        \"\"\"\n",
    "        params = {'constraint_class', 'constraint_parameters'}\n",
    "        keys = constraint_dict.keys()\n",
    "        missing_params = params - keys\n",
    "        if missing_params:\n",
    "            raise SynthesizerInputError(\n",
    "                f'A constraint is missing required parameters {missing_params}. '\n",
    "                'Please add these parameters to your constraint definition.'\n",
    "            )\n",
    "\n",
    "        extra_params = keys - params\n",
    "        if extra_params:\n",
    "            raise SynthesizerInputError(\n",
    "                f'Unrecognized constraint parameter {extra_params}. '\n",
    "                'Please remove these parameters from your constraint definition.'\n",
    "            )\n",
    "\n",
    "        constraint_class = constraint_dict['constraint_class']\n",
    "        constraint_parameters = constraint_dict['constraint_parameters']\n",
    "        try:\n",
    "            if constraint_class in self._custom_constraint_classes:\n",
    "                constraint_class = self._custom_constraint_classes[constraint_class]\n",
    "\n",
    "            else:\n",
    "                constraint_class = Constraint._get_class_from_dict(constraint_class)\n",
    "\n",
    "        except KeyError:\n",
    "            raise InvalidConstraintsError(f\"Invalid constraint class ('{constraint_class}').\")\n",
    "\n",
    "        if 'column_name' in constraint_parameters:\n",
    "            column_names = [constraint_parameters.get('column_name')]\n",
    "        else:\n",
    "            column_names = constraint_parameters.get('column_names')\n",
    "\n",
    "        columns_in_address = self._get_columns_in_address_transformer()\n",
    "        if columns_in_address and column_names:\n",
    "            address_constraint_columns = set(column_names) & set(columns_in_address)\n",
    "            if address_constraint_columns:\n",
    "                to_print = \"', '\".join(address_constraint_columns)\n",
    "                raise InvalidConstraintsError(\n",
    "                    f\"The '{to_print}' columns are part of an address. You cannot add constraints \"\n",
    "                    'to columns that are part of an address group.'\n",
    "                )\n",
    "\n",
    "        constraint_class._validate_metadata(**constraint_parameters)\n",
    "\n",
    "    def add_constraints(self, constraints):\n",
    "        \"\"\"Add constraints to the data processor.\n",
    "\n",
    "        Args:\n",
    "            constraints (list):\n",
    "                List of constraints described as dictionaries in the following format:\n",
    "                    * ``constraint_class``: Name of the constraint to apply.\n",
    "                    * ``constraint_parameters``: A dictionary with the constraint parameters.\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        validated_constraints = []\n",
    "        for constraint_dict in constraints:\n",
    "            constraint_dict = deepcopy(constraint_dict)\n",
    "            if 'constraint_parameters' in constraint_dict:\n",
    "                constraint_dict['constraint_parameters'].update({'metadata': self.metadata})\n",
    "            try:\n",
    "                self._validate_constraint_dict(constraint_dict)\n",
    "                validated_constraints.append(constraint_dict)\n",
    "            except (AggregateConstraintsError, InvalidConstraintsError) as e:\n",
    "                reformated_errors = '\\n'.join(map(str, e.errors))\n",
    "                errors.append(reformated_errors)\n",
    "\n",
    "        if errors:\n",
    "            raise InvalidConstraintsError(errors)\n",
    "\n",
    "        self._constraints_list.extend(validated_constraints)\n",
    "        self._prepared_for_fitting = False\n",
    "\n",
    "    def get_constraints(self):\n",
    "        \"\"\"Get a list of the current constraints that will be used.\n",
    "\n",
    "        Returns:\n",
    "            list:\n",
    "                List of dictionaries describing the constraints for this data processor.\n",
    "        \"\"\"\n",
    "        constraints = deepcopy(self._constraints_list)\n",
    "        for i in range(len(constraints)):\n",
    "            del constraints[i]['constraint_parameters']['metadata']\n",
    "\n",
    "        return constraints\n",
    "\n",
    "    def _load_constraints(self):\n",
    "        loaded_constraints = []\n",
    "        default_constraints_classes = list(get_subclasses(Constraint))\n",
    "        for constraint in self._constraints_list:\n",
    "            if constraint['constraint_class'] in default_constraints_classes:\n",
    "                loaded_constraints.append(Constraint.from_dict(constraint))\n",
    "\n",
    "            else:\n",
    "                constraint_class = self._custom_constraint_classes[constraint['constraint_class']]\n",
    "                loaded_constraints.append(\n",
    "                    constraint_class(**constraint.get('constraint_parameters', {}))\n",
    "                )\n",
    "\n",
    "        return loaded_constraints\n",
    "\n",
    "    def _fit_constraints(self, data):\n",
    "        self._constraints = self._load_constraints()\n",
    "        errors = []\n",
    "        for constraint in self._constraints:\n",
    "            try:\n",
    "                constraint.fit(data)\n",
    "            except Exception as e:\n",
    "                errors.append(e)\n",
    "\n",
    "        if errors:\n",
    "            raise AggregateConstraintsError(errors)\n",
    "\n",
    "    def _transform_constraints(self, data, is_condition=False):\n",
    "        errors = []\n",
    "        if not is_condition:\n",
    "            self._constraints_to_reverse = []\n",
    "\n",
    "        for constraint in self._constraints:\n",
    "            try:\n",
    "                data = constraint.transform(data)\n",
    "                if not is_condition:\n",
    "                    self._constraints_to_reverse.append(constraint)\n",
    "\n",
    "            except (MissingConstraintColumnError, FunctionError) as error:\n",
    "                if isinstance(error, MissingConstraintColumnError):\n",
    "                    LOGGER.info(\n",
    "                        'Unable to transform %s with columns %s because they are not all available'\n",
    "                        ' in the data. This happens due to multiple, overlapping constraints.',\n",
    "                        constraint.__class__.__name__,\n",
    "                        error.missing_columns\n",
    "                    )\n",
    "                    log_exc_stacktrace(LOGGER, error)\n",
    "                else:\n",
    "                    # Error came from custom constraint. We don't want to crash but we do\n",
    "                    # want to log it.\n",
    "                    LOGGER.info(\n",
    "                        'Unable to transform %s with columns %s due to an error in transform: \\n'\n",
    "                        '%s\\nUsing the reject sampling approach instead.',\n",
    "                        constraint.__class__.__name__,\n",
    "                        constraint.column_names,\n",
    "                        str(error)\n",
    "                    )\n",
    "                    log_exc_stacktrace(LOGGER, error)\n",
    "                if is_condition:\n",
    "                    indices_to_drop = data.columns.isin(constraint.constraint_columns)\n",
    "                    columns_to_drop = data.columns.where(indices_to_drop).dropna()\n",
    "                    data = data.drop(columns_to_drop, axis=1)\n",
    "\n",
    "            except Exception as error:\n",
    "                errors.append(error)\n",
    "\n",
    "        if errors:\n",
    "            raise AggregateConstraintsError(errors)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _update_transformers_by_sdtypes(self, sdtype, transformer):\n",
    "        self._transformers_by_sdtype[sdtype] = transformer\n",
    "\n",
    "    @staticmethod\n",
    "    def create_anonymized_transformer(sdtype, column_metadata, enforce_uniqueness, locales=None):\n",
    "        \"\"\"Create an instance of an ``AnonymizedFaker``.\n",
    "\n",
    "        Read the extra keyword arguments from the ``column_metadata`` and use them to create\n",
    "        an instance of an ``AnonymizedFaker`` transformer.\n",
    "\n",
    "        Args:\n",
    "            sdtype (str):\n",
    "                Sematic data type or a ``Faker`` function name.\n",
    "            column_metadata (dict):\n",
    "                A dictionary representing the rest of the metadata for the given ``sdtype``.\n",
    "            enforce_uniqueness (bool):\n",
    "                If ``True`` overwrite ``enforce_uniqueness`` with ``True`` to ensure unique\n",
    "                generation for primary keys.\n",
    "            locales (str or list):\n",
    "                Locale or list of locales to use for the AnonymizedFaker transfomer. Optional,\n",
    "                defaults to using Faker's default locale.\n",
    "\n",
    "        Returns:\n",
    "            Instance of ``rdt.transformers.pii.AnonymizedFaker``.\n",
    "        \"\"\"\n",
    "        kwargs = {'locales': locales}\n",
    "        for key, value in column_metadata.items():\n",
    "            if key not in ['pii', 'sdtype']:\n",
    "                kwargs[key] = value\n",
    "\n",
    "        if enforce_uniqueness:\n",
    "            kwargs['enforce_uniqueness'] = True\n",
    "\n",
    "        try:\n",
    "            transformer = get_anonymized_transformer(sdtype, kwargs)\n",
    "        except AttributeError as error:\n",
    "            raise SynthesizerInputError(\n",
    "                f\"The sdtype '{sdtype}' is not compatible with any of the locales. To \"\n",
    "                \"continue, try changing the locales or adding 'en_US' as a possible option.\"\n",
    "            ) from error\n",
    "\n",
    "        return transformer\n",
    "\n",
    "    def create_regex_generator(self, column_name, sdtype, column_metadata, is_numeric):\n",
    "        \"\"\"Create a ``RegexGenerator`` for the ``id`` columns.\n",
    "\n",
    "        Read the keyword arguments from the ``column_metadata`` and use them to create\n",
    "        an instance of a ``RegexGenerator``. If ``regex_format`` is not present in the\n",
    "        metadata a default ``[0-1a-z]{5}`` will be used for object like data and an increasing\n",
    "        integer from ``0`` will be used for numerical data. Also if the column name is a primary\n",
    "        key or alternate key this will enforce the values to be unique.\n",
    "\n",
    "        Args:\n",
    "            column_name (str):\n",
    "                Name of the column.\n",
    "            sdtype (str):\n",
    "                Sematic data type or a ``Faker`` function name.\n",
    "            column_metadata (dict):\n",
    "                A dictionary representing the rest of the metadata for the given ``sdtype``.\n",
    "            is_numeric (boolean):\n",
    "                A boolean representing whether or not data type is numeric or not.\n",
    "\n",
    "        Returns:\n",
    "            transformer:\n",
    "                Instance of ``rdt.transformers.text.RegexGenerator`` or\n",
    "                ``rdt.transformers.pii.AnonymizedFaker`` with ``enforce_uniqueness`` set to\n",
    "                ``True``.\n",
    "        \"\"\"\n",
    "        default_regex_format = r'\\d{30}' if is_numeric else '[0-1a-z]{5}'\n",
    "        regex_format = column_metadata.get('regex_format', default_regex_format)\n",
    "        transformer = rdt.transformers.RegexGenerator(\n",
    "            regex_format=regex_format,\n",
    "            enforce_uniqueness=(column_name in self._keys)\n",
    "        )\n",
    "\n",
    "        return transformer\n",
    "\n",
    "    def _get_transformer_instance(self, sdtype, column_metadata):\n",
    "        transformer = self._transformers_by_sdtype[sdtype]\n",
    "        if isinstance(transformer, AnonymizedFaker):\n",
    "            is_lexify = transformer.function_name == 'lexify'\n",
    "            is_baseprovider = transformer.provider_name == 'BaseProvider'\n",
    "            if is_lexify and is_baseprovider:  # Default settings\n",
    "                return self.create_anonymized_transformer(\n",
    "                    sdtype, column_metadata, False, self._locales\n",
    "                )\n",
    "\n",
    "        kwargs = {\n",
    "            key: value for key, value in column_metadata.items()\n",
    "            if key not in ['pii', 'sdtype']\n",
    "        }\n",
    "        if sdtype == 'datetime':\n",
    "            kwargs['enforce_min_max_values'] = self._enforce_min_max_values\n",
    "\n",
    "        if kwargs and transformer is not None:\n",
    "            transformer_class = transformer.__class__\n",
    "            return transformer_class(**kwargs)\n",
    "\n",
    "        return deepcopy(transformer)\n",
    "\n",
    "    def _update_constraint_transformers(self, data, columns_created_by_constraints, config):\n",
    "        missing_columns = set(columns_created_by_constraints) - config['transformers'].keys()\n",
    "        for column in missing_columns:\n",
    "            dtype_kind = data[column].dtype.kind\n",
    "            if dtype_kind in ('i', 'f'):\n",
    "                config['sdtypes'][column] = 'numerical'\n",
    "                config['transformers'][column] = rdt.transformers.FloatFormatter(\n",
    "                    missing_value_replacement='mean',\n",
    "                    missing_value_generation='random',\n",
    "                    enforce_min_max_values=self._enforce_min_max_values\n",
    "                )\n",
    "            else:\n",
    "                sdtype = self._DTYPE_TO_SDTYPE.get(dtype_kind, 'categorical')\n",
    "                config['sdtypes'][column] = sdtype\n",
    "                config['transformers'][column] = self._get_transformer_instance(sdtype, {})\n",
    "\n",
    "        # Remove columns that have been dropped by the constraint\n",
    "        for column in list(config['sdtypes'].keys()):\n",
    "            if column not in data:\n",
    "                LOGGER.info(\n",
    "                    f\"A constraint has dropped the column '{column}', removing the transformer \"\n",
    "                    \"from the 'HyperTransformer'.\"\n",
    "                )\n",
    "                config['sdtypes'].pop(column)\n",
    "                config['transformers'].pop(column)\n",
    "\n",
    "        return config\n",
    "\n",
    "    def _create_config(self, data, columns_created_by_constraints):\n",
    "        sdtypes = {}\n",
    "        transformers = {}\n",
    "\n",
    "        columns_in_multi_col_transformer = self._get_grouped_columns()\n",
    "        for column in set(data.columns) - columns_created_by_constraints:\n",
    "            column_metadata = self.metadata.columns.get(column)\n",
    "            sdtype = column_metadata.get('sdtype')\n",
    "\n",
    "            if column in columns_in_multi_col_transformer:\n",
    "                sdtypes[column] = sdtype\n",
    "                continue\n",
    "\n",
    "            pii = column_metadata.get('pii', sdtype not in self._transformers_by_sdtype)\n",
    "            sdtypes[column] = 'pii' if pii else sdtype\n",
    "\n",
    "            if sdtype == 'id':\n",
    "                is_numeric = pd.api.types.is_numeric_dtype(data[column].dtype)\n",
    "                if column_metadata.get('regex_format', False):\n",
    "                    transformers[column] = self.create_regex_generator(\n",
    "                        column,\n",
    "                        sdtype,\n",
    "                        column_metadata,\n",
    "                        is_numeric\n",
    "                    )\n",
    "                    sdtypes[column] = 'text'\n",
    "\n",
    "                elif column in self._keys:\n",
    "                    prefix = None\n",
    "                    if not is_numeric:\n",
    "                        prefix = 'sdv-id-'\n",
    "\n",
    "                    transformers[column] = IDGenerator(prefix=prefix)\n",
    "                    sdtypes[column] = 'text'\n",
    "\n",
    "                else:\n",
    "                    transformers[column] = AnonymizedFaker(\n",
    "                        provider_name=None,\n",
    "                        function_name='bothify',\n",
    "                        function_kwargs={'text': '#####'}\n",
    "                    )\n",
    "                    sdtypes[column] = 'pii'\n",
    "\n",
    "            elif sdtype == 'unknown':\n",
    "                transformers[column] = AnonymizedFaker(\n",
    "                    function_name='bothify',\n",
    "                )\n",
    "                transformers[column].function_kwargs = {\n",
    "                    'text': 'sdv-pii-?????',\n",
    "                    'letters': '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "                }\n",
    "\n",
    "            elif pii:\n",
    "                enforce_uniqueness = bool(column in self._keys)\n",
    "                transformers[column] = self.create_anonymized_transformer(\n",
    "                    sdtype,\n",
    "                    column_metadata,\n",
    "                    enforce_uniqueness,\n",
    "                    self._locales\n",
    "                )\n",
    "\n",
    "            elif sdtype in self._transformers_by_sdtype:\n",
    "                transformers[column] = self._get_transformer_instance(sdtype, column_metadata)\n",
    "\n",
    "            else:\n",
    "                sdtypes[column] = 'categorical'\n",
    "                transformers[column] = self._get_transformer_instance(\n",
    "                    'categorical',\n",
    "                    column_metadata\n",
    "                )\n",
    "\n",
    "        for columns, transformer in self.grouped_columns_to_transformers.items():\n",
    "            transformers[columns] = transformer\n",
    "\n",
    "        config = {'transformers': transformers, 'sdtypes': sdtypes}\n",
    "        config = self._update_constraint_transformers(data, columns_created_by_constraints, config)\n",
    "\n",
    "        return config\n",
    "\n",
    "    def update_transformers(self, column_name_to_transformer):\n",
    "        \"\"\"Update any of the transformers assigned to each of the column names.\n",
    "\n",
    "        Args:\n",
    "            column_name_to_transformer (dict):\n",
    "                Dict mapping column names to transformers to be used for that column.\n",
    "        \"\"\"\n",
    "        if self._hyper_transformer.field_transformers == {}:\n",
    "            raise NotFittedError(\n",
    "                'The DataProcessor must be prepared for fitting before the transformers can be '\n",
    "                'updated.'\n",
    "            )\n",
    "\n",
    "        for column, transformer in column_name_to_transformer.items():\n",
    "            if column in self._keys and not type(transformer) in (AnonymizedFaker, RegexGenerator):\n",
    "                raise SynthesizerInputError(\n",
    "                    f\"Invalid transformer '{transformer.__class__.__name__}' for a primary \"\n",
    "                    f\"or alternate key '{column}'. Please use 'AnonymizedFaker' or \"\n",
    "                    \"'RegexGenerator' instead.\"\n",
    "                )\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', module='rdt.hyper_transformer')\n",
    "            self._hyper_transformer.update_transformers(column_name_to_transformer)\n",
    "\n",
    "        self.grouped_columns_to_transformers = {\n",
    "            col_tuple: transformer\n",
    "            for col_tuple, transformer in self._hyper_transformer.field_transformers.items()\n",
    "            if isinstance(col_tuple, tuple)\n",
    "        }\n",
    "\n",
    "    def _fit_hyper_transformer(self, data):\n",
    "        \"\"\"Create and return a new ``rdt.HyperTransformer`` instance.\n",
    "\n",
    "        First get the ``dtypes`` and then use them to build a transformer dictionary\n",
    "        to be used by the ``HyperTransformer``.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Data to transform.\n",
    "\n",
    "        Returns:\n",
    "            rdt.HyperTransformer\n",
    "        \"\"\"\n",
    "        self._hyper_transformer.fit(data)\n",
    "\n",
    "    def _fit_formatters(self, data):\n",
    "        \"\"\"Fit ``NumericalFormatter`` and ``DatetimeFormatter`` for each column in the data.\"\"\"\n",
    "        for column_name in data:\n",
    "            column_metadata = self.metadata.columns.get(column_name)\n",
    "            sdtype = column_metadata.get('sdtype')\n",
    "            if sdtype == 'numerical' and column_name != self._primary_key:\n",
    "                representation = column_metadata.get('computer_representation', 'Float')\n",
    "                self.formatters[column_name] = NumericalFormatter(\n",
    "                    enforce_rounding=self._enforce_rounding,\n",
    "                    enforce_min_max_values=self._enforce_min_max_values,\n",
    "                    computer_representation=representation\n",
    "                )\n",
    "                self.formatters[column_name].learn_format(data[column_name])\n",
    "\n",
    "            elif sdtype == 'datetime' and column_name != self._primary_key:\n",
    "                datetime_format = column_metadata.get('datetime_format')\n",
    "                self.formatters[column_name] = DatetimeFormatter(datetime_format=datetime_format)\n",
    "                self.formatters[column_name].learn_format(data[column_name])\n",
    "\n",
    "    def prepare_for_fitting(self, data):\n",
    "        \"\"\"Prepare the ``DataProcessor`` for fitting.\n",
    "\n",
    "        This method will learn the ``dtypes`` of the data, fit the numerical formatters,\n",
    "        fit the constraints and create the configuration for the ``rdt.HyperTransformer``.\n",
    "        If the ``rdt.HyperTransformer`` has already been updated, this will not perform the\n",
    "        actions again.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Table data to be learnt.\n",
    "        \"\"\"\n",
    "        if not self._prepared_for_fitting:\n",
    "            LOGGER.info(f'Fitting table {self.table_name} metadata')\n",
    "            self._dtypes = data[list(data.columns)].dtypes\n",
    "\n",
    "            self.formatters = {}\n",
    "            LOGGER.info(f'Fitting formatters for table {self.table_name}')\n",
    "            self._fit_formatters(data)\n",
    "\n",
    "            LOGGER.info(f'Fitting constraints for table {self.table_name}')\n",
    "            if len(self._constraints_list) != len(self._constraints):\n",
    "                self._fit_constraints(data)\n",
    "\n",
    "            constrained = self._transform_constraints(data)\n",
    "            columns_created_by_constraints = set(constrained.columns) - set(data.columns)\n",
    "\n",
    "            config = self._hyper_transformer.get_config()\n",
    "            missing_columns = columns_created_by_constraints - config.get('sdtypes').keys()\n",
    "            if not config.get('sdtypes'):\n",
    "                LOGGER.info((\n",
    "                    'Setting the configuration for the ``HyperTransformer`` '\n",
    "                    f'for table {self.table_name}'\n",
    "                ))\n",
    "                config = self._create_config(constrained, columns_created_by_constraints)\n",
    "                self._hyper_transformer.set_config(config)\n",
    "\n",
    "            elif missing_columns:\n",
    "                config = self._update_constraint_transformers(\n",
    "                    constrained,\n",
    "                    missing_columns,\n",
    "                    config\n",
    "                )\n",
    "                self._hyper_transformer = rdt.HyperTransformer()\n",
    "                self._hyper_transformer.set_config(config)\n",
    "\n",
    "            self._prepared_for_fitting = True\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit this metadata to the given data.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Table to be analyzed.\n",
    "        \"\"\"\n",
    "        if data.empty:\n",
    "            raise ValueError('The fit dataframe is empty, synthesizer will not be fitted.')\n",
    "        self._prepared_for_fitting = False\n",
    "        self.prepare_for_fitting(data)\n",
    "        constrained = self._transform_constraints(data)\n",
    "        if constrained.empty:\n",
    "            raise ValueError(\n",
    "                'The constrained fit dataframe is empty, synthesizer will not be fitted.')\n",
    "        LOGGER.info(f'Fitting HyperTransformer for table {self.table_name}')\n",
    "        self._fit_hyper_transformer(constrained)\n",
    "        self.fitted = True\n",
    "        self.columns = list(data.columns)\n",
    "\n",
    "    def reset_sampling(self):\n",
    "        \"\"\"Reset the sampling state for the anonymized columns and primary keys.\"\"\"\n",
    "        self._hyper_transformer.reset_randomization()\n",
    "\n",
    "    def generate_keys(self, num_rows, reset_keys=False):\n",
    "        \"\"\"Generate the columns that are identified as ``keys``.\n",
    "\n",
    "        Args:\n",
    "            num_rows (int):\n",
    "                Number of rows to be created. Must be an integer greater than 0.\n",
    "            reset_keys (bool):\n",
    "                Whether or not to reset the keys generators. Defaults to ``False``.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                A dataframe with the newly generated primary keys of the size ``num_rows``.\n",
    "        \"\"\"\n",
    "        generated_keys = self._hyper_transformer.create_anonymized_columns(\n",
    "            num_rows=num_rows,\n",
    "            column_names=self._keys,\n",
    "        )\n",
    "        return generated_keys\n",
    "\n",
    "    def transform(self, data, is_condition=False):\n",
    "        \"\"\"Transform the given data.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Table data.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Transformed data.\n",
    "        \"\"\"\n",
    "        data = data.copy()\n",
    "        if not self.fitted:\n",
    "            raise NotFittedError()\n",
    "\n",
    "        # Filter columns that can be transformed\n",
    "        columns = [\n",
    "            column for column in self.get_sdtypes(primary_keys=not is_condition)\n",
    "            if column in data.columns\n",
    "        ]\n",
    "        LOGGER.debug(f'Transforming constraints for table {self.table_name}')\n",
    "        data = self._transform_constraints(data[columns], is_condition)\n",
    "\n",
    "        LOGGER.debug(f'Transforming table {self.table_name}')\n",
    "        if self._keys and not is_condition:\n",
    "            data = data.set_index(self._primary_key, drop=False)\n",
    "\n",
    "        try:\n",
    "            transformed = self._hyper_transformer.transform_subset(data)\n",
    "        except (rdt.errors.NotFittedError, rdt.errors.ConfigNotSetError):\n",
    "            transformed = data\n",
    "\n",
    "        return transformed\n",
    "\n",
    "    def reverse_transform(self, data, reset_keys=False):\n",
    "        \"\"\"Reverse the transformed data to the original format.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Data to be reverse transformed.\n",
    "            reset_keys (bool):\n",
    "                Whether or not to reset the keys generators. Defaults to ``False``.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise NotFittedError()\n",
    "\n",
    "        reversible_columns = [\n",
    "            column\n",
    "            for column in self._hyper_transformer._output_columns\n",
    "            if column in data.columns\n",
    "        ]\n",
    "\n",
    "        reversed_data = data\n",
    "        try:\n",
    "            if not data.empty:\n",
    "                reversed_data = self._hyper_transformer.reverse_transform_subset(\n",
    "                    data[reversible_columns]\n",
    "                )\n",
    "        except rdt.errors.NotFittedError:\n",
    "            LOGGER.info(f'HyperTransformer has not been fitted for table {self.table_name}')\n",
    "\n",
    "        for transformer in self.grouped_columns_to_transformers.values():\n",
    "            if not transformer.output_columns:\n",
    "                reversed_data = transformer.reverse_transform(reversed_data)\n",
    "\n",
    "        num_rows = len(reversed_data)\n",
    "        sampled_columns = list(reversed_data.columns)\n",
    "        missing_columns = [\n",
    "            column\n",
    "            for column in self.metadata.columns.keys() - set(sampled_columns + self._keys)\n",
    "            if self._hyper_transformer.field_transformers.get(column)\n",
    "        ]\n",
    "        if missing_columns and num_rows:\n",
    "            anonymized_data = self._hyper_transformer.create_anonymized_columns(\n",
    "                num_rows=num_rows,\n",
    "                column_names=missing_columns\n",
    "            )\n",
    "            sampled_columns.extend(missing_columns)\n",
    "            reversed_data[anonymized_data.columns] = anonymized_data[anonymized_data.notna()]\n",
    "\n",
    "        if self._keys and num_rows:\n",
    "            generated_keys = self.generate_keys(num_rows, reset_keys)\n",
    "            sampled_columns.extend(self._keys)\n",
    "            reversed_data[generated_keys.columns] = generated_keys[generated_keys.notna()]\n",
    "\n",
    "        for constraint in reversed(self._constraints_to_reverse):\n",
    "            reversed_data = constraint.reverse_transform(reversed_data)\n",
    "\n",
    "        # Add new columns generated by the constraint\n",
    "        new_columns = list(set(reversed_data.columns) - set(sampled_columns))\n",
    "        sampled_columns.extend(new_columns)\n",
    "\n",
    "        # Sort the sampled columns in the order of the metadata.\n",
    "        # Any extra columns not present in the metadata will be dropped.\n",
    "        # In multitable there may be missing columns in the sample such as foreign keys\n",
    "        # And alternate keys. Thats the reason of ensuring that the metadata column is within\n",
    "        # The sampled columns.\n",
    "        sampled_columns = [\n",
    "            column for column in self.metadata.columns.keys()\n",
    "            if column in sampled_columns\n",
    "        ]\n",
    "        for column_name in sampled_columns:\n",
    "            column_data = reversed_data[column_name]\n",
    "\n",
    "            dtype = self._dtypes[column_name]\n",
    "            if is_integer_dtype(dtype) and is_float_dtype(column_data.dtype):\n",
    "                column_data = column_data.round()\n",
    "\n",
    "            reversed_data[column_name] = column_data[column_data.notna()]\n",
    "            try:\n",
    "                reversed_data[column_name] = reversed_data[column_name].astype(dtype)\n",
    "            except ValueError as e:\n",
    "                column_metadata = self.metadata.columns.get(column_name)\n",
    "                sdtype = column_metadata.get('sdtype')\n",
    "                if sdtype not in self._DTYPE_TO_SDTYPE.values():\n",
    "                    LOGGER.info(\n",
    "                        f\"The real data in '{column_name}' was stored as '{dtype}' but the \"\n",
    "                        'synthetic data could not be cast back to this type. If this is a '\n",
    "                        'problem, please check your input data and metadata settings.'\n",
    "                    )\n",
    "                    if column_name in self.formatters:\n",
    "                        self.formatters.pop(column_name)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(e)\n",
    "\n",
    "        # reformat columns using the formatters\n",
    "        for column in sampled_columns:\n",
    "            if column in self.formatters:\n",
    "                data_to_format = reversed_data[column]\n",
    "                reversed_data[column] = self.formatters[column].format_data(data_to_format)\n",
    "        d = reversed_data[sampled_columns]\n",
    "        new_column_order = self.columns\n",
    "        df_syn = d[new_column_order]\n",
    "        return df_syn\n",
    "\n",
    "    def filter_valid(self, data):\n",
    "        \"\"\"Filter the data using the constraints and return only the valid rows.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Table data.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Table containing only the valid rows.\n",
    "        \"\"\"\n",
    "        for constraint in self._constraints:\n",
    "            data = constraint.filter_valid(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Get a dict representation of this DataProcessor.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                Dict representation of this DataProcessor.\n",
    "        \"\"\"\n",
    "        constraints_to_reverse = [cnt.to_dict() for cnt in self._constraints_to_reverse]\n",
    "        return {\n",
    "            'metadata': deepcopy(self.metadata.to_dict()),\n",
    "            'constraints_list': self.get_constraints(),\n",
    "            'constraints_to_reverse': constraints_to_reverse,\n",
    "            'model_kwargs': deepcopy(self._model_kwargs)\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, metadata_dict, enforce_rounding=True, enforce_min_max_values=True):\n",
    "        \"\"\"Load a DataProcessor from a metadata dict.\n",
    "\n",
    "        Args:\n",
    "            metadata_dict (dict):\n",
    "                Dict metadata to load.\n",
    "            enforce_rounding (bool):\n",
    "                If passed, set the ``enforce_rounding`` on the new instance.\n",
    "            enforce_min_max_values (bool):\n",
    "                If passed, set the ``enforce_min_max_values`` on the new instance.\n",
    "        \"\"\"\n",
    "        instance = cls(\n",
    "            metadata=SingleTableMetadata.load_from_dict(metadata_dict['metadata']),\n",
    "            enforce_rounding=enforce_rounding,\n",
    "            enforce_min_max_values=enforce_min_max_values,\n",
    "            model_kwargs=metadata_dict.get('model_kwargs')\n",
    "        )\n",
    "\n",
    "        instance._constraints_to_reverse = [\n",
    "            Constraint.from_dict(cnt) for cnt in metadata_dict.get('constraints_to_reverse', [])\n",
    "        ]\n",
    "        instance._constraints_list = metadata_dict.get('constraints_list', [])\n",
    "\n",
    "        return instance\n",
    "\n",
    "    def to_json(self, filepath):\n",
    "        \"\"\"Dump this DataProcessor into a JSON file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str):\n",
    "                Path of the JSON file where this metadata will be stored.\n",
    "        \"\"\"\n",
    "        with open(filepath, 'w') as out_file:\n",
    "            json.dump(self.to_dict(), out_file, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, filepath):\n",
    "        \"\"\"Load a DataProcessor from a JSON.\n",
    "\n",
    "        Args:\n",
    "            filepath (str):\n",
    "                Path of the JSON file to load\n",
    "        \"\"\"\n",
    "        with open(filepath, 'r') as in_file:\n",
    "            return cls.from_dict(json.load(in_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to handle errors\n",
    "class SynthesizerInputError(Exception):\n",
    "    \"\"\"Error to raise when a bad input is provided to a ``Synthesizer``.\"\"\"\n",
    "def log_exc_stacktrace(logger, error):\n",
    "    \"\"\"Log the stack trace of an exception.\n",
    "\n",
    "    Args:\n",
    "        logger (logging.Logger):\n",
    "            A logger object to use for the logging.\n",
    "        error (Exception):\n",
    "            The error to log.\n",
    "    \"\"\"\n",
    "    message = ''.join(traceback.format_exception(type(error), error, error.__traceback__))\n",
    "    logger.debug(message)\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "class AggregateConstraintsError(Exception):\n",
    "    \"\"\"Error used to represent a list of constraint errors.\"\"\"\n",
    "\n",
    "    def __init__(self, errors):\n",
    "        self.errors = errors\n",
    "        for error in self.errors:\n",
    "            log_exc_stacktrace(LOGGER, error)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n' + '\\n\\n'.join(map(str, self.errors))\n",
    "class ConstraintMetadataError(Exception):\n",
    "    \"\"\"Error to raise when Metadata is not valid.\"\"\"\n",
    "class ConstraintsNotMetError(ValueError):\n",
    "    \"\"\"Exception raised when the given data is not valid for the constraints.\"\"\"\n",
    "\n",
    "    def __init__(self, message=''):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "class InvalidFunctionError(Exception):\n",
    "    \"\"\"Error used when an invalid function is utilized.\"\"\"\n",
    "class FunctionError(Exception):\n",
    "    \"\"\"Error used when an a function produces an unexpected error.\"\"\"\n",
    "class MissingConstraintColumnError(Exception):\n",
    "    \"\"\"Error used when constraint is provided a table with missing columns.\"\"\"\n",
    "\n",
    "    def __init__(self, missing_columns):\n",
    "        self.missing_columns = missing_columns\n",
    "class InvalidConstraintsError(Exception):\n",
    "    \"\"\"Error to raise when constraints are not valid.\"\"\"\n",
    "\n",
    "    def __init__(self, errors):\n",
    "        errors = errors if isinstance(errors, list) else [errors]\n",
    "        self.errors = errors\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            'The provided constraint is invalid:\\n' +\n",
    "            '\\n\\n'.join(map(str, self.errors))\n",
    "        )\n",
    "class NotFittedError(Exception):\n",
    "    \"\"\"Error to raise when ``DataProcessor`` is used before fitting.\"\"\"\n",
    "class InvalidDataError(Exception):\n",
    "    \"\"\"Error to raise when data is not valid.\"\"\"\n",
    "\n",
    "    def __init__(self, errors):\n",
    "        self.errors = errors\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            'The provided data does not match the metadata:\\n' +\n",
    "            '\\n\\n'.join(map(str, self.errors))\n",
    "        )\n",
    "def handle_sampling_error(is_tmp_file, output_file_path, sampling_error):\n",
    "    \"\"\"Handle sampling errors by printing a user-legible error and then raising.\n",
    "\n",
    "    Args:\n",
    "        is_tmp_file (bool):\n",
    "            Whether or not the output file is a temp file.\n",
    "        output_file_path (str):\n",
    "            The output file path.\n",
    "        sampling_error:\n",
    "            The error to raise.\n",
    "\n",
    "    Side Effects:\n",
    "        The error will be raised.\n",
    "    \"\"\"\n",
    "    if 'Unable to sample any rows for the given conditions' in str(sampling_error):\n",
    "        raise sampling_error\n",
    "\n",
    "    error_msg = None\n",
    "    if is_tmp_file:\n",
    "        error_msg = (\n",
    "            'Error: Sampling terminated. Partial results are stored in a temporary file: '\n",
    "            f'{output_file_path}. This file will be overridden the next time you sample. '\n",
    "            'Please rename the file if you wish to save these results.'\n",
    "        )\n",
    "    elif output_file_path is not None:\n",
    "        error_msg = (\n",
    "            f'Error: Sampling terminated. Partial results are stored in {output_file_path}.'\n",
    "        )\n",
    "\n",
    "    if error_msg:\n",
    "        raise type(sampling_error)(error_msg + '\\n' + str(sampling_error))\n",
    "\n",
    "    raise sampling_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other essential functions\n",
    "class LossValuesMixin:\n",
    "    \"\"\"Mixin for accessing loss values from synthesizers.\"\"\"\n",
    "\n",
    "    def get_loss_values(self):\n",
    "        \"\"\"Get the loss values from the model.\n",
    "\n",
    "        Raises:\n",
    "            - ``NotFittedError`` if synthesizer has not been fitted.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame:\n",
    "                Dataframe containing the loss values per epoch.\n",
    "        \"\"\"\n",
    "        if not self._fitted:\n",
    "            err_msg = 'Loss values are not available yet. Please fit your synthesizer first.'\n",
    "            raise NotFittedError(err_msg)\n",
    "\n",
    "        return self._model.loss_values.copy()\n",
    "def _check_import_address_transformers():\n",
    "    \"\"\"Check that the address transformers can be imported.\"\"\"\n",
    "    error_message = (\n",
    "        'You must have SDV Enterprise with the address add-on to use the address features'\n",
    "    )\n",
    "    if not hasattr(rdt.transformers, 'address'):\n",
    "        raise ImportError(error_message)\n",
    "\n",
    "    has_randomlocationgenerator = hasattr(rdt.transformers.address, 'RandomLocationGenerator')\n",
    "    has_regionalanonymizer = hasattr(rdt.transformers.address, 'RegionalAnonymizer')\n",
    "    if not has_randomlocationgenerator or not has_regionalanonymizer:\n",
    "        raise ImportError(error_message)\n",
    "def get_subclasses(cls):\n",
    "    \"\"\"Recursively find subclasses for the current class object.\"\"\"\n",
    "    subclasses = {}\n",
    "    for subclass in cls.__subclasses__():\n",
    "        subclasses[subclass.__name__] = subclass\n",
    "        subclasses.update(get_subclasses(subclass))\n",
    "\n",
    "    return subclasses\n",
    "class ConstraintMeta(type):\n",
    "    \"\"\"Metaclass for Constraints.\n",
    "\n",
    "    This metaclass replaces the ``__init__`` method with a new function\n",
    "    that stores the arguments passed to the __init__ method in a dict\n",
    "    as the attribute ``__kwargs__``.\n",
    "\n",
    "    This allows us to later on dump the class definition as a dict.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, bases, attr):  # noqa: N804\n",
    "        super().__init__(name, bases, attr)\n",
    "\n",
    "        old__init__ = self.__init__\n",
    "        signature = inspect.signature(old__init__)\n",
    "        arg_names = list(signature.parameters.keys())[1:]\n",
    "\n",
    "        def __init__(self, *args, **kwargs):  # noqa: N807\n",
    "            class_name = self.__class__.__name__\n",
    "            if name == class_name:\n",
    "                self.__kwargs__ = copy.deepcopy(kwargs)\n",
    "                self.__kwargs__.update(dict(zip(arg_names, args)))\n",
    "                self.metadata = self.__kwargs__.get('metadata')\n",
    "                if 'metadata' in kwargs:\n",
    "                    del kwargs['metadata']\n",
    "\n",
    "            old__init__(self, *args, **kwargs)\n",
    "\n",
    "        __init__.__doc__ = old__init__.__doc__\n",
    "        __init__.__signature__ = signature\n",
    "        self.__init__ = __init__\n",
    "def format_invalid_values_string(invalid_values, num_values):\n",
    "    \"\"\"Convert ``invalid_values`` into a string of invalid values.\n",
    "\n",
    "    Args:\n",
    "        invalid_values (pd.DataFrame, set):\n",
    "            Object of values to be converted into string.\n",
    "        num_values (int):\n",
    "            Maximum number of values of the object to show.\n",
    "\n",
    "    Returns:\n",
    "        str:\n",
    "            A stringified version of the object.\n",
    "    \"\"\"\n",
    "    if isinstance(invalid_values, pd.DataFrame):\n",
    "        if len(invalid_values) > num_values:\n",
    "            return f'{invalid_values.head(num_values)}\\n+{len(invalid_values) - num_values} more'\n",
    "\n",
    "    if isinstance(invalid_values, set):\n",
    "        invalid_values = sorted(invalid_values, key=lambda x: str(x))\n",
    "        if len(invalid_values) > num_values:\n",
    "            extra_missing_values = [f'+ {len(invalid_values) - num_values} more']\n",
    "            return f'{invalid_values[:num_values] + extra_missing_values}'\n",
    "\n",
    "    return f'{invalid_values}'\n",
    "def import_object(obj):\n",
    "    \"\"\"Import an object from its qualified name.\"\"\"\n",
    "    if isinstance(obj, str):\n",
    "        package, name = obj.rsplit('.', 1)\n",
    "        return getattr(importlib.import_module(package), name)\n",
    "\n",
    "    return obj\n",
    "def _get_qualified_name(obj):\n",
    "    \"\"\"Return the Fully Qualified Name from an instance or class.\"\"\"\n",
    "    module = obj.__module__\n",
    "    if hasattr(obj, '__name__'):\n",
    "        obj_name = obj.__name__\n",
    "    else:\n",
    "        obj_name = obj.__class__.__name__\n",
    "\n",
    "    return module + '.' + obj_name\n",
    "def _module_contains_callable_name(obj):\n",
    "    \"\"\"Return if module contains the name of the callable object.\"\"\"\n",
    "    if hasattr(obj, '__name__'):\n",
    "        obj_name = obj.__name__\n",
    "    else:\n",
    "        obj_name = obj.__class__.__name__\n",
    "    return obj_name in importlib.import_module(obj.__module__).__dict__\n",
    "def load_module_from_path(path):\n",
    "    \"\"\"Return the module from a given ``PosixPath``.\n",
    "\n",
    "    Args:\n",
    "        path (pathlib.Path):\n",
    "            A ``PosixPath`` object from where the module should be imported from.\n",
    "\n",
    "    Returns:\n",
    "        module:\n",
    "            The in memory module for the given file.\n",
    "    \"\"\"\n",
    "    assert path.exists(), 'The expected file was not found.'\n",
    "    module_path = path.parent\n",
    "    module_name = path.name.split('.')[0]\n",
    "    module_path = f'{module_path.name}.{module_name}'\n",
    "    spec = importlib.util.spec_from_file_location(module_path, path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "\n",
    "    return module\n",
    "class Constraint(metaclass=ConstraintMeta):\n",
    "    \"\"\"Constraint base class.\n",
    "\n",
    "    This class is not intended to be used directly and should rather be\n",
    "    subclassed to create different types of constraints.\n",
    "\n",
    "    Attributes:\n",
    "        constraint_columns (tuple[str]):\n",
    "            The names of the columns used by this constraint.\n",
    "        rebuild_columns (tuple[str]):\n",
    "            The names of the columns that this constraint will rebuild during\n",
    "            ``reverse_transform``.\n",
    "    \"\"\"\n",
    "\n",
    "    constraint_columns = ()\n",
    "    _hyper_transformer = None\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_inputs(cls, **kwargs):\n",
    "        errors = []\n",
    "        required_args = []\n",
    "        args = []\n",
    "        params = inspect.signature(cls).parameters\n",
    "        for arg_name, value in params.items():\n",
    "            args.append(arg_name)\n",
    "            if value.default is inspect._empty:\n",
    "                required_args.append(arg_name)\n",
    "\n",
    "        missing_values = set(required_args) - set(kwargs)\n",
    "        constraint = cls.__name__\n",
    "        article = 'an' if constraint == 'Inequality' else 'a'\n",
    "        if missing_values:\n",
    "            errors.append(ValueError(\n",
    "                f'Missing required values {missing_values} in {article} {constraint} constraint.'\n",
    "            ))\n",
    "\n",
    "        invalid_vals = set(kwargs) - set(args)\n",
    "        if invalid_vals:\n",
    "            errors.append(ValueError(\n",
    "                f'Invalid values {invalid_vals} are present in {article} {constraint} constraint.'\n",
    "            ))\n",
    "\n",
    "        if errors:\n",
    "            raise AggregateConstraintsError(errors)\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_metadata_columns(cls, metadata, **kwargs):\n",
    "        if 'column_name' in kwargs:\n",
    "            column_names = [kwargs.get('column_name')]\n",
    "        else:\n",
    "            column_names = kwargs.get('column_names')\n",
    "\n",
    "        missing_columns = set(column_names) - set(metadata.columns) - {None}\n",
    "        if missing_columns:\n",
    "            article = 'An' if cls.__name__ == 'Inequality' else 'A'\n",
    "            raise ConstraintMetadataError(\n",
    "                f'{article} {cls.__name__} constraint is being applied to invalid column names '\n",
    "                f'{missing_columns}. The columns must exist in the table.'\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_metadata_specific_to_constraint(metadata, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_metadata(cls, metadata, **kwargs):\n",
    "        \"\"\"Validate the metadata against the constraint.\n",
    "\n",
    "        Args:\n",
    "            metadata (sdv.metadata.SingleTableMetadata):\n",
    "                Single table metadata instance.\n",
    "            **kwargs (dict):\n",
    "                Any required kwargs for the constraint.\n",
    "\n",
    "        Raises:\n",
    "            AggregateConstraintsError:\n",
    "                All the errors from validating the metadata.\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        try:\n",
    "            cls._validate_inputs(**kwargs)\n",
    "        except AggregateConstraintsError as agg_error:\n",
    "            errors.extend(agg_error.errors)\n",
    "\n",
    "        try:\n",
    "            cls._validate_metadata_columns(metadata, **kwargs)\n",
    "        except Exception as e:\n",
    "            errors.append(e)\n",
    "\n",
    "        try:\n",
    "            cls._validate_metadata_specific_to_constraint(metadata, **kwargs)\n",
    "        except Exception as e:\n",
    "            errors.append(e)\n",
    "\n",
    "        if errors:\n",
    "            raise AggregateConstraintsError(errors)\n",
    "\n",
    "    def _validate_data_meets_constraint(self, table_data):\n",
    "        \"\"\"Make sure the given data is valid for the constraint.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Table data.\n",
    "\n",
    "        Raises:\n",
    "            ConstraintsNotMetError:\n",
    "                If the table data is not valid for the provided constraints.\n",
    "        \"\"\"\n",
    "        if set(self.constraint_columns).issubset(table_data.columns.to_numpy()):\n",
    "            is_valid_data = self.is_valid(table_data)\n",
    "            if not is_valid_data.all():\n",
    "                constraint_data = table_data[list(self.constraint_columns)]\n",
    "                invalid_rows = constraint_data[~is_valid_data]\n",
    "                invalid_rows_str = format_invalid_values_string(invalid_rows, 5)\n",
    "                err_msg = (\n",
    "                    f\"Data is not valid for the '{self.__class__.__name__}' constraint:\\n\"\n",
    "                    f'{invalid_rows_str}'\n",
    "                )\n",
    "\n",
    "                raise ConstraintsNotMetError(err_msg)\n",
    "\n",
    "    def _fit(self, table_data):\n",
    "        del table_data\n",
    "\n",
    "    def fit(self, table_data):\n",
    "        \"\"\"Fit ``Constraint`` class to data.\n",
    "\n",
    "        Args:\n",
    "            table_data (pandas.DataFrame):\n",
    "                Table data.\n",
    "        \"\"\"\n",
    "        self._fit(table_data)\n",
    "        self._validate_data_meets_constraint(table_data)\n",
    "\n",
    "    def _transform(self, table_data):\n",
    "        return table_data\n",
    "\n",
    "    def transform(self, table_data):\n",
    "        \"\"\"Perform necessary transformations needed by constraint.\n",
    "\n",
    "        Subclasses can optionally overwrite this method. If the transformation\n",
    "        requires certain columns to be present in ``table_data``, then the subclass\n",
    "        should overwrite the ``_transform`` method instead. This method raises a\n",
    "        ``MissingConstraintColumnError`` if the ``table_data`` is missing any columns\n",
    "        needed to do the transformation. If columns are present, this method will call\n",
    "        the ``_transform`` method. If ``_transform`` fails, the data will be returned\n",
    "        unchanged.\n",
    "\n",
    "        Args:\n",
    "            table_data (pandas.DataFrame):\n",
    "                Table data.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Input data unmodified.\n",
    "        \"\"\"\n",
    "        table_data = table_data.copy()\n",
    "        missing_columns = [col for col in self.constraint_columns if col not in table_data.columns]\n",
    "        if missing_columns:\n",
    "            raise MissingConstraintColumnError(missing_columns=missing_columns)\n",
    "\n",
    "        return self._transform(table_data)\n",
    "\n",
    "    def fit_transform(self, table_data):\n",
    "        \"\"\"Fit this Constraint to the data and then transform it.\n",
    "\n",
    "        Args:\n",
    "            table_data (pandas.DataFrame):\n",
    "                Table data.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Transformed data.\n",
    "        \"\"\"\n",
    "        self.fit(table_data)\n",
    "        return self.transform(table_data)\n",
    "\n",
    "    def _reverse_transform(self, table_data):\n",
    "        return table_data\n",
    "\n",
    "    def reverse_transform(self, table_data):\n",
    "        \"\"\"Handle logic around reverse transforming constraints.\n",
    "\n",
    "        If the ``transform`` method was skipped, then this method should be too.\n",
    "        Otherwise attempt to reverse transform and if that fails, return the data\n",
    "        unchanged to fall back on reject sampling.\n",
    "\n",
    "        Args:\n",
    "            table_data (pandas.DataFrame):\n",
    "                Table data.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Input data unmodified.\n",
    "        \"\"\"\n",
    "        table_data = table_data.copy()\n",
    "        return self._reverse_transform(table_data)\n",
    "\n",
    "    def is_valid(self, table_data):\n",
    "        \"\"\"Say whether the given table rows are valid.\n",
    "\n",
    "        This is a dummy version of the method that returns a series of ``True``\n",
    "        values to avoid dropping any rows. This should be overwritten by all\n",
    "        the subclasses that have a way to decide which rows are valid and which\n",
    "        are not.\n",
    "\n",
    "        Args:\n",
    "            table_data (pandas.DataFrame):\n",
    "                Table data.\n",
    "\n",
    "        Returns:\n",
    "            pandas.Series:\n",
    "                Series of ``True`` values\n",
    "        \"\"\"\n",
    "        return pd.Series(True, index=table_data.index)\n",
    "\n",
    "    def filter_valid(self, table_data):\n",
    "        \"\"\"Get only the rows that are valid.\n",
    "\n",
    "        The filtering is done by calling the method ``is_valid``, which should\n",
    "        be overwritten by subclasses, while this method should stay untouched.\n",
    "\n",
    "        Args:\n",
    "            table_data (pandas.DataFrame):\n",
    "                Table data.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Input data unmodified.\n",
    "        \"\"\"\n",
    "        valid = self.is_valid(table_data)\n",
    "        invalid = sum(~valid)\n",
    "        if invalid:\n",
    "            LOGGER.debug('%s: %s invalid rows out of %s.',\n",
    "                         self.__class__.__name__, sum(~valid), len(valid))\n",
    "\n",
    "        if isinstance(valid, pd.Series):\n",
    "            return table_data[valid.to_numpy()]\n",
    "\n",
    "        return table_data[valid]\n",
    "\n",
    "    @classmethod\n",
    "    def _get_class_from_dict(cls, constraint_class):\n",
    "        subclasses = get_subclasses(cls)\n",
    "        if isinstance(constraint_class, str):\n",
    "            if '.' in constraint_class:\n",
    "                constraint_class = import_object(constraint_class)\n",
    "            else:\n",
    "                constraint_class = subclasses[constraint_class]\n",
    "\n",
    "        return constraint_class\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, constraint_dict):\n",
    "        \"\"\"Build a Constraint object from a dict.\n",
    "\n",
    "        Args:\n",
    "            constraint_dict (dict):\n",
    "                Dict containing the keyword ``constraint_name`` alongside\n",
    "                any additional arguments needed to create the instance.\n",
    "\n",
    "        Returns:\n",
    "            Constraint:\n",
    "                New constraint instance.\n",
    "        \"\"\"\n",
    "        constraint_class = constraint_dict.get('constraint_class')\n",
    "        constraint_class = cls._get_class_from_dict(constraint_class)\n",
    "\n",
    "        return constraint_class(**constraint_dict.get('constraint_parameters', {}))\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Return a dict representation of this Constraint.\n",
    "\n",
    "        The dictionary will contain the Qualified Name of the constraint\n",
    "        class in the key ``constraint_name``, as well as any other arguments\n",
    "        that were passed to the constructor when the instance was created.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                Dict representation of this Constraint.\n",
    "        \"\"\"\n",
    "        constraint_dict = {'constraint_class': _get_qualified_name(self.__class__)}\n",
    "\n",
    "        constraint_parameters = {}\n",
    "        for key, obj in copy.deepcopy(self.__kwargs__).items():\n",
    "            if callable(obj) and _module_contains_callable_name(obj):\n",
    "                constraint_parameters[key] = _get_qualified_name(obj)\n",
    "            else:\n",
    "                constraint_parameters[key] = obj\n",
    "\n",
    "        constraint_dict['constraint_parameters'] = constraint_parameters\n",
    "        return constraint_dict\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "MAX_DECIMALS = sys.float_info.dig - 1\n",
    "INTEGER_BOUNDS = {\n",
    "    'Int8': (-2**7, 2**7 - 1),\n",
    "    'Int16': (-2**15, 2**15 - 1),\n",
    "    'Int32': (-2**31, 2**31 - 1),\n",
    "    'Int64': (-2**63, 2**63 - 1),\n",
    "    'UInt8': (0, 2**8 - 1),\n",
    "    'UInt16': (0, 2**16 - 1),\n",
    "    'UInt32': (0, 2**32 - 1),\n",
    "    'UInt64': (0, 2**64 - 1),\n",
    "}\n",
    "\n",
    "class NumericalFormatter:\n",
    "    \"\"\"Formatter for numerical data.\n",
    "\n",
    "    Args:\n",
    "        enforce_rounding (bool):\n",
    "            Whether or not to learn what place to round to based on the data seen during ``fit``.\n",
    "            If ``True``, the data returned by ``reverse_transform`` will be rounded to that place.\n",
    "            Defaults to ``False``.\n",
    "        enforce_min_max_values (bool):\n",
    "            Whether or not to clip the data returned by ``reverse_transform`` to the min and\n",
    "            max values seen during ``fit``.\n",
    "            Defaults to ``False``.\n",
    "        computer_representation (dtype):\n",
    "            Accepts ``'Int8'``, ``'Int16'``, ``'Int32'``, ``'Int64'``, ``'UInt8'``, ``'UInt16'``,\n",
    "            ``'UInt32'``, ``'UInt64'``, ``'Float'``.\n",
    "            Defaults to ``'Float'``.\n",
    "    \"\"\"\n",
    "\n",
    "    _dtype = None\n",
    "    _min_value = None\n",
    "    _max_value = None\n",
    "    _rounding_digits = None\n",
    "\n",
    "    def __init__(self, enforce_rounding=False, enforce_min_max_values=False,\n",
    "                 computer_representation='Float'):\n",
    "        self.enforce_rounding = enforce_rounding\n",
    "        self.enforce_min_max_values = enforce_min_max_values\n",
    "        self.computer_representation = computer_representation\n",
    "\n",
    "    @staticmethod\n",
    "    def _learn_rounding_digits(data):\n",
    "        \"\"\"Check if data has any decimals.\"\"\"\n",
    "        name = data.name\n",
    "        data = np.array(data)\n",
    "        roundable_data = data[~(np.isinf(data) | pd.isna(data))]\n",
    "\n",
    "        # Doesn't contain numbers\n",
    "        if len(roundable_data) == 0:\n",
    "            return None\n",
    "\n",
    "        # Doesn't contain decimal digits\n",
    "        if ((roundable_data % 1) == 0).all():\n",
    "            return 0\n",
    "\n",
    "        # Try to round to fewer digits\n",
    "        if (roundable_data == roundable_data.round(MAX_DECIMALS)).all():\n",
    "            for decimal in range(MAX_DECIMALS + 1):\n",
    "                if (roundable_data == roundable_data.round(decimal)).all():\n",
    "                    return decimal\n",
    "\n",
    "        # Can't round, not equal after MAX_DECIMALS digits of precision\n",
    "        LOGGER.info(\n",
    "            f\"No rounding scheme detected for column '{name}'.\"\n",
    "            ' Synthetic data will not be rounded.'\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    def learn_format(self, column):\n",
    "        \"\"\"Learn the format of a column.\n",
    "\n",
    "        Args:\n",
    "            column (pandas.Series):\n",
    "                Data to learn the format.\n",
    "        \"\"\"\n",
    "        self._dtype = column.dtype\n",
    "        if self.enforce_min_max_values:\n",
    "            self._min_value = column.min()\n",
    "            self._max_value = column.max()\n",
    "\n",
    "        if self.enforce_rounding:\n",
    "            self._rounding_digits = self._learn_rounding_digits(column)\n",
    "\n",
    "    def format_data(self, column):\n",
    "        \"\"\"Format a column according to the learned format.\n",
    "\n",
    "        Args:\n",
    "            column (pd.Series):\n",
    "                Data to format.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray:\n",
    "                containing the formatted data.\n",
    "        \"\"\"\n",
    "        column = column.copy().to_numpy()\n",
    "        if self.enforce_min_max_values:\n",
    "            column = column.clip(self._min_value, self._max_value)\n",
    "        elif self.computer_representation != 'Float':\n",
    "            min_bound, max_bound = INTEGER_BOUNDS[self.computer_representation]\n",
    "            column = column.clip(min_bound, max_bound)\n",
    "\n",
    "        is_integer = np.dtype(self._dtype).kind == 'i'\n",
    "        if self.enforce_rounding and self._rounding_digits is not None:\n",
    "            column = column.round(self._rounding_digits)\n",
    "        elif is_integer:\n",
    "            column = column.round(0)\n",
    "\n",
    "        if pd.isna(column).any() and is_integer:\n",
    "            return column\n",
    "\n",
    "        return column.astype(self._dtype)\n",
    "def get_datetime_format(value):\n",
    "    \"\"\"Get the ``strftime`` format for a given ``value``.\n",
    "\n",
    "    This function returns the ``strftime`` format of a given ``value`` when possible.\n",
    "    If the ``_guess_datetime_format_for_array`` from ``pandas.core.tools.datetimes`` is\n",
    "    able to detect the ``strftime`` it will return it as a ``string`` if not, a ``None``\n",
    "    will be returned.\n",
    "\n",
    "    Args:\n",
    "        value (pandas.Series, np.ndarray, list, or str):\n",
    "            Input to attempt detecting the format.\n",
    "\n",
    "    Return:\n",
    "        String representing the datetime format in ``strftime`` format or ``None`` if not detected.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, pd.Series):\n",
    "        value = pd.Series(value)\n",
    "\n",
    "    value = value[~value.isna()]\n",
    "    value = value.astype(str).to_numpy()\n",
    "\n",
    "    return _guess_datetime_format_for_array(value)\n",
    "class DatetimeFormatter:\n",
    "    \"\"\"Formatter for datetime data.\n",
    "\n",
    "    Args:\n",
    "        datetime_format (str):\n",
    "            The strftime to use for parsing time. For more information, see\n",
    "            https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n",
    "            If ``None`` it will attempt to learn it by itself. Defaults to ``None``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datetime_format=None):\n",
    "        self.datetime_format = datetime_format\n",
    "\n",
    "    def learn_format(self, column):\n",
    "        \"\"\"Learn the format of a column.\n",
    "\n",
    "        Args:\n",
    "            column (pandas.Series):\n",
    "                Data to learn the format.\n",
    "        \"\"\"\n",
    "        self._dtype = column.dtype\n",
    "        if self.datetime_format is None:\n",
    "            self.datetime_format = get_datetime_format(column)\n",
    "\n",
    "    def format_data(self, column):\n",
    "        \"\"\"Format a column according to the learned format.\n",
    "\n",
    "        Args:\n",
    "            column (pd.Series):\n",
    "                Data to format.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray:\n",
    "                containing the formatted data.\n",
    "        \"\"\"\n",
    "        if self.datetime_format:\n",
    "            try:\n",
    "                datetime_column = pd.to_datetime(column, format=self.datetime_format)\n",
    "                column = datetime_column.dt.strftime(self.datetime_format)\n",
    "            except ValueError:\n",
    "                column = pd.to_datetime(column).dt.strftime(self.datetime_format)\n",
    "\n",
    "        return column.astype(self._dtype)\n",
    "TMP_FILE_NAME = '.sample.csv.temp'\n",
    "DISABLE_TMP_FILE = 'disable'\n",
    "IGNORED_DICT_KEYS = ['fitted', 'distribution', 'type']\n",
    "def validate_file_path(output_file_path):\n",
    "    \"\"\"Validate the user-passed output file arg, and create the file.\"\"\"\n",
    "    output_path = None\n",
    "    if output_file_path == DISABLE_TMP_FILE:\n",
    "        # Temporary way of disabling the output file feature, used by HMA1.\n",
    "        return output_path\n",
    "\n",
    "    elif output_file_path:\n",
    "        output_path = os.path.abspath(output_file_path)\n",
    "        if os.path.exists(output_path):\n",
    "            raise AssertionError(f'{output_path} already exists.')\n",
    "\n",
    "    else:\n",
    "        if os.path.exists(TMP_FILE_NAME):\n",
    "            os.remove(TMP_FILE_NAME)\n",
    "\n",
    "        output_path = TMP_FILE_NAME\n",
    "\n",
    "    # Create the file.\n",
    "    with open(output_path, 'w+'):\n",
    "        pass\n",
    "\n",
    "    return output_path\n",
    "def groupby_list(list_to_check):\n",
    "    \"\"\"Return the first element of the list if the length is 1 else the entire list.\"\"\"\n",
    "    return list_to_check[0] if len(list_to_check) == 1 else list_to_check\n",
    "def check_num_rows(num_rows, expected_num_rows, is_reject_sampling, max_tries_per_batch):\n",
    "    \"\"\"Check the number of sampled rows against the expected number of rows.\n",
    "\n",
    "    If the number of sampled rows is zero, throw a ValueError.\n",
    "    If the number of sampled rows is less than the expected number of rows,\n",
    "    raise a warning.\n",
    "\n",
    "    Args:\n",
    "        num_rows (int):\n",
    "            The number of sampled rows.\n",
    "        expected_num_rows (int):\n",
    "            The expected number of rows.\n",
    "        is_reject_sampling (bool):\n",
    "            If reject sampling is used or not.\n",
    "        max_tries_per_batch (int):\n",
    "            Number of times to retry sampling until the batch size is met.\n",
    "\n",
    "    Side Effects:\n",
    "        ValueError or warning.\n",
    "    \"\"\"\n",
    "    if num_rows < expected_num_rows:\n",
    "        if num_rows == 0:\n",
    "            user_msg = ('Unable to sample any rows for the given conditions. ')\n",
    "            if is_reject_sampling:\n",
    "                user_msg = user_msg + (\n",
    "                    f'Try increasing `max_tries_per_batch` (currently: {max_tries_per_batch}). '\n",
    "                    'Note that increasing this value will also increase the sampling time.'\n",
    "                )\n",
    "            else:\n",
    "                user_msg = user_msg + (\n",
    "                    'This may be because the provided values are out-of-bounds in the '\n",
    "                    'current model. \\nPlease try again with a different set of values.'\n",
    "                )\n",
    "            raise ValueError(user_msg)\n",
    "\n",
    "        else:\n",
    "            # This case should only happen with reject sampling.\n",
    "            user_msg = (\n",
    "                f'Only able to sample {num_rows} rows for the given conditions. '\n",
    "                'To sample more rows, try increasing `max_tries_per_batch` '\n",
    "                f'(currently: {max_tries_per_batch}). Note that increasing this value '\n",
    "                'will also increase the sampling time.'\n",
    "            )\n",
    "            warnings.warn(user_msg)\n",
    "def detect_discrete_columns(metadata, data, transformers):\n",
    "    \"\"\"Detect the discrete columns in a dataset.\n",
    "\n",
    "    Because the metadata doesn't necessarily match the data (we only preprocess the data,\n",
    "    while the metadata stays static), this method tries to infer whether the data is\n",
    "    discrete.\n",
    "\n",
    "    Args:\n",
    "        metadata (sdv.metadata.SingleTableMetadata):\n",
    "            Metadata that belongs to the given ``data``.\n",
    "\n",
    "        data (pandas.DataFrame):\n",
    "            ``pandas.DataFrame`` that matches the ``metadata``.\n",
    "\n",
    "        transformers (dict[str: rdt.transformers.BaseTransformer]):\n",
    "            A dictionary mapping between column names and the transformers assigned\n",
    "            for it.\n",
    "\n",
    "    Returns:\n",
    "        discrete_columns (list):\n",
    "            A list of discrete columns to be used with some of ``sdv`` synthesizers.\n",
    "    \"\"\"\n",
    "    discrete_columns = []\n",
    "    for column in data.columns:\n",
    "        if column in metadata.columns:\n",
    "            sdtype = metadata.columns[column]['sdtype']\n",
    "            # Numerical and datetime columns never get preprocessed into categorical ones\n",
    "            if sdtype in ['numerical', 'datetime']:\n",
    "                continue\n",
    "\n",
    "            elif sdtype in ['categorical', 'boolean']:\n",
    "                transformer = transformers.get(column)\n",
    "                if transformer and transformer.get_output_sdtypes().get(column) == 'float':\n",
    "                    continue\n",
    "\n",
    "                discrete_columns.append(column)\n",
    "                continue\n",
    "\n",
    "        # Logic to detect columns produced by transformers outside of the metadata scope\n",
    "        # or columns created by constraints.\n",
    "        column_data = data[column].dropna()\n",
    "\n",
    "        # Ignore columns with only nans and empty datasets\n",
    "        if column_data.empty:\n",
    "            continue\n",
    "\n",
    "        # Non-integer floats and integers with too many unique values are not categorical\n",
    "        try:\n",
    "            column_data = column_data.astype('float')\n",
    "            is_int = column_data.equals(column_data.round())\n",
    "            is_float = not is_int\n",
    "            num_values = len(column_data)\n",
    "            num_categories = column_data.nunique()\n",
    "            threshold = max(10, num_values * .1)\n",
    "            has_many_categories = num_categories > threshold\n",
    "            if is_float or (is_int and has_many_categories):\n",
    "                continue\n",
    "\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "        # Everything else is presumed categorical\n",
    "        discrete_columns.append(column)\n",
    "\n",
    "    return discrete_columns\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def set_random_states(random_state, set_model_random_state):\n",
    "    \"\"\"Context manager for managing the random state.\n",
    "\n",
    "    Args:\n",
    "        random_state (int or tuple):\n",
    "            The random seed or a tuple of (numpy.random.RandomState, torch.Generator).\n",
    "        set_model_random_state (function):\n",
    "            Function to set the random state on the model.\n",
    "    \"\"\"\n",
    "    original_np_state = np.random.get_state()\n",
    "    original_torch_state = torch.get_rng_state()\n",
    "\n",
    "    random_np_state, random_torch_state = random_state\n",
    "\n",
    "    np.random.set_state(random_np_state.get_state())\n",
    "    torch.set_rng_state(random_torch_state.get_state())\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        current_np_state = np.random.RandomState()\n",
    "        current_np_state.set_state(np.random.get_state())\n",
    "        current_torch_state = torch.Generator()\n",
    "        current_torch_state.set_state(torch.get_rng_state())\n",
    "        set_model_random_state((current_np_state, current_torch_state))\n",
    "\n",
    "        np.random.set_state(original_np_state)\n",
    "        torch.set_rng_state(original_torch_state)\n",
    "\n",
    "def random_state(function):\n",
    "    \"\"\"Set the random state before calling the function.\n",
    "\n",
    "    Args:\n",
    "        function (Callable):\n",
    "            The function to wrap around.\n",
    "    \"\"\"\n",
    "\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        if self.random_states is None:\n",
    "            return function(self, *args, **kwargs)\n",
    "\n",
    "        else:\n",
    "            with set_random_states(self.random_states, self.set_random_state):\n",
    "                return function(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTGAN BaseSynthesizer\n",
    "class BaseSynthesizer:\n",
    "    \"\"\"Base class for all ``Synthesizers``.\n",
    "\n",
    "    The ``BaseSynthesizer`` class defines the common API that all the\n",
    "    ``Synthesizers`` need to implement, as well as common functionality.\n",
    "\n",
    "    Args:\n",
    "        metadata (sdv.metadata.SingleTableMetadata):\n",
    "            Single table metadata representing the data that this synthesizer will be used for.\n",
    "        enforce_min_max_values (bool):\n",
    "            Specify whether or not to clip the data returned by ``reverse_transform`` of\n",
    "            the numerical transformer, ``FloatFormatter``, to the min and max values seen\n",
    "            during ``fit``. Defaults to ``True``.\n",
    "        enforce_rounding (bool):\n",
    "            Define rounding scheme for ``numerical`` columns. If ``True``, the data returned\n",
    "            by ``reverse_transform`` will be rounded as in the original data. Defaults to ``True``.\n",
    "        locales (list or str):\n",
    "            The default locale(s) to use for AnonymizedFaker transformers. Defaults to ``None``.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _model_sdtype_transformers = None\n",
    "\n",
    "    def _validate_inputs(self, enforce_min_max_values, enforce_rounding):\n",
    "        if not isinstance(enforce_min_max_values, bool):\n",
    "            raise SynthesizerInputError(\n",
    "                f\"Invalid value '{enforce_min_max_values}' for parameter 'enforce_min_max_values'.\"\n",
    "                ' Please provide True or False.'\n",
    "            )\n",
    "\n",
    "        if not isinstance(enforce_rounding, bool):\n",
    "            raise SynthesizerInputError(\n",
    "                f\"Invalid value '{enforce_rounding}' for parameter 'enforce_rounding'.\"\n",
    "                ' Please provide True or False.'\n",
    "            )\n",
    "\n",
    "    def _update_default_transformers(self):\n",
    "        if self._model_sdtype_transformers is not None:\n",
    "            for sdtype, transformer in self._model_sdtype_transformers.items():\n",
    "                self._data_processor._update_transformers_by_sdtypes(sdtype, transformer)\n",
    "\n",
    "    def __init__(self, metadata, enforce_min_max_values=True, enforce_rounding=True, locales=None):\n",
    "        self._validate_inputs(enforce_min_max_values, enforce_rounding)\n",
    "        self.metadata = metadata\n",
    "        self.metadata.validate()\n",
    "        self.enforce_min_max_values = enforce_min_max_values\n",
    "        self.enforce_rounding = enforce_rounding\n",
    "        self.locales = locales\n",
    "        self._data_processor = DataProcessor(\n",
    "            metadata=self.metadata,\n",
    "            enforce_rounding=self.enforce_rounding,\n",
    "            enforce_min_max_values=self.enforce_min_max_values,\n",
    "            locales=self.locales\n",
    "        )\n",
    "        self._fitted = False\n",
    "        self._random_state_set = False\n",
    "        self._update_default_transformers()\n",
    "        self._creation_date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "        self._fitted_date = None\n",
    "        self._fitted_sdv_version = None\n",
    "\n",
    "    def set_address_columns(self, column_names, anonymization_level='full'):\n",
    "        \"\"\"Set the address multi-column transformer.\"\"\"\n",
    "        warnings.warn(\n",
    "            '`set_address_columns` is deprecated. Please add these columns directly to your'\n",
    "            ' metadata using `add_column_relationship`.', DeprecationWarning\n",
    "        )\n",
    "\n",
    "    def _validate_metadata(self, data):\n",
    "        \"\"\"Validate that the data follows the metadata.\"\"\"\n",
    "        errors = []\n",
    "        try:\n",
    "            self.metadata.validate_data(data)\n",
    "        except InvalidDataError as e:\n",
    "            errors += e.errors\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def _validate_constraints(self, data):\n",
    "        \"\"\"Validate that the data satisfies the constraints.\"\"\"\n",
    "        errors = []\n",
    "        try:\n",
    "            self._data_processor._fit_constraints(data)\n",
    "        except AggregateConstraintsError as e:\n",
    "            errors.append(e)\n",
    "\n",
    "        return errors\n",
    "\n",
    "    def _validate(self, data):\n",
    "        \"\"\"Validate any rules that only apply to specific synthesizers.\n",
    "\n",
    "        This method should be overridden by subclasses.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def validate(self, data):\n",
    "        \"\"\"Validate data.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame):\n",
    "                The data to validate.\n",
    "\n",
    "        Raises:\n",
    "            ValueError:\n",
    "                Raised when data is not of type pd.DataFrame.\n",
    "            InvalidDataError:\n",
    "                Raised if:\n",
    "                    * data columns don't match metadata\n",
    "                    * keys have missing values\n",
    "                    * primary or alternate keys are not unique\n",
    "                    # constraints are not valid\n",
    "                    * context columns vary for a sequence key\n",
    "                    * values of a column don't satisfy their sdtype\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        errors += self._validate_metadata(data)\n",
    "        errors += self._validate_constraints(data)\n",
    "        errors += self._validate(data)  # Validate rules specific to each synthesizer\n",
    "\n",
    "        if errors:\n",
    "            raise InvalidDataError(errors)\n",
    "\n",
    "    def _validate_transformers(self, column_name_to_transformer):\n",
    "        primary_and_alternate_keys = self.metadata._get_primary_and_alternate_keys()\n",
    "        sequence_keys = self.metadata._get_set_of_sequence_keys()\n",
    "        keys = primary_and_alternate_keys | sequence_keys\n",
    "        for column, transformer in column_name_to_transformer.items():\n",
    "            if transformer is None:\n",
    "                continue\n",
    "\n",
    "            if column in keys and not transformer.is_generator():\n",
    "                raise SynthesizerInputError(\n",
    "                    f\"Column '{column}' is a key. It cannot be preprocessed using \"\n",
    "                    f\"the '{type(transformer).__name__}' transformer.\"\n",
    "                )\n",
    "\n",
    "            # If columns were set, the transformer was fitted\n",
    "            if transformer.columns:\n",
    "                raise SynthesizerInputError(\n",
    "                    f\"Transformer for column '{column}' has already been fit on data.\")\n",
    "\n",
    "    def _warn_for_update_transformers(self, column_name_to_transformer):\n",
    "        \"\"\"Raise warnings for update_transformers.\n",
    "\n",
    "        Args:\n",
    "            column_name_to_transformer (dict):\n",
    "                Dict mapping column names to transformers to be used for that column.\n",
    "        \"\"\"\n",
    "        for column in column_name_to_transformer:\n",
    "            sdtype = self.metadata.columns.get(column, {}).get('sdtype')\n",
    "            if sdtype in {'categorical', 'boolean'}:\n",
    "                warnings.warn(\n",
    "                    f\"Replacing the default transformer for column '{column}' \"\n",
    "                    'might impact the quality of your synthetic data.'\n",
    "                )\n",
    "\n",
    "    def update_transformers(self, column_name_to_transformer):\n",
    "        \"\"\"Update any of the transformers assigned to each of the column names.\n",
    "\n",
    "        Args:\n",
    "            column_name_to_transformer (dict):\n",
    "                Dict mapping column names to transformers to be used for that column.\n",
    "        \"\"\"\n",
    "        self._validate_transformers(column_name_to_transformer)\n",
    "        self._warn_for_update_transformers(column_name_to_transformer)\n",
    "        self._data_processor.update_transformers(column_name_to_transformer)\n",
    "        if self._fitted:\n",
    "            msg = 'For this change to take effect, please refit the synthesizer using `fit`.'\n",
    "            warnings.warn(msg, UserWarning)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"Return the parameters used to instantiate the synthesizer.\"\"\"\n",
    "        parameters = inspect.signature(self.__init__).parameters\n",
    "        instantiated_parameters = {}\n",
    "        for parameter_name in parameters:\n",
    "            if parameter_name != 'metadata':\n",
    "                instantiated_parameters[parameter_name] = self.__dict__.get(parameter_name)\n",
    "\n",
    "        return instantiated_parameters\n",
    "\n",
    "    def get_metadata(self):\n",
    "        \"\"\"Return the ``SingleTableMetadata`` for this synthesizer.\"\"\"\n",
    "        return self.metadata\n",
    "\n",
    "    def load_custom_constraint_classes(self, filepath, class_names):\n",
    "        \"\"\"Load a custom constraint class for the current synthesizer.\n",
    "\n",
    "        Args:\n",
    "            filepath (str):\n",
    "                String representing the absolute or relative path to the python file where\n",
    "                the custom constraints are declared.\n",
    "            class_names (list):\n",
    "                A list of custom constraint classes to be imported.\n",
    "        \"\"\"\n",
    "        self._data_processor.load_custom_constraint_classes(filepath, class_names)\n",
    "\n",
    "    def add_custom_constraint_class(self, class_object, class_name):\n",
    "        \"\"\"Add a custom constraint class for the synthesizer to use.\n",
    "\n",
    "        Args:\n",
    "            class_object (sdv.constraints.Constraint):\n",
    "                A custom constraint class object.\n",
    "            class_name (str):\n",
    "                The name to assign this custom constraint class. This will be the name to use\n",
    "                when writing a constraint dictionary for ``add_constraints``.\n",
    "        \"\"\"\n",
    "        self._data_processor.add_custom_constraint_class(class_object, class_name)\n",
    "\n",
    "    def add_constraints(self, constraints):\n",
    "        \"\"\"Add constraints to the synthesizer.\n",
    "\n",
    "        Args:\n",
    "            constraints (list):\n",
    "                List of constraints described as dictionaries in the following format:\n",
    "                    * ``constraint_class``: Name of the constraint to apply.\n",
    "                    * ``constraint_parameters``: A dictionary with the constraint parameters.\n",
    "        \"\"\"\n",
    "        if self._fitted:\n",
    "            warnings.warn(\n",
    "                \"For these constraints to take effect, please refit the synthesizer using 'fit'.\"\n",
    "            )\n",
    "\n",
    "        self._data_processor.add_constraints(constraints)\n",
    "\n",
    "    def get_constraints(self):\n",
    "        \"\"\"Get a list of the current constraints that will be used.\n",
    "\n",
    "        Returns:\n",
    "            list:\n",
    "                List of dictionaries describing the constraints for this synthesizer.\n",
    "        \"\"\"\n",
    "        return self._data_processor.get_constraints()\n",
    "\n",
    "    def auto_assign_transformers(self, data):\n",
    "        \"\"\"Automatically assign the required transformers for the given data and constraints.\n",
    "\n",
    "        This method will automatically set a configuration to the ``rdt.HyperTransformer``\n",
    "        with the required transformers for the current data.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                The raw data (before any transformations) that will be used to fit the model.\n",
    "        \"\"\"\n",
    "        self._data_processor.prepare_for_fitting(data)\n",
    "\n",
    "    def get_transformers(self):\n",
    "        \"\"\"Get a dictionary mapping of ``column_name``  and ``rdt.transformers``.\n",
    "\n",
    "        A dictionary representing the column names and the transformers that will be used\n",
    "        to transform the data.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                A dictionary mapping with column names and transformers.\n",
    "        \"\"\"\n",
    "        field_transformers = self._data_processor._hyper_transformer.field_transformers\n",
    "        if field_transformers == {}:\n",
    "            raise ValueError(\n",
    "                \"No transformers were returned in 'get_transformers'. \"\n",
    "                \"Use 'auto_assign_transformers' or 'fit' to create them.\"\n",
    "            )\n",
    "\n",
    "        # Order the output to match metadata\n",
    "        ordered_field_transformers = {\n",
    "            column_name: field_transformers.get(column_name)\n",
    "            for column_name in self.metadata.columns\n",
    "            if column_name in field_transformers\n",
    "        }\n",
    "\n",
    "        # Add missing columns created by the constraints\n",
    "        ordered_field_transformers.update(field_transformers)\n",
    "\n",
    "        return ordered_field_transformers\n",
    "\n",
    "    def get_info(self):\n",
    "        \"\"\"Get dictionary with information regarding the synthesizer.\n",
    "\n",
    "        Return:\n",
    "            dict:\n",
    "                * ``class_name``: synthesizer class name\n",
    "                * ``creation_date``: date of creation\n",
    "                * ``is_fit``: whether or not the synthesizer has been fit\n",
    "                * ``last_fit_date``: date for the last time it was fit\n",
    "                * ``fitted_sdv_version``: version of sdv it was on when fitted\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'class_name': self.__class__.__name__,\n",
    "            'creation_date': self._creation_date,\n",
    "            'is_fit': self._fitted,\n",
    "            'last_fit_date': self._fitted_date,\n",
    "            'fitted_sdv_version': self._fitted_sdv_version\n",
    "        }\n",
    "\n",
    "    def _preprocess(self, data):\n",
    "        self.validate(data)\n",
    "        self._data_processor.fit(data)\n",
    "        return self._data_processor.transform(data)\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        \"\"\"Transform the raw data to numerical space.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                The raw data to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                The preprocessed data.\n",
    "        \"\"\"\n",
    "        if self._fitted:\n",
    "            warnings.warn(\n",
    "                'This model has already been fitted. To use the new preprocessed data, '\n",
    "                \"please refit the model using 'fit' or 'fit_processed_data'.\"\n",
    "            )\n",
    "\n",
    "        return self._preprocess(data)\n",
    "\n",
    "    def _fit(self, processed_data):\n",
    "        \"\"\"Fit the model to the table.\n",
    "\n",
    "        Args:\n",
    "            processed_data (pandas.DataFrame):\n",
    "                Data to be learned.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def fit_processed_data(self, processed_data):\n",
    "        \"\"\"Fit this model to the transformed data.\n",
    "\n",
    "        Args:\n",
    "            processed_data (pandas.DataFrame):\n",
    "                The transformed data used to fit the model to.\n",
    "        \"\"\"\n",
    "        if not processed_data.empty:\n",
    "            self._fit(processed_data)\n",
    "\n",
    "        self._fitted = True\n",
    "        self._fitted_date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "        self._fitted_sdv_version = pkg_resources.get_distribution('sdv').version\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit this model to the original data.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                The raw data (before any transformations) to fit the model to.\n",
    "        \"\"\"\n",
    "        self._fitted = False\n",
    "        self._data_processor.reset_sampling()\n",
    "        self._random_state_set = False\n",
    "        processed_data = self._preprocess(data)\n",
    "        self.fit_processed_data(processed_data)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save this model instance to the given path using cloudpickle.\n",
    "\n",
    "        Args:\n",
    "            filepath (str):\n",
    "                Path where the synthesizer instance will be serialized.\n",
    "        \"\"\"\n",
    "        with open(filepath, 'wb') as output:\n",
    "            cloudpickle.dump(self, output)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"Load a TabularModel instance from a given path.\n",
    "\n",
    "        Args:\n",
    "            filepath (str):\n",
    "                Path from which to load the serialized synthesizer.\n",
    "\n",
    "        Returns:\n",
    "            SingleTableSynthesizer:\n",
    "                The loaded synthesizer.\n",
    "        \"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model = cloudpickle.load(f)\n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTGAN BaseSingleTableSynthesizer\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "COND_IDX = str(uuid.uuid4())\n",
    "FIXED_RNG_SEED = 73251\n",
    "TMP_FILE_NAME = '.sample.csv.temp'\n",
    "DISABLE_TMP_FILE = 'disable'\n",
    "class BaseSingleTableSynthesizer(BaseSynthesizer):\n",
    "    \"\"\"Base class for all single-table ``Synthesizers``.\n",
    "\n",
    "    The ``BaseSingleTableSynthesizer`` class defines the common sampling methods\n",
    "    for all single-table synthesizers.\n",
    "    \"\"\"\n",
    "\n",
    "    def _set_random_state(self, random_state):\n",
    "        \"\"\"Set the random state of the model's random number generator.\n",
    "\n",
    "        Args:\n",
    "            random_state (int, tuple[np.random.RandomState, torch.Generator], or None):\n",
    "                Seed or tuple of random states to use.\n",
    "        \"\"\"\n",
    "        self._model.set_random_state(random_state)\n",
    "        self._random_state_set = True\n",
    "\n",
    "    def reset_sampling(self):\n",
    "        \"\"\"Reset the sampling to the state that was left right after fitting.\"\"\"\n",
    "        self._data_processor.reset_sampling()\n",
    "        self._random_state_set = False\n",
    "\n",
    "    @staticmethod\n",
    "    def _filter_conditions(sampled, conditions, float_rtol):\n",
    "        \"\"\"Filter the sampled rows that match the conditions.\n",
    "\n",
    "        If condition columns are float values, consider a match anything that\n",
    "        is closer than the given ``float_rtol`` and then make the value exact.\n",
    "\n",
    "        Args:\n",
    "            sampled (pandas.DataFrame):\n",
    "                The sampled rows, reverse transformed.\n",
    "            conditions (dict):\n",
    "                The dictionary of conditioning values.\n",
    "            float_rtol (float):\n",
    "                Maximum tolerance when considering a float match.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Rows from the sampled data that match the conditions.\n",
    "        \"\"\"\n",
    "        for column, value in conditions.items():\n",
    "            column_values = sampled[column]\n",
    "            if column_values.dtype.kind == 'f':\n",
    "                distance = abs(value) * float_rtol\n",
    "                sampled = sampled[np.abs(column_values - value) <= distance]\n",
    "                sampled.loc[:, column] = value\n",
    "            else:\n",
    "                sampled = sampled[column_values == value]\n",
    "\n",
    "        return sampled\n",
    "\n",
    "    def _sample_rows(self, num_rows, conditions=None, transformed_conditions=None,\n",
    "                     float_rtol=0.1, previous_rows=None, keep_extra_columns=False):\n",
    "        \"\"\"Sample rows with the given conditions.\n",
    "\n",
    "        Input conditions is taken both in the raw input format, which will be used\n",
    "        for filtering during the reject-sampling loop, and already transformed\n",
    "        to the model format, which will be passed down to the model if it supports\n",
    "        conditional sampling natively.\n",
    "\n",
    "        If condition columns are float values, consider a match anything that\n",
    "        is closer than the given ``float_rtol`` and then make the value exact.\n",
    "\n",
    "        If the model does not have any data columns, the result of this call\n",
    "        is a dataframe of the requested length with no columns in it.\n",
    "\n",
    "        If there are no columns other than the ``primary_key``, this will proceed to sample\n",
    "        only the ``primary_key`` using the ``DataProcessor``.\n",
    "\n",
    "        Args:\n",
    "            num_rows (int):\n",
    "                Number of rows to sample.\n",
    "            conditions (dict):\n",
    "                The dictionary of conditioning values in the original format.\n",
    "            transformed_conditions (dict):\n",
    "                The dictionary of conditioning values transformed to the model format.\n",
    "            float_rtol (float):\n",
    "                Maximum tolerance when considering a float match.\n",
    "            previous_rows (pandas.DataFrame):\n",
    "                Valid rows sampled in the previous iterations.\n",
    "            keep_extra_columns (bool):\n",
    "                Whether to keep extra columns from the sampled data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                * pandas.DataFrame:\n",
    "                    Rows from the sampled data that match the conditions.\n",
    "                * int:\n",
    "                    Number of rows that are considered valid.\n",
    "        \"\"\"\n",
    "        if self._model and not self._random_state_set:\n",
    "            self._set_random_state(FIXED_RNG_SEED)\n",
    "\n",
    "        need_sample = self._data_processor.get_sdtypes(primary_keys=False) or keep_extra_columns\n",
    "        if self._model and need_sample:\n",
    "\n",
    "            if conditions is None:\n",
    "                raw_sampled = self._sample(num_rows)\n",
    "            else:\n",
    "                try:\n",
    "                    raw_sampled = self._sample(num_rows, transformed_conditions)\n",
    "                except NotImplementedError:\n",
    "                    raw_sampled = self._sample(num_rows)\n",
    "\n",
    "            sampled = self._data_processor.reverse_transform(raw_sampled)\n",
    "            if keep_extra_columns:\n",
    "                input_columns = self._data_processor._hyper_transformer._input_columns\n",
    "                missing_cols = list(\n",
    "                    set(raw_sampled.columns) - set(input_columns) - set(sampled.columns)\n",
    "                )\n",
    "                sampled = pd.concat([sampled, raw_sampled[missing_cols]], axis=1)\n",
    "\n",
    "            if previous_rows is not None:\n",
    "                sampled = pd.concat([previous_rows, sampled], ignore_index=True)\n",
    "\n",
    "            sampled = self._data_processor.filter_valid(sampled)\n",
    "\n",
    "            if conditions is not None:\n",
    "                sampled = self._filter_conditions(sampled, conditions, float_rtol)\n",
    "\n",
    "            num_valid = len(sampled)\n",
    "\n",
    "            return sampled, num_valid\n",
    "\n",
    "        else:\n",
    "            sampled = pd.DataFrame(index=range(num_rows))\n",
    "            sampled = self._data_processor.reverse_transform(sampled)\n",
    "            return sampled, num_rows\n",
    "\n",
    "    def _sample_batch(self, batch_size, max_tries=100,\n",
    "                      conditions=None, transformed_conditions=None, float_rtol=0.01,\n",
    "                      progress_bar=None, output_file_path=None, keep_extra_columns=False):\n",
    "        \"\"\"Sample a batch of rows with the given conditions.\n",
    "\n",
    "        This will enter a reject-sampling loop in which rows will be sampled until\n",
    "        all of them are valid and match the requested conditions. If ``max_tries``\n",
    "        is exceeded, it will return as many rows as it has sampled, which may be less\n",
    "        than the target number of rows.\n",
    "\n",
    "        Input conditions is taken both in the raw input format, which will be used\n",
    "        for filtering during the reject-sampling loop, and already transformed\n",
    "        to the model format, which will be passed down to the model if it supports\n",
    "        conditional sampling natively.\n",
    "\n",
    "        If condition columns are float values, consider a match anything that is\n",
    "        relatively closer than the given ``float_rtol`` and then make the value exact.\n",
    "\n",
    "        If the model does not have any data columns, the result of this call\n",
    "        is a dataframe of the requested length with no columns in it.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int):\n",
    "                Number of rows to sample for this batch. If not given the model\n",
    "                will generate as many rows as there were in the\n",
    "                data passed to the ``fit`` method.\n",
    "            max_tries (int):\n",
    "                Number of times to retry sampling until the batch size is met.\n",
    "                Defaults to 100.\n",
    "            conditions (dict):\n",
    "                The dictionary of conditioning values in the original input format.\n",
    "            transformed_conditions (dict):\n",
    "                The dictionary of conditioning values transformed to the model format.\n",
    "            float_rtol (float):\n",
    "                Maximum tolerance when considering a float match.\n",
    "            progress_bar (tqdm.tqdm or None):\n",
    "                The progress bar to update when sampling. If None, a new tqdm progress\n",
    "                bar will be created.\n",
    "            output_file_path (str or None):\n",
    "                The file to periodically write sampled rows to. If None, does not write\n",
    "                rows anywhere.\n",
    "            keep_extra_columns (bool):\n",
    "                Whether to keep extra columns from the sampled data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Sampled data.\n",
    "        \"\"\"\n",
    "        num_rows_to_sample = batch_size\n",
    "\n",
    "        counter = 0\n",
    "        num_valid = 0\n",
    "        prev_num_valid = None\n",
    "        remaining = batch_size\n",
    "        sampled = pd.DataFrame()\n",
    "\n",
    "        while num_valid < batch_size and counter < max_tries:\n",
    "            prev_num_valid = num_valid\n",
    "            sampled, num_valid = self._sample_rows(\n",
    "                num_rows_to_sample,\n",
    "                conditions,\n",
    "                transformed_conditions,\n",
    "                float_rtol,\n",
    "                sampled,\n",
    "                keep_extra_columns\n",
    "            )\n",
    "\n",
    "            num_new_valid_rows = num_valid - prev_num_valid\n",
    "            num_increase = min(num_new_valid_rows, remaining)\n",
    "            num_sampled = min(len(sampled), batch_size)\n",
    "            if num_increase > 0:\n",
    "                if output_file_path:\n",
    "                    append_kwargs = {'mode': 'a', 'header': False}\n",
    "                    append_kwargs = append_kwargs if os.path.getsize(output_file_path) > 0 else {}\n",
    "                    sampled.head(num_sampled).tail(num_increase).to_csv(\n",
    "                        output_file_path,\n",
    "                        index=False,\n",
    "                        **append_kwargs,\n",
    "                    )\n",
    "\n",
    "                if progress_bar is not None:\n",
    "                    progress_bar.update(num_increase)\n",
    "\n",
    "            remaining = batch_size - num_valid\n",
    "            valid_rate = max(num_new_valid_rows, 1) / max(num_rows_to_sample, 1)\n",
    "            num_rows_to_sample = min(10 * batch_size, int(remaining / valid_rate))\n",
    "\n",
    "            if remaining > 0:\n",
    "                LOGGER.info(\n",
    "                    f'{remaining} valid rows remaining. Resampling {num_rows_to_sample} rows')\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "        return sampled.head(min(len(sampled), batch_size))\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_condition_dfs(conditions):\n",
    "        \"\"\"Transform ``conditions`` into a list of dataframes.\n",
    "\n",
    "        Args:\n",
    "            conditions (list[sdv.sampling.Condition]):\n",
    "                A list of ``sdv.sampling.Condition``, where each ``Condition`` object\n",
    "                represents a desired column value mapping and the number of rows\n",
    "                to generate for that condition.\n",
    "\n",
    "        Returns:\n",
    "            list[pandas.DataFrame]:\n",
    "                A list of ``conditions`` as dataframes.\n",
    "        \"\"\"\n",
    "        condition_dataframes = defaultdict(list)\n",
    "        for condition in conditions:\n",
    "            column_values = condition.get_column_values()\n",
    "            condition_dataframes[tuple(column_values.keys())].append(\n",
    "                pd.DataFrame(column_values, index=range(condition.get_num_rows())))\n",
    "\n",
    "        return [\n",
    "            pd.concat(condition_list, ignore_index=True)\n",
    "            for condition_list in condition_dataframes.values()\n",
    "        ]\n",
    "\n",
    "    def _sample_in_batches(self, num_rows, batch_size, max_tries_per_batch, conditions=None,\n",
    "                           transformed_conditions=None, float_rtol=0.01, progress_bar=None,\n",
    "                           output_file_path=None):\n",
    "        sampled = []\n",
    "        batch_size = batch_size if num_rows > batch_size else num_rows\n",
    "        for step in range(math.ceil(num_rows / batch_size)):\n",
    "            sampled_rows = self._sample_batch(\n",
    "                batch_size=batch_size,\n",
    "                max_tries=max_tries_per_batch,\n",
    "                conditions=conditions,\n",
    "                transformed_conditions=transformed_conditions,\n",
    "                float_rtol=float_rtol,\n",
    "                progress_bar=progress_bar,\n",
    "                output_file_path=output_file_path,\n",
    "            )\n",
    "            sampled.append(sampled_rows)\n",
    "\n",
    "        sampled = pd.concat(sampled, ignore_index=True) if len(sampled) > 0 else pd.DataFrame()\n",
    "        return sampled.head(num_rows)\n",
    "\n",
    "    def _conditionally_sample_rows(self, dataframe, condition, transformed_condition,\n",
    "                                   max_tries_per_batch=None, batch_size=None, float_rtol=0.01,\n",
    "                                   graceful_reject_sampling=True, progress_bar=None,\n",
    "                                   output_file_path=None):\n",
    "        batch_size = batch_size or len(dataframe)\n",
    "        sampled_rows = self._sample_in_batches(\n",
    "            num_rows=len(dataframe),\n",
    "            batch_size=batch_size,\n",
    "            max_tries_per_batch=max_tries_per_batch,\n",
    "            conditions=condition,\n",
    "            transformed_conditions=transformed_condition,\n",
    "            float_rtol=float_rtol,\n",
    "            progress_bar=progress_bar,\n",
    "            output_file_path=output_file_path\n",
    "        )\n",
    "\n",
    "        if len(sampled_rows) > 0:\n",
    "            sampled_rows[COND_IDX] = dataframe[COND_IDX].to_numpy()[:len(sampled_rows)]\n",
    "\n",
    "        elif not graceful_reject_sampling:\n",
    "            user_msg = (\n",
    "                'Unable to sample any rows for the given conditions '\n",
    "                f\"'{transformed_condition}'. \"\n",
    "            )\n",
    "            if hasattr(self, '_model') and isinstance(self._model, GaussianMultivariate):\n",
    "                user_msg = user_msg + (\n",
    "                    'This may be because the provided values are out-of-bounds in the '\n",
    "                    'current model. \\nPlease try again with a different set of values.'\n",
    "                )\n",
    "            else:\n",
    "                user_msg = user_msg + (\n",
    "                    f\"Try increasing 'max_tries_per_batch' (currently: {max_tries_per_batch}) \"\n",
    "                    f\"or increasing 'batch_size' (currently: {batch_size}). Note that \"\n",
    "                    'increasing these values will also increase the sampling time.'\n",
    "                )\n",
    "\n",
    "            raise ValueError(user_msg)\n",
    "\n",
    "        return sampled_rows\n",
    "\n",
    "    def _sample_with_progress_bar(self, num_rows, max_tries_per_batch=100, batch_size=None,\n",
    "                                  output_file_path=None, show_progress_bar=True):\n",
    "        if num_rows is None:\n",
    "            raise ValueError('You must specify the number of rows to sample (e.g. num_rows=100).')\n",
    "\n",
    "        sampled = pd.DataFrame()\n",
    "        if num_rows == 0:\n",
    "            return sampled\n",
    "\n",
    "        output_file_path = validate_file_path(output_file_path)\n",
    "        batch_size = min(batch_size, num_rows) if batch_size else num_rows\n",
    "\n",
    "        try:\n",
    "            with tqdm1(total=num_rows, disable=not show_progress_bar) as progress_bar:\n",
    "                progress_bar.set_description('Sampling rows')\n",
    "                sampled = self._sample_in_batches(\n",
    "                    num_rows=num_rows,\n",
    "                    batch_size=batch_size,\n",
    "                    max_tries_per_batch=max_tries_per_batch,\n",
    "                    progress_bar=progress_bar,\n",
    "                    output_file_path=output_file_path\n",
    "                )\n",
    "\n",
    "        except (Exception, KeyboardInterrupt) as error:\n",
    "            handle_sampling_error(output_file_path == TMP_FILE_NAME, output_file_path, error)\n",
    "\n",
    "        else:\n",
    "            if output_file_path == TMP_FILE_NAME and os.path.exists(output_file_path):\n",
    "                os.remove(output_file_path)\n",
    "\n",
    "        return sampled\n",
    "\n",
    "    def sample(self, num_rows, max_tries_per_batch=100, batch_size=None, output_file_path=None):\n",
    "        \"\"\"Sample rows from this table.\n",
    "\n",
    "        Args:\n",
    "            num_rows (int):\n",
    "                Number of rows to sample. This parameter is required.\n",
    "            max_tries_per_batch (int):\n",
    "                Number of times to retry sampling until the batch size is met. Defaults to 100.\n",
    "            batch_size (int or None):\n",
    "                The batch size to sample. Defaults to ``num_rows``, if None.\n",
    "            output_file_path (str or None):\n",
    "                The file to periodically write sampled rows to. If None, does not\n",
    "                write rows anywhere.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Sampled data.\n",
    "        \"\"\"\n",
    "        has_constraints = bool(self._data_processor._constraints)\n",
    "        has_batches = batch_size is not None and batch_size != num_rows\n",
    "        show_progress_bar = has_constraints or has_batches\n",
    "\n",
    "        return self._sample_with_progress_bar(\n",
    "            num_rows,\n",
    "            max_tries_per_batch,\n",
    "            batch_size,\n",
    "            output_file_path,\n",
    "            show_progress_bar=show_progress_bar\n",
    "        )\n",
    "\n",
    "    def _validate_conditions(self, conditions):\n",
    "        \"\"\"Validate the user-passed conditions.\"\"\"\n",
    "        for column in conditions.columns:\n",
    "            if column not in self._data_processor.get_sdtypes():\n",
    "                raise ValueError(f\"Unexpected column name '{column}'. \"\n",
    "                                 f'Use a column name that was present in the original data.')\n",
    "\n",
    "    def _sample_with_conditions(self, conditions, max_tries_per_batch, batch_size,\n",
    "                                progress_bar=None, output_file_path=None):\n",
    "        \"\"\"Sample rows with conditions.\n",
    "\n",
    "        Args:\n",
    "            conditions (pandas.DataFrame):\n",
    "                A DataFrame representing the conditions to be sampled.\n",
    "            max_tries_per_batch (int):\n",
    "                Number of times to retry sampling until the batch size is met. Defaults to 100.\n",
    "            batch_size (int):\n",
    "                The batch size to use for each sampling call.\n",
    "            progress_bar (tqdm.tqdm or None):\n",
    "                The progress bar to update.\n",
    "            output_file_path (str or None):\n",
    "                The file to periodically write sampled rows to. Defaults to\n",
    "                a temporary file, if None.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Sampled data.\n",
    "\n",
    "        Raises:\n",
    "            ConstraintsNotMetError:\n",
    "                If the conditions are not valid for the given constraints.\n",
    "            ValueError:\n",
    "                If any of the following happens:\n",
    "                    * any of the conditions' columns are not valid.\n",
    "                    * no rows could be generated.\n",
    "        \"\"\"\n",
    "        condition_columns = list(conditions.columns)\n",
    "        conditions.index.name = COND_IDX\n",
    "        conditions = conditions.reset_index()\n",
    "        grouped_conditions = conditions.groupby(groupby_list(condition_columns))\n",
    "\n",
    "        # sample\n",
    "        all_sampled_rows = []\n",
    "\n",
    "        for group, dataframe in grouped_conditions:\n",
    "            if not isinstance(group, tuple):\n",
    "                group = [group]\n",
    "\n",
    "            condition = dict(zip(condition_columns, group))\n",
    "            condition_df = dataframe.iloc[0].to_frame().T\n",
    "            try:\n",
    "                transformed_condition = self._data_processor.transform(\n",
    "                    condition_df,\n",
    "                    is_condition=True\n",
    "                )\n",
    "            except ConstraintsNotMetError as error:\n",
    "                raise ConstraintsNotMetError(\n",
    "                    'Provided conditions are not valid for the given constraints.'\n",
    "                ) from error\n",
    "\n",
    "            transformed_conditions = pd.concat(\n",
    "                [transformed_condition] * len(dataframe),\n",
    "                ignore_index=True\n",
    "            )\n",
    "            transformed_columns = list(transformed_conditions.columns)\n",
    "            if not transformed_conditions.empty:\n",
    "                transformed_conditions.index = dataframe.index\n",
    "                transformed_conditions[COND_IDX] = dataframe[COND_IDX]\n",
    "\n",
    "            if len(transformed_columns) == 0:\n",
    "                sampled_rows = self._conditionally_sample_rows(\n",
    "                    dataframe=dataframe,\n",
    "                    condition=condition,\n",
    "                    transformed_condition=None,\n",
    "                    max_tries_per_batch=max_tries_per_batch,\n",
    "                    batch_size=batch_size,\n",
    "                    progress_bar=progress_bar,\n",
    "                    output_file_path=output_file_path,\n",
    "                )\n",
    "                all_sampled_rows.append(sampled_rows)\n",
    "            else:\n",
    "                transformed_groups = transformed_conditions.groupby(\n",
    "                    groupby_list(transformed_columns)\n",
    "                )\n",
    "                for transformed_group, transformed_dataframe in transformed_groups:\n",
    "                    if not isinstance(transformed_group, tuple):\n",
    "                        transformed_group = [transformed_group]\n",
    "\n",
    "                    transformed_condition = dict(zip(transformed_columns, transformed_group))\n",
    "                    sampled_rows = self._conditionally_sample_rows(\n",
    "                        dataframe=transformed_dataframe,\n",
    "                        condition=condition,\n",
    "                        transformed_condition=transformed_condition,\n",
    "                        max_tries_per_batch=max_tries_per_batch,\n",
    "                        batch_size=batch_size,\n",
    "                        progress_bar=progress_bar,\n",
    "                        output_file_path=output_file_path,\n",
    "                    )\n",
    "                    all_sampled_rows.append(sampled_rows)\n",
    "\n",
    "        all_sampled_rows = pd.concat(all_sampled_rows)\n",
    "        if len(all_sampled_rows) == 0:\n",
    "            return all_sampled_rows\n",
    "\n",
    "        all_sampled_rows = all_sampled_rows.set_index(COND_IDX)\n",
    "        all_sampled_rows.index.name = conditions.index.name\n",
    "        all_sampled_rows = all_sampled_rows.sort_index()\n",
    "\n",
    "        return all_sampled_rows\n",
    "\n",
    "    def sample_from_conditions(self, conditions, max_tries_per_batch=100,\n",
    "                               batch_size=None, output_file_path=None):\n",
    "        \"\"\"Sample rows from this table with the given conditions.\n",
    "\n",
    "        Args:\n",
    "            conditions (list[sdv.sampling.Condition]):\n",
    "                A list of sdv.sampling.Condition objects, which specify the column\n",
    "                values in a condition, along with the number of rows for that\n",
    "                condition.\n",
    "            max_tries_per_batch (int):\n",
    "                Number of times to retry sampling until the batch size is met. Defaults to 100.\n",
    "            batch_size (int):\n",
    "                The batch size to use per sampling call.\n",
    "            output_file_path (str or None):\n",
    "                The file to periodically write sampled rows to. Defaults to\n",
    "                a temporary file, if None.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Sampled data.\n",
    "\n",
    "        Raises:\n",
    "            ConstraintsNotMetError:\n",
    "                If the conditions are not valid for the given constraints.\n",
    "            ValueError:\n",
    "                If any of the following happens:\n",
    "                    * any of the conditions' columns are not valid.\n",
    "                    * no rows could be generated.\n",
    "        \"\"\"\n",
    "        output_file_path = validate_file_path(output_file_path)\n",
    "\n",
    "        num_rows = functools.reduce(\n",
    "            lambda num_rows, condition: condition.get_num_rows() + num_rows, conditions, 0)\n",
    "\n",
    "        conditions = self._make_condition_dfs(conditions)\n",
    "        for condition_dataframe in conditions:\n",
    "            self._validate_conditions(condition_dataframe)\n",
    "\n",
    "        sampled = pd.DataFrame()\n",
    "        try:\n",
    "            with tqdm1(total=num_rows) as progress_bar:\n",
    "                progress_bar.set_description('Sampling conditions')\n",
    "                for condition_dataframe in conditions:\n",
    "                    sampled_for_condition = self._sample_with_conditions(\n",
    "                        condition_dataframe,\n",
    "                        max_tries_per_batch,\n",
    "                        batch_size,\n",
    "                        progress_bar,\n",
    "                        output_file_path,\n",
    "                    )\n",
    "                    sampled = pd.concat([sampled, sampled_for_condition], ignore_index=True)\n",
    "\n",
    "            is_reject_sampling = bool(\n",
    "                hasattr(self, '_model') and not isinstance(self._model, GaussianMultivariate))\n",
    "            check_num_rows(\n",
    "                num_rows=len(sampled),\n",
    "                expected_num_rows=num_rows,\n",
    "                is_reject_sampling=is_reject_sampling,\n",
    "                max_tries_per_batch=max_tries_per_batch\n",
    "            )\n",
    "\n",
    "        except (Exception, KeyboardInterrupt) as error:\n",
    "            handle_sampling_error(output_file_path == TMP_FILE_NAME, output_file_path, error)\n",
    "\n",
    "        else:\n",
    "            if output_file_path == TMP_FILE_NAME and os.path.exists(output_file_path):\n",
    "                os.remove(output_file_path)\n",
    "\n",
    "        return sampled\n",
    "\n",
    "    def sample_remaining_columns(self, known_columns, max_tries_per_batch=100,\n",
    "                                 batch_size=None, output_file_path=None):\n",
    "        \"\"\"Sample remaining rows from already known columns.\n",
    "\n",
    "        Args:\n",
    "            known_columns (pandas.DataFrame):\n",
    "                A pandas.DataFrame with the columns that are already known. The output\n",
    "                is a DataFrame such that each row in the output is sampled\n",
    "                conditionally on the corresponding row in the input.\n",
    "            max_tries_per_batch (int):\n",
    "                Number of times to retry sampling until the batch size is met. Defaults to 100.\n",
    "            batch_size (int):\n",
    "                The batch size to use per sampling call.\n",
    "            output_file_path (str or None):\n",
    "                The file to periodically write sampled rows to. Defaults to\n",
    "                a temporary file, if None.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Sampled data.\n",
    "\n",
    "        Raises:\n",
    "            ConstraintsNotMetError:\n",
    "                If the conditions are not valid for the given constraints.\n",
    "            ValueError:\n",
    "                If any of the following happens:\n",
    "                    * any of the conditions' columns are not valid.\n",
    "                    * no rows could be generated.\n",
    "        \"\"\"\n",
    "        output_file_path = validate_file_path(output_file_path)\n",
    "\n",
    "        known_columns = known_columns.copy()\n",
    "        self._validate_conditions(known_columns)\n",
    "        sampled = pd.DataFrame()\n",
    "        try:\n",
    "            with tqdm1(total=len(known_columns)) as progress_bar:\n",
    "                progress_bar.set_description('Sampling remaining columns')\n",
    "                sampled = self._sample_with_conditions(\n",
    "                    known_columns, max_tries_per_batch, batch_size, progress_bar, output_file_path)\n",
    "\n",
    "            is_reject_sampling = (hasattr(self, '_model') and not isinstance(\n",
    "                self._model, copulas.multivariate.GaussianMultivariate))\n",
    "\n",
    "            check_num_rows(\n",
    "                num_rows=len(sampled),\n",
    "                expected_num_rows=len(known_columns),\n",
    "                is_reject_sampling=is_reject_sampling,\n",
    "                max_tries_per_batch=max_tries_per_batch\n",
    "            )\n",
    "\n",
    "        except (Exception, KeyboardInterrupt) as error:\n",
    "            handle_sampling_error(output_file_path == TMP_FILE_NAME, output_file_path, error)\n",
    "\n",
    "        else:\n",
    "            if output_file_path == TMP_FILE_NAME and os.path.exists(output_file_path):\n",
    "                os.remove(output_file_path)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTGAN DataSampler\n",
    "class DataSampler(object):\n",
    "    \"\"\"DataSampler samples the conditional vector and corresponding data for CTGAN.\"\"\"\n",
    "\n",
    "    def __init__(self, data, output_info, log_frequency):\n",
    "        self._data = data\n",
    "\n",
    "        def is_discrete_column(column_info):\n",
    "            return (len(column_info) == 1\n",
    "                    and column_info[0].activation_fn == 'softmax')\n",
    "\n",
    "        n_discrete_columns = sum(\n",
    "            [1 for column_info in output_info if is_discrete_column(column_info)])\n",
    "\n",
    "        self._discrete_column_matrix_st = np.zeros(\n",
    "            n_discrete_columns, dtype='int32')\n",
    "\n",
    "        # Store the row id for each category in each discrete column.\n",
    "        # For example _rid_by_cat_cols[a][b] is a list of all rows with the\n",
    "        # a-th discrete column equal value b.\n",
    "        self._rid_by_cat_cols = []\n",
    "\n",
    "        # Compute _rid_by_cat_cols\n",
    "        st = 0\n",
    "        for column_info in output_info:\n",
    "            if is_discrete_column(column_info):\n",
    "                span_info = column_info[0]\n",
    "                ed = st + span_info.dim\n",
    "\n",
    "                rid_by_cat = []\n",
    "                for j in range(span_info.dim):\n",
    "                    rid_by_cat.append(np.nonzero(data[:, st + j])[0])\n",
    "                self._rid_by_cat_cols.append(rid_by_cat)\n",
    "                st = ed\n",
    "            else:\n",
    "                st += sum([span_info.dim for span_info in column_info])\n",
    "        assert st == data.shape[1]\n",
    "\n",
    "        # Prepare an interval matrix for efficiently sample conditional vector\n",
    "        max_category = max([\n",
    "            column_info[0].dim\n",
    "            for column_info in output_info\n",
    "            if is_discrete_column(column_info)\n",
    "        ], default=0)\n",
    "\n",
    "        self._discrete_column_cond_st = np.zeros(n_discrete_columns, dtype='int32')\n",
    "        self._discrete_column_n_category = np.zeros(n_discrete_columns, dtype='int32')\n",
    "        self._discrete_column_category_prob = np.zeros((n_discrete_columns, max_category))\n",
    "        self._n_discrete_columns = n_discrete_columns\n",
    "        self._n_categories = sum([\n",
    "            column_info[0].dim\n",
    "            for column_info in output_info\n",
    "            if is_discrete_column(column_info)\n",
    "        ])\n",
    "\n",
    "        st = 0\n",
    "        current_id = 0\n",
    "        current_cond_st = 0\n",
    "        for column_info in output_info:\n",
    "            if is_discrete_column(column_info):\n",
    "                span_info = column_info[0]\n",
    "                ed = st + span_info.dim\n",
    "                category_freq = np.sum(data[:, st:ed], axis=0)\n",
    "                if log_frequency:\n",
    "                    category_freq = np.log(category_freq + 1)\n",
    "                category_prob = category_freq / np.sum(category_freq)\n",
    "                self._discrete_column_category_prob[current_id, :span_info.dim] = category_prob\n",
    "                self._discrete_column_cond_st[current_id] = current_cond_st\n",
    "                self._discrete_column_n_category[current_id] = span_info.dim\n",
    "                current_cond_st += span_info.dim\n",
    "                current_id += 1\n",
    "                st = ed\n",
    "            else:\n",
    "                st += sum([span_info.dim for span_info in column_info])\n",
    "\n",
    "    def _random_choice_prob_index(self, discrete_column_id):\n",
    "        probs = self._discrete_column_category_prob[discrete_column_id]\n",
    "        r = np.expand_dims(np.random.rand(probs.shape[0]), axis=1)\n",
    "        return (probs.cumsum(axis=1) > r).argmax(axis=1)\n",
    "\n",
    "    def sample_condvec(self, batch):\n",
    "        \"\"\"Generate the conditional vector for training.\n",
    "\n",
    "        Returns:\n",
    "            cond (batch x #categories):\n",
    "                The conditional vector.\n",
    "            mask (batch x #discrete columns):\n",
    "                A one-hot vector indicating the selected discrete column.\n",
    "            discrete column id (batch):\n",
    "                Integer representation of mask.\n",
    "            category_id_in_col (batch):\n",
    "                Selected category in the selected discrete column.\n",
    "        \"\"\"\n",
    "        if self._n_discrete_columns == 0:\n",
    "            return None\n",
    "\n",
    "        discrete_column_id = np.random.choice(\n",
    "            np.arange(self._n_discrete_columns), batch)\n",
    "\n",
    "        cond = np.zeros((batch, self._n_categories), dtype='float32')\n",
    "        mask = np.zeros((batch, self._n_discrete_columns), dtype='float32')\n",
    "        mask[np.arange(batch), discrete_column_id] = 1\n",
    "        category_id_in_col = self._random_choice_prob_index(discrete_column_id)\n",
    "        category_id = (self._discrete_column_cond_st[discrete_column_id] + category_id_in_col)\n",
    "        cond[np.arange(batch), category_id] = 1\n",
    "\n",
    "        return cond, mask, discrete_column_id, category_id_in_col\n",
    "\n",
    "    def sample_original_condvec(self, batch):\n",
    "        \"\"\"Generate the conditional vector for generation use original frequency.\"\"\"\n",
    "        if self._n_discrete_columns == 0:\n",
    "            return None\n",
    "\n",
    "        cond = np.zeros((batch, self._n_categories), dtype='float32')\n",
    "\n",
    "        for i in range(batch):\n",
    "            row_idx = np.random.randint(0, len(self._data))\n",
    "            col_idx = np.random.randint(0, self._n_discrete_columns)\n",
    "            matrix_st = self._discrete_column_matrix_st[col_idx]\n",
    "            matrix_ed = matrix_st + self._discrete_column_n_category[col_idx]\n",
    "            pick = np.argmax(self._data[row_idx, matrix_st:matrix_ed])\n",
    "            cond[i, pick + self._discrete_column_cond_st[col_idx]] = 1\n",
    "\n",
    "        return cond\n",
    "\n",
    "    def sample_data(self, n, col, opt):\n",
    "        \"\"\"Sample data from original training data satisfying the sampled conditional vector.\n",
    "\n",
    "        Returns:\n",
    "            n rows of matrix data.\n",
    "        \"\"\"\n",
    "        if col is None:\n",
    "            idx = np.random.randint(len(self._data), size=n)\n",
    "            return self._data[idx]\n",
    "\n",
    "        idx = []\n",
    "        for c, o in zip(col, opt):\n",
    "            idx.append(np.random.choice(self._rid_by_cat_cols[c][o]))\n",
    "\n",
    "        return self._data[idx]\n",
    "\n",
    "    def dim_cond_vec(self):\n",
    "        \"\"\"Return the total number of categories.\"\"\"\n",
    "        return self._n_categories\n",
    "\n",
    "    def generate_cond_from_condition_column_info(self, condition_info, batch):\n",
    "        \"\"\"Generate the condition vector.\"\"\"\n",
    "        vec = np.zeros((batch, self._n_categories), dtype='float32')\n",
    "        id_ = self._discrete_column_matrix_st[condition_info['discrete_column_id']]\n",
    "        id_ += condition_info['value_id']\n",
    "        vec[:, id_] = 1\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTGAN DataTransformer\n",
    "SpanInfo = namedtuple('SpanInfo', ['dim', 'activation_fn'])\n",
    "ColumnTransformInfo = namedtuple(\n",
    "    'ColumnTransformInfo', [\n",
    "        'column_name', 'column_type', 'transform', 'output_info', 'output_dimensions'\n",
    "    ]\n",
    ")\n",
    "class DataTransformer(object):\n",
    "    \"\"\"Data Transformer.\n",
    "\n",
    "    Model continuous columns with a BayesianGMM and normalize them to a scalar between [-1, 1]\n",
    "    and a vector. Discrete columns are encoded using a OneHotEncoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_clusters=10, weight_threshold=0.005):\n",
    "        \"\"\"Create a data transformer.\n",
    "\n",
    "        Args:\n",
    "            max_clusters (int):\n",
    "                Maximum number of Gaussian distributions in Bayesian GMM.\n",
    "            weight_threshold (float):\n",
    "                Weight threshold for a Gaussian distribution to be kept.\n",
    "        \"\"\"\n",
    "        self._max_clusters = max_clusters\n",
    "        self._weight_threshold = weight_threshold\n",
    "\n",
    "    def _fit_continuous(self, data):\n",
    "        \"\"\"Train Bayesian GMM for continuous columns.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame):\n",
    "                A dataframe containing a column.\n",
    "\n",
    "        Returns:\n",
    "            namedtuple:\n",
    "                A ``ColumnTransformInfo`` object.\n",
    "        \"\"\"\n",
    "        column_name = data.columns[0]\n",
    "        gm = ClusterBasedNormalizer(\n",
    "            missing_value_generation='from_column',\n",
    "            max_clusters=min(len(data), self._max_clusters),\n",
    "            weight_threshold=self._weight_threshold\n",
    "        )\n",
    "        gm.fit(data, column_name)\n",
    "        num_components = sum(gm.valid_component_indicator)\n",
    "\n",
    "        return ColumnTransformInfo(\n",
    "            column_name=column_name, column_type='continuous', transform=gm,\n",
    "            output_info=[SpanInfo(1, 'tanh'), SpanInfo(num_components, 'softmax')],\n",
    "            output_dimensions=1 + num_components)\n",
    "\n",
    "    def _fit_discrete(self, data):\n",
    "        \"\"\"Fit one hot encoder for discrete column.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame):\n",
    "                A dataframe containing a column.\n",
    "\n",
    "        Returns:\n",
    "            namedtuple:\n",
    "                A ``ColumnTransformInfo`` object.\n",
    "        \"\"\"\n",
    "        column_name = data.columns[0]\n",
    "        ohe = OneHotEncoder()\n",
    "        ohe.fit(data, column_name)\n",
    "        num_categories = len(ohe.dummies)\n",
    "\n",
    "        return ColumnTransformInfo(\n",
    "            column_name=column_name, column_type='discrete', transform=ohe,\n",
    "            output_info=[SpanInfo(num_categories, 'softmax')],\n",
    "            output_dimensions=num_categories)\n",
    "\n",
    "    def fit(self, raw_data, discrete_columns=()):\n",
    "        \"\"\"Fit the ``DataTransformer``.\n",
    "\n",
    "        Fits a ``ClusterBasedNormalizer`` for continuous columns and a\n",
    "        ``OneHotEncoder`` for discrete columns.\n",
    "\n",
    "        This step also counts the #columns in matrix data and span information.\n",
    "        \"\"\"\n",
    "        self.output_info_list = []\n",
    "        self.output_dimensions = 0\n",
    "        self.dataframe = True\n",
    "\n",
    "        if not isinstance(raw_data, pd.DataFrame):\n",
    "            self.dataframe = False\n",
    "            # work around for RDT issue #328 Fitting with numerical column names fails\n",
    "            discrete_columns = [str(column) for column in discrete_columns]\n",
    "            column_names = [str(num) for num in range(raw_data.shape[1])]\n",
    "            raw_data = pd.DataFrame(raw_data, columns=column_names)\n",
    "\n",
    "        self._column_raw_dtypes = raw_data.infer_objects().dtypes\n",
    "        self._column_transform_info_list = []\n",
    "        for column_name in raw_data.columns:\n",
    "            if column_name in discrete_columns:\n",
    "                column_transform_info = self._fit_discrete(raw_data[[column_name]])\n",
    "            else:\n",
    "                column_transform_info = self._fit_continuous(raw_data[[column_name]])\n",
    "\n",
    "            self.output_info_list.append(column_transform_info.output_info)\n",
    "            self.output_dimensions += column_transform_info.output_dimensions\n",
    "            self._column_transform_info_list.append(column_transform_info)\n",
    "\n",
    "    def _transform_continuous(self, column_transform_info, data):\n",
    "        column_name = data.columns[0]\n",
    "        flattened_column = data[column_name].to_numpy().flatten()\n",
    "        data = data.assign(**{column_name: flattened_column})\n",
    "        gm = column_transform_info.transform\n",
    "        transformed = gm.transform(data)\n",
    "\n",
    "        #  Converts the transformed data to the appropriate output format.\n",
    "        #  The first column (ending in '.normalized') stays the same,\n",
    "        #  but the lable encoded column (ending in '.component') is one hot encoded.\n",
    "        output = np.zeros((len(transformed), column_transform_info.output_dimensions))\n",
    "        output[:, 0] = transformed[f'{column_name}.normalized'].to_numpy()\n",
    "        index = transformed[f'{column_name}.component'].to_numpy().astype(int)\n",
    "        output[np.arange(index.size), index + 1] = 1.0\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _transform_discrete(self, column_transform_info, data):\n",
    "        ohe = column_transform_info.transform\n",
    "        return ohe.transform(data).to_numpy()\n",
    "\n",
    "    def _synchronous_transform(self, raw_data, column_transform_info_list):\n",
    "        \"\"\"Take a Pandas DataFrame and transform columns synchronous.\n",
    "\n",
    "        Outputs a list with Numpy arrays.\n",
    "        \"\"\"\n",
    "        column_data_list = []\n",
    "        for column_transform_info in column_transform_info_list:\n",
    "            column_name = column_transform_info.column_name\n",
    "            data = raw_data[[column_name]]\n",
    "            if column_transform_info.column_type == 'continuous':\n",
    "                column_data_list.append(self._transform_continuous(column_transform_info, data))\n",
    "            else:\n",
    "                column_data_list.append(self._transform_discrete(column_transform_info, data))\n",
    "\n",
    "        return column_data_list\n",
    "\n",
    "    def _parallel_transform(self, raw_data, column_transform_info_list):\n",
    "        \"\"\"Take a Pandas DataFrame and transform columns in parallel.\n",
    "\n",
    "        Outputs a list with Numpy arrays.\n",
    "        \"\"\"\n",
    "        processes = []\n",
    "        for column_transform_info in column_transform_info_list:\n",
    "            column_name = column_transform_info.column_name\n",
    "            data = raw_data[[column_name]]\n",
    "            process = None\n",
    "            if column_transform_info.column_type == 'continuous':\n",
    "                process = delayed(self._transform_continuous)(column_transform_info, data)\n",
    "            else:\n",
    "                process = delayed(self._transform_discrete)(column_transform_info, data)\n",
    "            processes.append(process)\n",
    "\n",
    "        return Parallel(n_jobs=-1)(processes)\n",
    "\n",
    "    def transform(self, raw_data):\n",
    "        \"\"\"Take raw data and output a matrix data.\"\"\"\n",
    "        if not isinstance(raw_data, pd.DataFrame):\n",
    "            column_names = [str(num) for num in range(raw_data.shape[1])]\n",
    "            raw_data = pd.DataFrame(raw_data, columns=column_names)\n",
    "\n",
    "        # Only use parallelization with larger data sizes.\n",
    "        # Otherwise, the transformation will be slower.\n",
    "        if raw_data.shape[0] < 500:\n",
    "            column_data_list = self._synchronous_transform(\n",
    "                raw_data,\n",
    "                self._column_transform_info_list\n",
    "            )\n",
    "        else:\n",
    "            column_data_list = self._parallel_transform(\n",
    "                raw_data,\n",
    "                self._column_transform_info_list\n",
    "            )\n",
    "\n",
    "        return np.concatenate(column_data_list, axis=1).astype(float)\n",
    "\n",
    "    def _inverse_transform_continuous(self, column_transform_info, column_data, sigmas, st):\n",
    "        gm = column_transform_info.transform\n",
    "        data = pd.DataFrame(column_data[:, :2], columns=list(gm.get_output_sdtypes()))\n",
    "        data[data.columns[1]] = np.argmax(column_data[:, 1:], axis=1)\n",
    "        if sigmas is not None:\n",
    "            selected_normalized_value = np.random.normal(data.iloc[:, 0], sigmas[st])\n",
    "            data.iloc[:, 0] = selected_normalized_value\n",
    "\n",
    "        return gm.reverse_transform(data)\n",
    "\n",
    "    def _inverse_transform_discrete(self, column_transform_info, column_data):\n",
    "        ohe = column_transform_info.transform\n",
    "        data = pd.DataFrame(column_data, columns=list(ohe.get_output_sdtypes()))\n",
    "        return ohe.reverse_transform(data)[column_transform_info.column_name]\n",
    "\n",
    "    def inverse_transform(self, data, sigmas=None):\n",
    "        \"\"\"Take matrix data and output raw data.\n",
    "\n",
    "        Output uses the same type as input to the transform function.\n",
    "        Either np array or pd dataframe.\n",
    "        \"\"\"\n",
    "        st = 0\n",
    "        recovered_column_data_list = []\n",
    "        column_names = []\n",
    "        for column_transform_info in self._column_transform_info_list:\n",
    "            dim = column_transform_info.output_dimensions\n",
    "            column_data = data[:, st:st + dim]\n",
    "            if column_transform_info.column_type == 'continuous':\n",
    "                recovered_column_data = self._inverse_transform_continuous(\n",
    "                    column_transform_info, column_data, sigmas, st)\n",
    "            else:\n",
    "                recovered_column_data = self._inverse_transform_discrete(\n",
    "                    column_transform_info, column_data)\n",
    "\n",
    "            recovered_column_data_list.append(recovered_column_data)\n",
    "            column_names.append(column_transform_info.column_name)\n",
    "            st += dim\n",
    "\n",
    "        recovered_data = np.column_stack(recovered_column_data_list)\n",
    "        recovered_data = (pd.DataFrame(recovered_data, columns=column_names)\n",
    "                          .astype(self._column_raw_dtypes))\n",
    "        if not self.dataframe:\n",
    "            recovered_data = recovered_data.to_numpy()\n",
    "\n",
    "        return recovered_data\n",
    "\n",
    "    def convert_column_name_value_to_id(self, column_name, value):\n",
    "        \"\"\"Get the ids of the given `column_name`.\"\"\"\n",
    "        discrete_counter = 0\n",
    "        column_id = 0\n",
    "        for column_transform_info in self._column_transform_info_list:\n",
    "            if column_transform_info.column_name == column_name:\n",
    "                break\n",
    "            if column_transform_info.column_type == 'discrete':\n",
    "                discrete_counter += 1\n",
    "\n",
    "            column_id += 1\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"The column_name `{column_name}` doesn't exist in the data.\")\n",
    "\n",
    "        ohe = column_transform_info.transform\n",
    "        data = pd.DataFrame([value], columns=[column_transform_info.column_name])\n",
    "        one_hot = ohe.transform(data).to_numpy()[0]\n",
    "        if sum(one_hot) == 0:\n",
    "            raise ValueError(f\"The value `{value}` doesn't exist in the column `{column_name}`.\")\n",
    "\n",
    "        return {\n",
    "            'discrete_column_id': discrete_counter,\n",
    "            'column_id': column_id,\n",
    "            'value_id': np.argmax(one_hot)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator and Generator model\n",
    "class Discriminator(Module):\n",
    "    \"\"\"Discriminator for the CTGAN.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, discriminator_dim, pac=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        dim = input_dim * pac\n",
    "        self.pac = pac\n",
    "        self.pacdim = dim\n",
    "        seq = []\n",
    "        for item in list(discriminator_dim):\n",
    "            seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]\n",
    "            dim = item\n",
    "\n",
    "        seq += [Linear(dim, 1)]\n",
    "        self.seq = Sequential(*seq)\n",
    "\n",
    "    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n",
    "        \"\"\"Compute the gradient penalty.\"\"\"\n",
    "        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n",
    "        alpha = alpha.repeat(1, pac, real_data.size(1))\n",
    "        alpha = alpha.view(-1, real_data.size(1))\n",
    "\n",
    "        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "        disc_interpolates = self(interpolates)\n",
    "\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=disc_interpolates, inputs=interpolates,\n",
    "            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n",
    "            create_graph=True, retain_graph=True, only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        gradients_view = gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n",
    "        gradient_penalty = ((gradients_view) ** 2).mean() * lambda_\n",
    "\n",
    "        return gradient_penalty\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"Apply the Discriminator to the `input_`.\"\"\"\n",
    "        assert input_.size()[0] % self.pac == 0\n",
    "        return self.seq(input_.view(-1, self.pacdim))\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, i, o):\n",
    "        super(Residual, self).__init__()\n",
    "        self.fc = Linear(i, o)\n",
    "        self.bn = BatchNorm1d(o)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.fc(input)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        return torch.cat([out, input], dim=1)\n",
    "\n",
    "class Generator(Module):\n",
    "    def __init__(self, embedding_dim, gen_dims, data_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        dim = embedding_dim\n",
    "        seq = []\n",
    "        for item in list(gen_dims):\n",
    "            seq += [\n",
    "                Residual(dim, item)\n",
    "            ]\n",
    "            dim += item\n",
    "        seq.append(Linear(dim, data_dim))\n",
    "        self.seq = Sequential(*seq)\n",
    "\n",
    "    def forward(self, input):\n",
    "        data = self.seq(input)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original CTGANBaseSynthesizer\n",
    "class CTGANBaseSynthesizer:\n",
    "    \"\"\"Base class for all default synthesizers of ``CTGAN``.\"\"\"\n",
    "\n",
    "    random_states = None\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\"Improve pickling state for ``BaseSynthesizer``.\n",
    "\n",
    "        Convert to ``cpu`` device before starting the pickling process in order to be able to\n",
    "        load the model even when used from an external tool such as ``SDV``. Also, if\n",
    "        ``random_states`` are set, store their states as dictionaries rather than generators.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                Python dict representing the object.\n",
    "        \"\"\"\n",
    "        device_backup = self._device\n",
    "        self.set_device(torch.device('cpu'))\n",
    "        state = self.__dict__.copy()\n",
    "        self.set_device(device_backup)\n",
    "        if (\n",
    "            isinstance(self.random_states, tuple) and\n",
    "            isinstance(self.random_states[0], np.random.RandomState) and\n",
    "            isinstance(self.random_states[1], torch.Generator)\n",
    "        ):\n",
    "            state['_numpy_random_state'] = self.random_states[0].get_state()\n",
    "            state['_torch_random_state'] = self.random_states[1].get_state()\n",
    "            state.pop('random_states')\n",
    "\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        \"\"\"Restore the state of a ``BaseSynthesizer``.\n",
    "\n",
    "        Restore the ``random_states`` from the state dict if those are present and then\n",
    "        set the device according to the current hardware.\n",
    "        \"\"\"\n",
    "        if '_numpy_random_state' in state and '_torch_random_state' in state:\n",
    "            np_state = state.pop('_numpy_random_state')\n",
    "            torch_state = state.pop('_torch_random_state')\n",
    "\n",
    "            current_torch_state = torch.Generator()\n",
    "            current_torch_state.set_state(torch_state)\n",
    "\n",
    "            current_numpy_state = np.random.RandomState()\n",
    "            current_numpy_state.set_state(np_state)\n",
    "            state['random_states'] = (\n",
    "                current_numpy_state,\n",
    "                current_torch_state\n",
    "            )\n",
    "\n",
    "        self.__dict__ = state\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.set_device(device)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save the model in the passed `path`.\"\"\"\n",
    "        device_backup = self._device\n",
    "        self.set_device(torch.device('cpu'))\n",
    "        torch.save(self, path)\n",
    "        self.set_device(device_backup)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load the model stored in the passed `path`.\"\"\"\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model = torch.load(path)\n",
    "        model.set_device(device)\n",
    "        return model\n",
    "\n",
    "    def set_random_state(self, random_state):\n",
    "        \"\"\"Set the random state.\n",
    "\n",
    "        Args:\n",
    "            random_state (int, tuple, or None):\n",
    "                Either a tuple containing the (numpy.random.RandomState, torch.Generator)\n",
    "                or an int representing the random seed to use for both random states.\n",
    "        \"\"\"\n",
    "        if random_state is None:\n",
    "            self.random_states = random_state\n",
    "        elif isinstance(random_state, int):\n",
    "            self.random_states = (\n",
    "                np.random.RandomState(seed=random_state),\n",
    "                torch.Generator().manual_seed(random_state),\n",
    "            )\n",
    "        elif (\n",
    "            isinstance(random_state, tuple) and\n",
    "            isinstance(random_state[0], np.random.RandomState) and\n",
    "            isinstance(random_state[1], torch.Generator)\n",
    "        ):\n",
    "            self.random_states = random_state\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f'`random_state` {random_state} expected to be an int or a tuple of '\n",
    "                '(`np.random.RandomState`, `torch.Generator`)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an autoencoder to generate input of Generator\n",
    "def fit_encoder(train_data, valid_data, embedding_dim):\n",
    "    attr_size = train_data.shape[1]\n",
    "    data_in = Input(shape=(attr_size,))\n",
    "    encoded = Dense(embedding_dim, activation='tanh')(data_in)\n",
    "    decoded = Dense(attr_size, activation='tanh')(encoded)\n",
    "    autoencoder = Model(data_in, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    autoencoder.fit(train_data, train_data, epochs=50, batch_size=128, validation_data=(valid_data, valid_data), verbose=0)\n",
    "    encoder_model = Model(data_in, encoded)\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoderCTGAN method that uses the encoded values as input of CTGAN\n",
    "class encoderCTGAN(CTGANBaseSynthesizer):\n",
    "    \"\"\"Conditional Table GAN Synthesizer.\n",
    "\n",
    "    This is the core class of the CTGAN project, where the different components\n",
    "    are orchestrated together.\n",
    "    For more details about the process, please check the [Modeling Tabular data using\n",
    "    Conditional GAN](https://arxiv.org/abs/1907.00503) paper.\n",
    "\n",
    "    Args:\n",
    "        embedding_dim (int):\n",
    "            Size of the random sample passed to the Generator. Defaults to 128.\n",
    "        generator_dim (tuple or list of ints):\n",
    "            Size of the output samples for each one of the Residuals. A Residual Layer\n",
    "            will be created for each one of the values provided. Defaults to (256, 256).\n",
    "        discriminator_dim (tuple or list of ints):\n",
    "            Size of the output samples for each one of the Discriminator Layers. A Linear Layer\n",
    "            will be created for each one of the values provided. Defaults to (256, 256).\n",
    "        generator_lr (float):\n",
    "            Learning rate for the generator. Defaults to 2e-4.\n",
    "        generator_decay (float):\n",
    "            Generator weight decay for the Adam Optimizer. Defaults to 1e-6.\n",
    "        discriminator_lr (float):\n",
    "            Learning rate for the discriminator. Defaults to 2e-4.\n",
    "        discriminator_decay (float):\n",
    "            Discriminator weight decay for the Adam Optimizer. Defaults to 1e-6.\n",
    "        batch_size (int):\n",
    "            Number of data samples to process in each step.\n",
    "        discriminator_steps (int):\n",
    "            Number of discriminator updates to do for each generator update.\n",
    "            From the WGAN paper: https://arxiv.org/abs/1701.07875. WGAN paper\n",
    "            default is 5. Default used is 1 to match original CTGAN implementation.\n",
    "        log_frequency (boolean):\n",
    "            Whether to use log frequency of categorical levels in conditional\n",
    "            sampling. Defaults to ``True``.\n",
    "        verbose (boolean):\n",
    "            Whether to have print statements for progress results. Defaults to ``False``.\n",
    "        epochs (int):\n",
    "            Number of training epochs. Defaults to 300.\n",
    "        pac (int):\n",
    "            Number of samples to group together when applying the discriminator.\n",
    "            Defaults to 10.\n",
    "        cuda (bool):\n",
    "            Whether to attempt to use cuda for GPU computation.\n",
    "            If this is False or CUDA is not available, CPU will be used.\n",
    "            Defaults to ``True``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=128, generator_dim=(256, 256), discriminator_dim=(256, 256),\n",
    "                 generator_lr=2e-4, generator_decay=1e-6, discriminator_lr=2e-4,\n",
    "                 discriminator_decay=1e-6, batch_size=500, discriminator_steps=1,\n",
    "                 log_frequency=True, verbose=False, epochs=300, pac=10, cuda=True):\n",
    "\n",
    "        assert batch_size % 2 == 0\n",
    "\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._generator_dim = generator_dim\n",
    "        self._discriminator_dim = discriminator_dim\n",
    "\n",
    "        self._generator_lr = generator_lr\n",
    "        self._generator_decay = generator_decay\n",
    "        self._discriminator_lr = discriminator_lr\n",
    "        self._discriminator_decay = discriminator_decay\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._discriminator_steps = discriminator_steps\n",
    "        self._log_frequency = log_frequency\n",
    "        self._verbose = verbose\n",
    "        self._epochs = epochs\n",
    "        self.pac = pac\n",
    "\n",
    "        if not cuda or not torch.cuda.is_available():\n",
    "            device = 'cpu'\n",
    "        elif isinstance(cuda, str):\n",
    "            device = cuda\n",
    "        else:\n",
    "            device = 'cuda'\n",
    "\n",
    "        self._device = torch.device(device)\n",
    "\n",
    "        self._transformer = None\n",
    "        self._data_sampler = None\n",
    "        self._generator = None\n",
    "\n",
    "        self.loss_values = pd.DataFrame(columns=['Epoch', 'Generator Loss', 'Distriminator Loss'])\n",
    "\n",
    "    @staticmethod\n",
    "    def _gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):\n",
    "        \"\"\"Deals with the instability of the gumbel_softmax for older versions of torch.\n",
    "\n",
    "        For more details about the issue:\n",
    "        https://drive.google.com/file/d/1AA5wPfZ1kquaRtVruCd6BiYZGcDeNxyP/view?usp=sharing\n",
    "\n",
    "        Args:\n",
    "            logits [, num_features]:\n",
    "                Unnormalized log probabilities\n",
    "            tau:\n",
    "                Non-negative scalar temperature\n",
    "            hard (bool):\n",
    "                If True, the returned samples will be discretized as one-hot vectors,\n",
    "                but will be differentiated as if it is the soft sample in autograd\n",
    "            dim (int):\n",
    "                A dimension along which softmax will be computed. Default: -1.\n",
    "\n",
    "        Returns:\n",
    "            Sampled tensor of same shape as logits from the Gumbel-Softmax distribution.\n",
    "        \"\"\"\n",
    "        for _ in range(10):\n",
    "            transformed = functional.gumbel_softmax(logits, tau=tau, hard=hard, eps=eps, dim=dim)\n",
    "            if not torch.isnan(transformed).any():\n",
    "                return transformed\n",
    "\n",
    "        raise ValueError('gumbel_softmax returning NaN.')\n",
    "\n",
    "    def _apply_activate(self, data):\n",
    "        \"\"\"Apply proper activation function to the output of the generator.\"\"\"\n",
    "        data_t = []\n",
    "        st = 0\n",
    "        for column_info in self._transformer.output_info_list:\n",
    "            for span_info in column_info:\n",
    "                if span_info.activation_fn == 'tanh':\n",
    "                    ed = st + span_info.dim\n",
    "                    data_t.append(torch.tanh(data[:, st:ed]))\n",
    "                    st = ed\n",
    "                elif span_info.activation_fn == 'softmax':\n",
    "                    ed = st + span_info.dim\n",
    "                    transformed = self._gumbel_softmax(data[:, st:ed], tau=0.2)\n",
    "                    data_t.append(transformed)\n",
    "                    st = ed\n",
    "                else:\n",
    "                    raise ValueError(f'Unexpected activation function {span_info.activation_fn}.')\n",
    "\n",
    "        return torch.cat(data_t, dim=1)\n",
    "\n",
    "    def _cond_loss(self, data, c, m):\n",
    "        \"\"\"Compute the cross entropy loss on the fixed discrete column.\"\"\"\n",
    "        loss = []\n",
    "        st = 0\n",
    "        st_c = 0\n",
    "        for column_info in self._transformer.output_info_list:\n",
    "            for span_info in column_info:\n",
    "                if len(column_info) != 1 or span_info.activation_fn != 'softmax':\n",
    "                    # not discrete column\n",
    "                    st += span_info.dim\n",
    "                else:\n",
    "                    ed = st + span_info.dim\n",
    "                    ed_c = st_c + span_info.dim\n",
    "                    tmp = functional.cross_entropy(\n",
    "                        data[:, st:ed],\n",
    "                        torch.argmax(c[:, st_c:ed_c], dim=1),\n",
    "                        reduction='none'\n",
    "                    )\n",
    "                    loss.append(tmp)\n",
    "                    st = ed\n",
    "                    st_c = ed_c\n",
    "\n",
    "        loss = torch.stack(loss, dim=1)  # noqa: PD013\n",
    "\n",
    "        return (loss * m).sum() / data.size()[0]\n",
    "\n",
    "    def _validate_discrete_columns(self, train_data, discrete_columns):\n",
    "        \"\"\"Check whether ``discrete_columns`` exists in ``train_data``.\n",
    "\n",
    "        Args:\n",
    "            train_data (numpy.ndarray or pandas.DataFrame):\n",
    "                Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.\n",
    "            discrete_columns (list-like):\n",
    "                List of discrete columns to be used to generate the Conditional\n",
    "                Vector. If ``train_data`` is a Numpy array, this list should\n",
    "                contain the integer indices of the columns. Otherwise, if it is\n",
    "                a ``pandas.DataFrame``, this list should contain the column names.\n",
    "        \"\"\"\n",
    "        if isinstance(train_data, pd.DataFrame):\n",
    "            invalid_columns = set(discrete_columns) - set(train_data.columns)\n",
    "        elif isinstance(train_data, np.ndarray):\n",
    "            invalid_columns = []\n",
    "            for column in discrete_columns:\n",
    "                if column < 0 or column >= train_data.shape[1]:\n",
    "                    invalid_columns.append(column)\n",
    "        else:\n",
    "            raise TypeError('``train_data`` should be either pd.DataFrame or np.array.')\n",
    "\n",
    "        if invalid_columns:\n",
    "            raise ValueError(f'Invalid columns found: {invalid_columns}')\n",
    "\n",
    "    @random_state\n",
    "    def fit(self, train_data, discrete_columns=(), epochs=None):\n",
    "        \"\"\"Fit the CTGAN Synthesizer models to the training data.\n",
    "\n",
    "        Args:\n",
    "            train_data (numpy.ndarray or pandas.DataFrame):\n",
    "                Training Data. It must be a 2-dimensional numpy array or a pandas.DataFrame.\n",
    "            discrete_columns (list-like):\n",
    "                List of discrete columns to be used to generate the Conditional\n",
    "                Vector. If ``train_data`` is a Numpy array, this list should\n",
    "                contain the integer indices of the columns. Otherwise, if it is\n",
    "                a ``pandas.DataFrame``, this list should contain the column names.\n",
    "        \"\"\"\n",
    "        self._validate_discrete_columns(train_data, discrete_columns)\n",
    "\n",
    "        if epochs is None:\n",
    "            epochs = self._epochs\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                ('`epochs` argument in `fit` method has been deprecated and will be removed '\n",
    "                 'in a future version. Please pass `epochs` to the constructor instead'),\n",
    "                DeprecationWarning\n",
    "            )\n",
    "\n",
    "        self._transformer = DataTransformer()\n",
    "        self._transformer.fit(train_data, discrete_columns)\n",
    "\n",
    "        train_data = self._transformer.transform(train_data)\n",
    "        tr, te = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "        self.encoder = fit_encoder(tr, te, self._embedding_dim)\n",
    "\n",
    "        self._data_sampler = DataSampler(\n",
    "            train_data,\n",
    "            self._transformer.output_info_list,\n",
    "            self._log_frequency)\n",
    "\n",
    "        data_dim = self._transformer.output_dimensions\n",
    "\n",
    "        self._generator = Generator(\n",
    "            self._embedding_dim + self._data_sampler.dim_cond_vec(),\n",
    "            self._generator_dim,\n",
    "            data_dim\n",
    "        ).to(self._device)\n",
    "\n",
    "        discriminator = Discriminator(\n",
    "            data_dim + self._data_sampler.dim_cond_vec(),\n",
    "            self._discriminator_dim,\n",
    "            pac=self.pac\n",
    "        ).to(self._device)\n",
    "\n",
    "        optimizerG = optim.Adam(\n",
    "            self._generator.parameters(), lr=self._generator_lr, betas=(0.5, 0.9),\n",
    "            weight_decay=self._generator_decay\n",
    "        )\n",
    "\n",
    "        optimizerD = optim.Adam(\n",
    "            discriminator.parameters(), lr=self._discriminator_lr,\n",
    "            betas=(0.5, 0.9), weight_decay=self._discriminator_decay\n",
    "        )\n",
    "\n",
    "        mean = torch.zeros(self._batch_size, self._embedding_dim, device=self._device)\n",
    "        std = mean + 1\n",
    "\n",
    "        self.loss_values = pd.DataFrame(columns=['Epoch', 'Generator Loss', 'Distriminator Loss'])\n",
    "\n",
    "        epoch_iterator = tqdm2(range(epochs), disable=(not self._verbose))\n",
    "        if self._verbose:\n",
    "            description = 'Gen. ({gen:.2f}) | Discrim. ({dis:.2f})'\n",
    "            epoch_iterator.set_description(description.format(gen=0, dis=0))\n",
    "\n",
    "        steps_per_epoch = max(len(train_data) // self._batch_size, 1)\n",
    "        for i in epoch_iterator:\n",
    "            for id_ in range(steps_per_epoch):\n",
    "\n",
    "                for n in range(self._discriminator_steps):\n",
    "                    # fakez = torch.normal(mean=mean, std=std)\n",
    "\n",
    "                    condvec = self._data_sampler.sample_condvec(self._batch_size)\n",
    "                    if condvec is None:\n",
    "                        c1, m1, col, opt = None, None, None, None\n",
    "                        real = self._data_sampler.sample_data(self._batch_size, col, opt)\n",
    "                        s1 = self._data_sampler.sample_data(self._batch_size, col, opt)\n",
    "                        fakez = self.encoder.predict(s1)\n",
    "                        fakez = torch.tensor(fakez)\n",
    "                    else:\n",
    "                        c1, m1, col, opt = condvec\n",
    "                        c1 = torch.from_numpy(c1).to(self._device)\n",
    "                        m1 = torch.from_numpy(m1).to(self._device)\n",
    "                        # fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "                        perm = np.arange(self._batch_size)\n",
    "                        np.random.shuffle(perm)\n",
    "                        real = self._data_sampler.sample_data(\n",
    "                            self._batch_size, col[perm], opt[perm])\n",
    "                        s1 = self._data_sampler.sample_data(self._batch_size, col[perm], opt[perm])\n",
    "                        fakez = self.encoder.predict(s1)\n",
    "                        fakez = torch.tensor(fakez)\n",
    "                        fakez = torch.cat([fakez, c1], dim=1)\n",
    "                        c2 = c1[perm]\n",
    "\n",
    "                    fake = self._generator(fakez)\n",
    "                    fakeact = self._apply_activate(fake)\n",
    "\n",
    "                    real = torch.from_numpy(real.astype('float32')).to(self._device)\n",
    "\n",
    "                    if c1 is not None:\n",
    "                        fake_cat = torch.cat([fakeact, c1], dim=1)\n",
    "                        real_cat = torch.cat([real, c2], dim=1)\n",
    "                    else:\n",
    "                        real_cat = real\n",
    "                        fake_cat = fakeact\n",
    "\n",
    "                    y_fake = discriminator(fake_cat)\n",
    "                    y_real = discriminator(real_cat)\n",
    "\n",
    "                    pen = discriminator.calc_gradient_penalty(\n",
    "                        real_cat, fake_cat, self._device, self.pac)\n",
    "                    loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n",
    "\n",
    "                    optimizerD.zero_grad(set_to_none=False)\n",
    "                    pen.backward(retain_graph=True)\n",
    "                    loss_d.backward()\n",
    "                    optimizerD.step()\n",
    "\n",
    "                # fakez = torch.normal(mean=mean, std=std)\n",
    "                s2 = self._data_sampler.sample_data(self._batch_size, col, opt)\n",
    "                fakez = self.encoder.predict(s2)\n",
    "                fakez = torch.tensor(fakez)\n",
    "                condvec = self._data_sampler.sample_condvec(self._batch_size)\n",
    "\n",
    "                if condvec is None:\n",
    "                    c1, m1, col, opt = None, None, None, None\n",
    "                else:\n",
    "                    c1, m1, col, opt = condvec\n",
    "                    c1 = torch.from_numpy(c1).to(self._device)\n",
    "                    m1 = torch.from_numpy(m1).to(self._device)\n",
    "                    fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "                fake = self._generator(fakez)\n",
    "                fakeact = self._apply_activate(fake)\n",
    "\n",
    "                if c1 is not None:\n",
    "                    y_fake = discriminator(torch.cat([fakeact, c1], dim=1))\n",
    "                else:\n",
    "                    y_fake = discriminator(fakeact)\n",
    "\n",
    "                if condvec is None:\n",
    "                    cross_entropy = 0\n",
    "                else:\n",
    "                    cross_entropy = self._cond_loss(fake, c1, m1)\n",
    "\n",
    "                loss_g = -torch.mean(y_fake) + cross_entropy\n",
    "\n",
    "                optimizerG.zero_grad(set_to_none=False)\n",
    "                loss_g.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "            generator_loss = loss_g.detach().cpu()\n",
    "            discriminator_loss = loss_d.detach().cpu()\n",
    "\n",
    "            epoch_loss_df = pd.DataFrame({\n",
    "                'Epoch': [i],\n",
    "                'Generator Loss': [generator_loss],\n",
    "                'Discriminator Loss': [discriminator_loss]\n",
    "            })\n",
    "            if not self.loss_values.empty:\n",
    "                self.loss_values = pd.concat(\n",
    "                    [self.loss_values, epoch_loss_df]\n",
    "                ).reset_index(drop=True)\n",
    "            else:\n",
    "                self.loss_values = epoch_loss_df\n",
    "\n",
    "            if self._verbose:\n",
    "                epoch_iterator.set_description(\n",
    "                    description.format(gen=generator_loss, dis=discriminator_loss)\n",
    "                )\n",
    "\n",
    "    @random_state\n",
    "    def sample(self, n, condition_column=None, condition_value=None):\n",
    "        \"\"\"Sample data similar to the training data.\n",
    "\n",
    "        Choosing a condition_column and condition_value will increase the probability of the\n",
    "        discrete condition_value happening in the condition_column.\n",
    "\n",
    "        Args:\n",
    "            n (int):\n",
    "                Number of rows to sample.\n",
    "            condition_column (string):\n",
    "                Name of a discrete column.\n",
    "            condition_value (string):\n",
    "                Name of the category in the condition_column which we wish to increase the\n",
    "                probability of happening.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray or pandas.DataFrame\n",
    "        \"\"\"\n",
    "        if condition_column is not None and condition_value is not None:\n",
    "            condition_info = self._transformer.convert_column_name_value_to_id(\n",
    "                condition_column, condition_value)\n",
    "            global_condition_vec = self._data_sampler.generate_cond_from_condition_column_info(\n",
    "                condition_info, self._batch_size)\n",
    "        else:\n",
    "            global_condition_vec = None\n",
    "\n",
    "        steps = n // self._batch_size + 1\n",
    "        data = []\n",
    "        for i in range(steps):\n",
    "            mean = torch.zeros(self._batch_size, self._embedding_dim)\n",
    "            std = mean + 1\n",
    "            # fakez = torch.normal(mean=mean, std=std).to(self._device)\n",
    "            s1 = self._data_sampler.sample_data(self._batch_size, None, None)\n",
    "            fakez = self.encoder.predict(s1)\n",
    "            fakez = torch.tensor(fakez).to(self._device)\n",
    "\n",
    "            if global_condition_vec is not None:\n",
    "                condvec = global_condition_vec.copy()\n",
    "            else:\n",
    "                condvec = self._data_sampler.sample_original_condvec(self._batch_size)\n",
    "\n",
    "            if condvec is None:\n",
    "                pass\n",
    "            else:\n",
    "                c1 = condvec\n",
    "                c1 = torch.from_numpy(c1).to(self._device)\n",
    "                fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "            fake = self._generator(fakez)\n",
    "            fakeact = self._apply_activate(fake)\n",
    "            data.append(fakeact.detach().cpu().numpy())\n",
    "\n",
    "        data = np.concatenate(data, axis=0)\n",
    "        data = data[:n]\n",
    "\n",
    "        return self._transformer.inverse_transform(data)\n",
    "\n",
    "    def set_device(self, device):\n",
    "        \"\"\"Set the `device` to be used ('GPU' or 'CPU).\"\"\"\n",
    "        self._device = device\n",
    "        if self._generator is not None:\n",
    "            self._generator.to(self._device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoderCTGANSynthesizer method that can fit and sample given training data and metadata\n",
    "class encoderCTGANSynthesizer(LossValuesMixin, BaseSingleTableSynthesizer):\n",
    "    \"\"\"Model wrapping ``CTGAN`` model.\n",
    "\n",
    "    Args:\n",
    "        metadata (sdv.metadata.SingleTableMetadata):\n",
    "            Single table metadata representing the data that this synthesizer will be used for.\n",
    "        enforce_min_max_values (bool):\n",
    "            Specify whether or not to clip the data returned by ``reverse_transform`` of\n",
    "            the numerical transformer, ``FloatFormatter``, to the min and max values seen\n",
    "            during ``fit``. Defaults to ``True``.\n",
    "        enforce_rounding (bool):\n",
    "            Define rounding scheme for ``numerical`` columns. If ``True``, the data returned\n",
    "            by ``reverse_transform`` will be rounded as in the original data. Defaults to ``True``.\n",
    "        locales (list or str):\n",
    "            The default locale(s) to use for AnonymizedFaker transformers. Defaults to ``None``.\n",
    "        embedding_dim (int):\n",
    "            Size of the random sample passed to the Generator. Defaults to 128.\n",
    "        generator_dim (tuple or list of ints):\n",
    "            Size of the output samples for each one of the Residuals. A Residual Layer\n",
    "            will be created for each one of the values provided. Defaults to (256, 256).\n",
    "        discriminator_dim (tuple or list of ints):\n",
    "            Size of the output samples for each one of the Discriminator Layers. A Linear Layer\n",
    "            will be created for each one of the values provided. Defaults to (256, 256).\n",
    "        generator_lr (float):\n",
    "            Learning rate for the generator. Defaults to 2e-4.\n",
    "        generator_decay (float):\n",
    "            Generator weight decay for the Adam Optimizer. Defaults to 1e-6.\n",
    "        discriminator_lr (float):\n",
    "            Learning rate for the discriminator. Defaults to 2e-4.\n",
    "        discriminator_decay (float):\n",
    "            Discriminator weight decay for the Adam Optimizer. Defaults to 1e-6.\n",
    "        batch_size (int):\n",
    "            Number of data samples to process in each step.\n",
    "        discriminator_steps (int):\n",
    "            Number of discriminator updates to do for each generator update.\n",
    "            From the WGAN paper: https://arxiv.org/abs/1701.07875. WGAN paper\n",
    "            default is 5. Default used is 1 to match original CTGAN implementation.\n",
    "        log_frequency (boolean):\n",
    "            Whether to use log frequency of categorical levels in conditional\n",
    "            sampling. Defaults to ``True``.\n",
    "        verbose (boolean):\n",
    "            Whether to have print statements for progress results. Defaults to ``False``.\n",
    "        epochs (int):\n",
    "            Number of training epochs. Defaults to 300.\n",
    "        pac (int):\n",
    "            Number of samples to group together when applying the discriminator.\n",
    "            Defaults to 10.\n",
    "        cuda (bool or str):\n",
    "            If ``True``, use CUDA. If a ``str``, use the indicated device.\n",
    "            If ``False``, do not use cuda at all.\n",
    "    \"\"\"\n",
    "\n",
    "    _model_sdtype_transformers = {\n",
    "        'categorical': None,\n",
    "        'boolean': None\n",
    "    }\n",
    "\n",
    "    def __init__(self, metadata, enforce_min_max_values=True, enforce_rounding=True, locales=None,\n",
    "                 embedding_dim=128, generator_dim=(256, 256), discriminator_dim=(256, 256),\n",
    "                 generator_lr=2e-4, generator_decay=1e-6, discriminator_lr=2e-4,\n",
    "                 discriminator_decay=1e-6, batch_size=500, discriminator_steps=1,\n",
    "                 log_frequency=True, verbose=False, epochs=300, pac=10, cuda=True):\n",
    "\n",
    "        super().__init__(\n",
    "            metadata=metadata,\n",
    "            enforce_min_max_values=enforce_min_max_values,\n",
    "            enforce_rounding=enforce_rounding,\n",
    "            locales=locales\n",
    "        )\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.generator_dim = generator_dim\n",
    "        self.discriminator_dim = discriminator_dim\n",
    "        self.generator_lr = generator_lr\n",
    "        self.generator_decay = generator_decay\n",
    "        self.discriminator_lr = discriminator_lr\n",
    "        self.discriminator_decay = discriminator_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.discriminator_steps = discriminator_steps\n",
    "        self.log_frequency = log_frequency\n",
    "        self.verbose = verbose\n",
    "        self.epochs = epochs\n",
    "        self.pac = pac\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self._model_kwargs = {\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'generator_dim': generator_dim,\n",
    "            'discriminator_dim': discriminator_dim,\n",
    "            'generator_lr': generator_lr,\n",
    "            'generator_decay': generator_decay,\n",
    "            'discriminator_lr': discriminator_lr,\n",
    "            'discriminator_decay': discriminator_decay,\n",
    "            'batch_size': batch_size,\n",
    "            'discriminator_steps': discriminator_steps,\n",
    "            'log_frequency': log_frequency,\n",
    "            'verbose': verbose,\n",
    "            'epochs': epochs,\n",
    "            'pac': pac,\n",
    "            'cuda': cuda\n",
    "        }\n",
    "\n",
    "    def _estimate_num_columns(self, data):\n",
    "        \"\"\"Estimate the number of columns that the data will generate.\n",
    "\n",
    "        Estimates that continuous columns generate 11 columns and categorical ones\n",
    "        create n where n is the number of unique categories.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Data to estimate the number of columns from.\n",
    "\n",
    "        Returns:\n",
    "            int:\n",
    "                Number of estimate columns.\n",
    "        \"\"\"\n",
    "        sdtypes = self._data_processor.get_sdtypes()\n",
    "        transformers = self.get_transformers()\n",
    "        num_generated_columns = {}\n",
    "        for column in data.columns:\n",
    "            if column not in sdtypes:\n",
    "                continue\n",
    "\n",
    "            if sdtypes[column] in {'numerical', 'datetime'}:\n",
    "                num_generated_columns[column] = 11\n",
    "\n",
    "            elif sdtypes[column] in {'categorical', 'boolean'}:\n",
    "                if transformers.get(column) is None:\n",
    "                    num_categories = data[column].fillna(np.nan).nunique(dropna=False)\n",
    "                    num_generated_columns[column] = num_categories\n",
    "                else:\n",
    "                    num_generated_columns[column] = 11\n",
    "\n",
    "        return num_generated_columns\n",
    "\n",
    "    def _print_warning(self, data):\n",
    "        \"\"\"Print a warning if the number of columns generated is over 1000.\"\"\"\n",
    "        dict_generated_columns = self._estimate_num_columns(data)\n",
    "        if sum(dict_generated_columns.values()) > 1000:\n",
    "            header = {'Original Column Name  ': 'Est # of Columns (CTGAN)'}\n",
    "            dict_generated_columns = {**header, **dict_generated_columns}\n",
    "            longest_column_name = len(max(dict_generated_columns, key=len))\n",
    "            cap = '<' + str(longest_column_name)\n",
    "            lines_to_print = []\n",
    "            for column, num_generated_columns in dict_generated_columns.items():\n",
    "                lines_to_print.append(f'{column:{cap}} {num_generated_columns}')\n",
    "\n",
    "            generated_columns_str = '\\n'.join(lines_to_print)\n",
    "            print(  # noqa: T001\n",
    "                'PerformanceAlert: Using the CTGANSynthesizer on this data is not recommended. '\n",
    "                'To model this data, CTGAN will generate a large number of columns.'\n",
    "                '\\n\\n'\n",
    "                f'{generated_columns_str}'\n",
    "                '\\n\\n'\n",
    "                'We recommend preprocessing discrete columns that can have many values, '\n",
    "                \"using 'update_transformers'. Or you may drop columns that are not necessary \"\n",
    "                'to model. (Exit this script using ctrl-C)'\n",
    "            )\n",
    "\n",
    "    def _preprocess(self, data):\n",
    "        self.validate(data)\n",
    "        self._data_processor.fit(data)\n",
    "        self._print_warning(data)\n",
    "\n",
    "        return self._data_processor.transform(data)\n",
    "\n",
    "    def _fit(self, processed_data):\n",
    "        \"\"\"Fit the model to the table.\n",
    "\n",
    "        Args:\n",
    "            processed_data (pandas.DataFrame):\n",
    "                Data to be learned.\n",
    "        \"\"\"\n",
    "        transformers = self._data_processor._hyper_transformer.field_transformers\n",
    "        discrete_columns = detect_discrete_columns(\n",
    "            self.get_metadata(),\n",
    "            processed_data,\n",
    "            transformers\n",
    "        )\n",
    "        self._model = encoderCTGAN(**self._model_kwargs)\n",
    "        self._model.fit(processed_data, discrete_columns=discrete_columns)\n",
    "\n",
    "    def _sample(self, num_rows, conditions=None):\n",
    "        \"\"\"Sample the indicated number of rows from the model.\n",
    "\n",
    "        Args:\n",
    "            num_rows (int):\n",
    "                Amount of rows to sample.\n",
    "            conditions (dict):\n",
    "                If specified, this dictionary maps column names to the column\n",
    "                value. Then, this method generates ``num_rows`` samples, all of\n",
    "                which are conditioned on the given variables.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Sampled data.\n",
    "        \"\"\"\n",
    "        if conditions is None:\n",
    "            return self._model.sample(num_rows)\n",
    "\n",
    "        raise NotImplementedError(\"CTGANSynthesizer doesn't support conditional sampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to balance the dataset\n",
    "def balance_original_data(data, label):\n",
    "  d_data = pd.DataFrame()\n",
    "  class_counts = data[label].value_counts()\n",
    "  class_counts_dict = class_counts.to_dict()\n",
    "  total_rows = len(data)\n",
    "  n_class = len(class_counts)\n",
    "  step = int(total_rows/n_class)\n",
    "  for l, count in class_counts_dict.items():\n",
    "    filtered = data[data[label]==l]\n",
    "    sample = filtered.sample(n=step, random_state=42, replace=True)\n",
    "    d_data = pd.concat([d_data, sample], axis=0)\n",
    "  return d_data\n",
    "# function to do balanced augmentation\n",
    "def BCTGAN(data, metadata, label, sample_n):    \n",
    "    synthetic_data = pd.DataFrame()\n",
    "    class_counts = data[label].value_counts()\n",
    "    class_counts_dict = class_counts.to_dict()\n",
    "    total_rows = len(data)\n",
    "    threshold = 0.3\n",
    "    small_classes = [label for label, count in class_counts_dict.items() if count < threshold * total_rows]\n",
    "    n_class = len(class_counts)\n",
    "    step = int(sample_n/n_class)\n",
    "    for l in small_classes:\n",
    "        filtered_df = data[data[label]==l]\n",
    "        model = encoderCTGANSynthesizer(metadata)\n",
    "        model.fit(filtered_df)\n",
    "        syn = model.sample(step)\n",
    "        synthetic_data = pd.concat([synthetic_data, syn], axis=0)\n",
    "    model = encoderCTGANSynthesizer(metadata)\n",
    "    data1 = data[~data[label].isin(small_classes)]\n",
    "    model.fit(data1)\n",
    "    syn = model.sample(sample_n - synthetic_data.shape[0])\n",
    "    synthetic_data = pd.concat([synthetic_data, syn], axis=0)\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../original data/adult/train.csv') # load the original training data\n",
    "with open('../original data/adult/metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)  # load the metadata object\n",
    "metadata_obj = SingleTableMetadata.load_from_dict(metadata)\n",
    "label = 'label' # label indicates the column to be classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment data by CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer_1 = CTGANSynthesizer(metadata_obj)\n",
    "synthesizer_1.fit(train)\n",
    "result_1 = synthesizer_1.sample(train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment data by BCTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = BCTGAN(train, metadata_obj, label, train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the balance of label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = train[label].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = value_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of label in original Intrusion dataset')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels if needed\n",
    "for i, v in enumerate(value_counts):\n",
    "    ax.text(i, v + 10, str(v), ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = result_1[label].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = value_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of label in synthetic Intrusion dataset B')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels if needed\n",
    "for i, v in enumerate(value_counts):\n",
    "    ax.text(i, v + 10, str(v), ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = result_2[label].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = value_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of label in synthetic Intrusion dataset A')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels if needed\n",
    "for i, v in enumerate(value_counts):\n",
    "    ax.text(i, v + 10, str(v), ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_evaluator = TableEvaluator(train, result_1)\n",
    "table_evaluator.visual_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_evaluator = TableEvaluator(train, result_2)\n",
    "table_evaluator.visual_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
