{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.single_table import (\n",
    "    CopulaGANSynthesizer, CTGANSynthesizer, GaussianCopulaSynthesizer, TVAESynthesizer)\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential, functional\n",
    "from torch.nn import functional as F\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import sdgym\n",
    "from sdv.metadata.single_table import SingleTableMetadata\n",
    "from sdgym.datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import rdt\n",
    "from rdt.transformers import AnonymizedFaker, IDGenerator, RegexGenerator, get_default_transformers\n",
    "from copy import deepcopy\n",
    "import inspect\n",
    "import copy\n",
    "import traceback\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from rdt.transformers.pii.anonymization import get_anonymized_transformer\n",
    "import warnings\n",
    "import sys\n",
    "from pandas.core.tools.datetimes import _guess_datetime_format_for_array\n",
    "from pandas.api.types import is_float_dtype, is_integer_dtype\n",
    "import json\n",
    "import datetime\n",
    "import pkg_resources\n",
    "import cloudpickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import uuid\n",
    "import math\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "from tqdm import tqdm as tqdm1\n",
    "from tqdm import tqdm as tqdm2\n",
    "import functools\n",
    "import copulas\n",
    "from ctgan import CTGAN\n",
    "from sdgym import create_single_table_synthesizer\n",
    "from joblib import Parallel, delayed\n",
    "from rdt.transformers import ClusterBasedNormalizer, OneHotEncoder\n",
    "from collections import namedtuple\n",
    "import contextlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from table_evaluator import TableEvaluator\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Single table data processor.\n",
    "\n",
    "    This class handles all pre and post processing that is done to a single table to get it ready\n",
    "    for modeling and finalize sampling. These processes include formatting, transformations,\n",
    "    anonymization and constraint handling.\n",
    "\n",
    "    Args:\n",
    "        metadata (metadata.SingleTableMetadata):\n",
    "            The single table metadata instance that will be used to apply constraints and\n",
    "            transformations to the data.\n",
    "        enforce_rounding (bool):\n",
    "            Define rounding scheme for FloatFormatter. If True, the data returned by\n",
    "            reverse_transform will be rounded to that place. Defaults to True.\n",
    "        enforce_min_max_values (bool):\n",
    "            Specify whether or not to clip the data returned by reverse_transform of the numerical\n",
    "            transformer, FloatFormatter, to the min and max values seen during fit.\n",
    "            Defaults to True.\n",
    "        model_kwargs (dict):\n",
    "            Dictionary specifying the kwargs that need to be used in each tabular\n",
    "            model when working on this table. This dictionary contains as keys the name of the\n",
    "            TabularModel class and as values a dictionary containing the keyword arguments to use.\n",
    "            This argument exists mostly to ensure that the models are fitted using the same\n",
    "            arguments when the same DataProcessor is used to fit different model instances on\n",
    "            different slices of the same table.\n",
    "        table_name (str):\n",
    "            Name of table this processor is for. Optional.\n",
    "        locales (str or list):\n",
    "            Default locales to use for AnonymizedFaker transformers. Optional, defaults to using\n",
    "            Faker's default locale.\n",
    "    \"\"\"\n",
    "\n",
    "    _DTYPE_TO_SDTYPE = {\n",
    "        'i': 'numerical',\n",
    "        'f': 'numerical',\n",
    "        'O': 'categorical',\n",
    "        'b': 'boolean',\n",
    "        'M': 'datetime',\n",
    "    }\n",
    "\n",
    "    _COLUMN_RELATIONSHIP_TO_TRANSFORMER = {\n",
    "        'address': 'RandomLocationGenerator',\n",
    "    }\n",
    "\n",
    "    def _update_numerical_transformer(self, enforce_rounding, enforce_min_max_values):\n",
    "        custom_float_formatter = rdt.transformers.FloatFormatter(\n",
    "            missing_value_replacement='mean',\n",
    "            missing_value_generation='random',\n",
    "            learn_rounding_scheme=enforce_rounding,\n",
    "            enforce_min_max_values=enforce_min_max_values\n",
    "        )\n",
    "        self._transformers_by_sdtype.update({'numerical': custom_float_formatter})\n",
    "\n",
    "    def _detect_multi_column_transformers(self):\n",
    "        \"\"\"Detect if there are any multi column transformers in the metadata.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                A dictionary mapping column names to the multi column transformer.\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        # if self.metadata.column_relationships:\n",
    "        #     for relationship in self.metadata._valid_column_relationships:\n",
    "        #         column_names = tuple(relationship['column_names'])\n",
    "        #         relationship_type = relationship['type']\n",
    "        #         if relationship_type in self._COLUMN_RELATIONSHIP_TO_TRANSFORMER:\n",
    "        #             transformer_name = self._COLUMN_RELATIONSHIP_TO_TRANSFORMER[relationship_type]\n",
    "        #             module = getattr(rdt.transformers, relationship_type)\n",
    "        #             transformer = getattr(module, transformer_name)\n",
    "        #             result[column_names] = transformer(locales=self._locales)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __init__(self, metadata, enforce_rounding=True, enforce_min_max_values=True,\n",
    "                 model_kwargs=None, table_name=None, locales=None):\n",
    "        self.metadata = metadata\n",
    "        self._enforce_rounding = enforce_rounding\n",
    "        self._enforce_min_max_values = enforce_min_max_values\n",
    "        self._model_kwargs = model_kwargs or {}\n",
    "        self._locales = locales\n",
    "        self._constraints_list = []\n",
    "        self._constraints = []\n",
    "        self._constraints_to_reverse = []\n",
    "        self._custom_constraint_classes = {}\n",
    "\n",
    "        self._transformers_by_sdtype = deepcopy(get_default_transformers())\n",
    "        self._transformers_by_sdtype['id'] = rdt.transformers.RegexGenerator()\n",
    "        del self._transformers_by_sdtype['text']\n",
    "        self.grouped_columns_to_transformers = self._detect_multi_column_transformers()\n",
    "\n",
    "        self._update_numerical_transformer(enforce_rounding, enforce_min_max_values)\n",
    "        self._hyper_transformer = rdt.HyperTransformer()\n",
    "        self.table_name = table_name\n",
    "        self._dtypes = None\n",
    "        self.fitted = False\n",
    "        self.formatters = {}\n",
    "        self._primary_key = self.metadata.primary_key\n",
    "        self._prepared_for_fitting = False\n",
    "        self._keys = deepcopy(self.metadata.alternate_keys)\n",
    "        if self._primary_key:\n",
    "            self._keys.append(self._primary_key)\n",
    "        self.columns = None\n",
    "\n",
    "    def _get_grouped_columns(self):\n",
    "        \"\"\"Get the columns that are part of a multi column transformer.\n",
    "\n",
    "        Returns:\n",
    "            list:\n",
    "                A list of columns that are part of a multi column transformer.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            col for col_tuple in self.grouped_columns_to_transformers for col in col_tuple\n",
    "        ]\n",
    "\n",
    "    def _get_columns_in_address_transformer(self):\n",
    "        \"\"\"Get the columns that are part of an address transformer.\n",
    "\n",
    "        Returns:\n",
    "            list:\n",
    "                A list of columns that are part of the address transformers.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            _check_import_address_transformers()\n",
    "            result = []\n",
    "            for col_tuple, transformer in self.grouped_columns_to_transformers.items():\n",
    "                is_randomlocationgenerator = isinstance(\n",
    "                    transformer, rdt.transformers.address.RandomLocationGenerator\n",
    "                )\n",
    "                is_regionalanonymizer = isinstance(\n",
    "                    transformer, rdt.transformers.address.RegionalAnonymizer\n",
    "                )\n",
    "                if is_randomlocationgenerator or is_regionalanonymizer:\n",
    "                    result.extend(list(col_tuple))\n",
    "\n",
    "            return result\n",
    "        except ImportError:\n",
    "            return []\n",
    "\n",
    "    def get_model_kwargs(self, model_name):\n",
    "        \"\"\"Return the required model kwargs for the indicated model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str):\n",
    "                Qualified Name of the model for which model kwargs\n",
    "                are needed.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                Keyword arguments to use on the indicated model.\n",
    "        \"\"\"\n",
    "        return deepcopy(self._model_kwargs.get(model_name))\n",
    "\n",
    "    def set_model_kwargs(self, model_name, model_kwargs):\n",
    "        \"\"\"Set the model kwargs used for the indicated model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str):\n",
    "                Qualified Name of the model for which the kwargs will be set.\n",
    "            model_kwargs (dict):\n",
    "                The key word arguments for the model.\n",
    "        \"\"\"\n",
    "        self._model_kwargs[model_name] = model_kwargs\n",
    "\n",
    "    def get_sdtypes(self, primary_keys=False):\n",
    "        \"\"\"Get a ``dict`` with the ``sdtypes`` for each column of the table.\n",
    "\n",
    "        Args:\n",
    "            primary_keys (bool):\n",
    "                Whether or not to include the primary key fields. Defaults to ``False``.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                Dictionary that contains the column names and ``sdtypes``.\n",
    "        \"\"\"\n",
    "        sdtypes = {}\n",
    "        for name, column_metadata in self.metadata.columns.items():\n",
    "            sdtype = column_metadata['sdtype']\n",
    "\n",
    "            if primary_keys or (name not in self._keys):\n",
    "                sdtypes[name] = sdtype\n",
    "\n",
    "        return sdtypes\n",
    "\n",
    "    def _validate_custom_constraint_name(self, class_name):\n",
    "        reserved_class_names = list(get_subclasses(Constraint))\n",
    "        if class_name in reserved_class_names:\n",
    "            error_message = (\n",
    "                f\"The name '{class_name}' is a reserved constraint name. \"\n",
    "                'Please use a different one for the custom constraint.'\n",
    "            )\n",
    "            raise InvalidConstraintsError(error_message)\n",
    "\n",
    "    def _validate_custom_constraints(self, filepath, class_names, module):\n",
    "        errors = []\n",
    "        for class_name in class_names:\n",
    "            try:\n",
    "                self._validate_custom_constraint_name(class_name)\n",
    "            except InvalidConstraintsError as err:\n",
    "                errors += err.errors\n",
    "\n",
    "            if not hasattr(module, class_name):\n",
    "                errors.append(f\"The constraint '{class_name}' is not defined in '{filepath}'.\")\n",
    "\n",
    "        if errors:\n",
    "            raise InvalidConstraintsError(errors)\n",
    "\n",
    "    def load_custom_constraint_classes(self, filepath, class_names):\n",
    "        \"\"\"Load a custom constraint class for the current synthesizer.\n",
    "\n",
    "        Args:\n",
    "            filepath (str):\n",
    "                String representing the absolute or relative path to the python file where\n",
    "                the custom constraints are declared.\n",
    "            class_names (list):\n",
    "                A list of custom constraint classes to be imported.\n",
    "        \"\"\"\n",
    "        path = Path(filepath)\n",
    "        module = load_module_from_path(path)\n",
    "        self._validate_custom_constraints(filepath, class_names, module)\n",
    "        for class_name in class_names:\n",
    "            constraint_class = getattr(module, class_name)\n",
    "            self._custom_constraint_classes[class_name] = constraint_class\n",
    "\n",
    "    def add_custom_constraint_class(self, class_object, class_name):\n",
    "        \"\"\"Add a custom constraint class for the synthesizer to use.\n",
    "\n",
    "        Args:\n",
    "            class_object (sdv.constraints.Constraint):\n",
    "                A custom constraint class object.\n",
    "            class_name (str):\n",
    "                The name to assign this custom constraint class. This will be the name to use\n",
    "                when writing a constraint dictionary for ``add_constraints``.\n",
    "        \"\"\"\n",
    "        self._validate_custom_constraint_name(class_name)\n",
    "        self._custom_constraint_classes[class_name] = class_object\n",
    "\n",
    "    def _validate_constraint_dict(self, constraint_dict):\n",
    "        \"\"\"Validate a constraint against the single table metadata.\n",
    "\n",
    "        Args:\n",
    "            constraint_dict (dict):\n",
    "                A dictionary containing:\n",
    "                    * ``constraint_class``: Name of the constraint to apply.\n",
    "                    * ``constraint_parameters``: A dictionary with the constraint parameters.\n",
    "        \"\"\"\n",
    "        params = {'constraint_class', 'constraint_parameters'}\n",
    "        keys = constraint_dict.keys()\n",
    "        missing_params = params - keys\n",
    "        if missing_params:\n",
    "            raise SynthesizerInputError(\n",
    "                f'A constraint is missing required parameters {missing_params}. '\n",
    "                'Please add these parameters to your constraint definition.'\n",
    "            )\n",
    "\n",
    "        extra_params = keys - params\n",
    "        if extra_params:\n",
    "            raise SynthesizerInputError(\n",
    "                f'Unrecognized constraint parameter {extra_params}. '\n",
    "                'Please remove these parameters from your constraint definition.'\n",
    "            )\n",
    "\n",
    "        constraint_class = constraint_dict['constraint_class']\n",
    "        constraint_parameters = constraint_dict['constraint_parameters']\n",
    "        try:\n",
    "            if constraint_class in self._custom_constraint_classes:\n",
    "                constraint_class = self._custom_constraint_classes[constraint_class]\n",
    "\n",
    "            else:\n",
    "                constraint_class = Constraint._get_class_from_dict(constraint_class)\n",
    "\n",
    "        except KeyError:\n",
    "            raise InvalidConstraintsError(f\"Invalid constraint class ('{constraint_class}').\")\n",
    "\n",
    "        if 'column_name' in constraint_parameters:\n",
    "            column_names = [constraint_parameters.get('column_name')]\n",
    "        else:\n",
    "            column_names = constraint_parameters.get('column_names')\n",
    "\n",
    "        columns_in_address = self._get_columns_in_address_transformer()\n",
    "        if columns_in_address and column_names:\n",
    "            address_constraint_columns = set(column_names) & set(columns_in_address)\n",
    "            if address_constraint_columns:\n",
    "                to_print = \"', '\".join(address_constraint_columns)\n",
    "                raise InvalidConstraintsError(\n",
    "                    f\"The '{to_print}' columns are part of an address. You cannot add constraints \"\n",
    "                    'to columns that are part of an address group.'\n",
    "                )\n",
    "\n",
    "        constraint_class._validate_metadata(**constraint_parameters)\n",
    "\n",
    "    def add_constraints(self, constraints):\n",
    "        \"\"\"Add constraints to the data processor.\n",
    "\n",
    "        Args:\n",
    "            constraints (list):\n",
    "                List of constraints described as dictionaries in the following format:\n",
    "                    * ``constraint_class``: Name of the constraint to apply.\n",
    "                    * ``constraint_parameters``: A dictionary with the constraint parameters.\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        validated_constraints = []\n",
    "        for constraint_dict in constraints:\n",
    "            constraint_dict = deepcopy(constraint_dict)\n",
    "            if 'constraint_parameters' in constraint_dict:\n",
    "                constraint_dict['constraint_parameters'].update({'metadata': self.metadata})\n",
    "            try:\n",
    "                self._validate_constraint_dict(constraint_dict)\n",
    "                validated_constraints.append(constraint_dict)\n",
    "            except (AggregateConstraintsError, InvalidConstraintsError) as e:\n",
    "                reformated_errors = '\\n'.join(map(str, e.errors))\n",
    "                errors.append(reformated_errors)\n",
    "\n",
    "        if errors:\n",
    "            raise InvalidConstraintsError(errors)\n",
    "\n",
    "        self._constraints_list.extend(validated_constraints)\n",
    "        self._prepared_for_fitting = False\n",
    "\n",
    "    def get_constraints(self):\n",
    "        \"\"\"Get a list of the current constraints that will be used.\n",
    "\n",
    "        Returns:\n",
    "            list:\n",
    "                List of dictionaries describing the constraints for this data processor.\n",
    "        \"\"\"\n",
    "        constraints = deepcopy(self._constraints_list)\n",
    "        for i in range(len(constraints)):\n",
    "            del constraints[i]['constraint_parameters']['metadata']\n",
    "\n",
    "        return constraints\n",
    "\n",
    "    def _load_constraints(self):\n",
    "        loaded_constraints = []\n",
    "        default_constraints_classes = list(get_subclasses(Constraint))\n",
    "        for constraint in self._constraints_list:\n",
    "            if constraint['constraint_class'] in default_constraints_classes:\n",
    "                loaded_constraints.append(Constraint.from_dict(constraint))\n",
    "\n",
    "            else:\n",
    "                constraint_class = self._custom_constraint_classes[constraint['constraint_class']]\n",
    "                loaded_constraints.append(\n",
    "                    constraint_class(**constraint.get('constraint_parameters', {}))\n",
    "                )\n",
    "\n",
    "        return loaded_constraints\n",
    "\n",
    "    def _fit_constraints(self, data):\n",
    "        self._constraints = self._load_constraints()\n",
    "        errors = []\n",
    "        for constraint in self._constraints:\n",
    "            try:\n",
    "                constraint.fit(data)\n",
    "            except Exception as e:\n",
    "                errors.append(e)\n",
    "\n",
    "        if errors:\n",
    "            raise AggregateConstraintsError(errors)\n",
    "\n",
    "    def _transform_constraints(self, data, is_condition=False):\n",
    "        errors = []\n",
    "        if not is_condition:\n",
    "            self._constraints_to_reverse = []\n",
    "\n",
    "        for constraint in self._constraints:\n",
    "            try:\n",
    "                data = constraint.transform(data)\n",
    "                if not is_condition:\n",
    "                    self._constraints_to_reverse.append(constraint)\n",
    "\n",
    "            except (MissingConstraintColumnError, FunctionError) as error:\n",
    "                if isinstance(error, MissingConstraintColumnError):\n",
    "                    LOGGER.info(\n",
    "                        'Unable to transform %s with columns %s because they are not all available'\n",
    "                        ' in the data. This happens due to multiple, overlapping constraints.',\n",
    "                        constraint.__class__.__name__,\n",
    "                        error.missing_columns\n",
    "                    )\n",
    "                    log_exc_stacktrace(LOGGER, error)\n",
    "                else:\n",
    "                    # Error came from custom constraint. We don't want to crash but we do\n",
    "                    # want to log it.\n",
    "                    LOGGER.info(\n",
    "                        'Unable to transform %s with columns %s due to an error in transform: \\n'\n",
    "                        '%s\\nUsing the reject sampling approach instead.',\n",
    "                        constraint.__class__.__name__,\n",
    "                        constraint.column_names,\n",
    "                        str(error)\n",
    "                    )\n",
    "                    log_exc_stacktrace(LOGGER, error)\n",
    "                if is_condition:\n",
    "                    indices_to_drop = data.columns.isin(constraint.constraint_columns)\n",
    "                    columns_to_drop = data.columns.where(indices_to_drop).dropna()\n",
    "                    data = data.drop(columns_to_drop, axis=1)\n",
    "\n",
    "            except Exception as error:\n",
    "                errors.append(error)\n",
    "\n",
    "        if errors:\n",
    "            raise AggregateConstraintsError(errors)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _update_transformers_by_sdtypes(self, sdtype, transformer):\n",
    "        self._transformers_by_sdtype[sdtype] = transformer\n",
    "\n",
    "    @staticmethod\n",
    "    def create_anonymized_transformer(sdtype, column_metadata, enforce_uniqueness, locales=None):\n",
    "        \"\"\"Create an instance of an ``AnonymizedFaker``.\n",
    "\n",
    "        Read the extra keyword arguments from the ``column_metadata`` and use them to create\n",
    "        an instance of an ``AnonymizedFaker`` transformer.\n",
    "\n",
    "        Args:\n",
    "            sdtype (str):\n",
    "                Sematic data type or a ``Faker`` function name.\n",
    "            column_metadata (dict):\n",
    "                A dictionary representing the rest of the metadata for the given ``sdtype``.\n",
    "            enforce_uniqueness (bool):\n",
    "                If ``True`` overwrite ``enforce_uniqueness`` with ``True`` to ensure unique\n",
    "                generation for primary keys.\n",
    "            locales (str or list):\n",
    "                Locale or list of locales to use for the AnonymizedFaker transfomer. Optional,\n",
    "                defaults to using Faker's default locale.\n",
    "\n",
    "        Returns:\n",
    "            Instance of ``rdt.transformers.pii.AnonymizedFaker``.\n",
    "        \"\"\"\n",
    "        kwargs = {'locales': locales}\n",
    "        for key, value in column_metadata.items():\n",
    "            if key not in ['pii', 'sdtype']:\n",
    "                kwargs[key] = value\n",
    "\n",
    "        if enforce_uniqueness:\n",
    "            kwargs['enforce_uniqueness'] = True\n",
    "\n",
    "        try:\n",
    "            transformer = get_anonymized_transformer(sdtype, kwargs)\n",
    "        except AttributeError as error:\n",
    "            raise SynthesizerInputError(\n",
    "                f\"The sdtype '{sdtype}' is not compatible with any of the locales. To \"\n",
    "                \"continue, try changing the locales or adding 'en_US' as a possible option.\"\n",
    "            ) from error\n",
    "\n",
    "        return transformer\n",
    "\n",
    "    def create_regex_generator(self, column_name, sdtype, column_metadata, is_numeric):\n",
    "        \"\"\"Create a ``RegexGenerator`` for the ``id`` columns.\n",
    "\n",
    "        Read the keyword arguments from the ``column_metadata`` and use them to create\n",
    "        an instance of a ``RegexGenerator``. If ``regex_format`` is not present in the\n",
    "        metadata a default ``[0-1a-z]{5}`` will be used for object like data and an increasing\n",
    "        integer from ``0`` will be used for numerical data. Also if the column name is a primary\n",
    "        key or alternate key this will enforce the values to be unique.\n",
    "\n",
    "        Args:\n",
    "            column_name (str):\n",
    "                Name of the column.\n",
    "            sdtype (str):\n",
    "                Sematic data type or a ``Faker`` function name.\n",
    "            column_metadata (dict):\n",
    "                A dictionary representing the rest of the metadata for the given ``sdtype``.\n",
    "            is_numeric (boolean):\n",
    "                A boolean representing whether or not data type is numeric or not.\n",
    "\n",
    "        Returns:\n",
    "            transformer:\n",
    "                Instance of ``rdt.transformers.text.RegexGenerator`` or\n",
    "                ``rdt.transformers.pii.AnonymizedFaker`` with ``enforce_uniqueness`` set to\n",
    "                ``True``.\n",
    "        \"\"\"\n",
    "        default_regex_format = r'\\d{30}' if is_numeric else '[0-1a-z]{5}'\n",
    "        regex_format = column_metadata.get('regex_format', default_regex_format)\n",
    "        transformer = rdt.transformers.RegexGenerator(\n",
    "            regex_format=regex_format,\n",
    "            enforce_uniqueness=(column_name in self._keys)\n",
    "        )\n",
    "\n",
    "        return transformer\n",
    "\n",
    "    def _get_transformer_instance(self, sdtype, column_metadata):\n",
    "        transformer = self._transformers_by_sdtype[sdtype]\n",
    "        if isinstance(transformer, AnonymizedFaker):\n",
    "            is_lexify = transformer.function_name == 'lexify'\n",
    "            is_baseprovider = transformer.provider_name == 'BaseProvider'\n",
    "            if is_lexify and is_baseprovider:  # Default settings\n",
    "                return self.create_anonymized_transformer(\n",
    "                    sdtype, column_metadata, False, self._locales\n",
    "                )\n",
    "\n",
    "        kwargs = {\n",
    "            key: value for key, value in column_metadata.items()\n",
    "            if key not in ['pii', 'sdtype']\n",
    "        }\n",
    "        if sdtype == 'datetime':\n",
    "            kwargs['enforce_min_max_values'] = self._enforce_min_max_values\n",
    "\n",
    "        if kwargs and transformer is not None:\n",
    "            transformer_class = transformer.__class__\n",
    "            return transformer_class(**kwargs)\n",
    "\n",
    "        return deepcopy(transformer)\n",
    "\n",
    "    def _update_constraint_transformers(self, data, columns_created_by_constraints, config):\n",
    "        missing_columns = set(columns_created_by_constraints) - config['transformers'].keys()\n",
    "        for column in missing_columns:\n",
    "            dtype_kind = data[column].dtype.kind\n",
    "            if dtype_kind in ('i', 'f'):\n",
    "                config['sdtypes'][column] = 'numerical'\n",
    "                config['transformers'][column] = rdt.transformers.FloatFormatter(\n",
    "                    missing_value_replacement='mean',\n",
    "                    missing_value_generation='random',\n",
    "                    enforce_min_max_values=self._enforce_min_max_values\n",
    "                )\n",
    "            else:\n",
    "                sdtype = self._DTYPE_TO_SDTYPE.get(dtype_kind, 'categorical')\n",
    "                config['sdtypes'][column] = sdtype\n",
    "                config['transformers'][column] = self._get_transformer_instance(sdtype, {})\n",
    "\n",
    "        # Remove columns that have been dropped by the constraint\n",
    "        for column in list(config['sdtypes'].keys()):\n",
    "            if column not in data:\n",
    "                LOGGER.info(\n",
    "                    f\"A constraint has dropped the column '{column}', removing the transformer \"\n",
    "                    \"from the 'HyperTransformer'.\"\n",
    "                )\n",
    "                config['sdtypes'].pop(column)\n",
    "                config['transformers'].pop(column)\n",
    "\n",
    "        return config\n",
    "\n",
    "    def _create_config(self, data, columns_created_by_constraints):\n",
    "        sdtypes = {}\n",
    "        transformers = {}\n",
    "\n",
    "        columns_in_multi_col_transformer = self._get_grouped_columns()\n",
    "        for column in set(data.columns) - columns_created_by_constraints:\n",
    "            column_metadata = self.metadata.columns.get(column)\n",
    "            sdtype = column_metadata.get('sdtype')\n",
    "\n",
    "            if column in columns_in_multi_col_transformer:\n",
    "                sdtypes[column] = sdtype\n",
    "                continue\n",
    "\n",
    "            pii = column_metadata.get('pii', sdtype not in self._transformers_by_sdtype)\n",
    "            sdtypes[column] = 'pii' if pii else sdtype\n",
    "\n",
    "            if sdtype == 'id':\n",
    "                is_numeric = pd.api.types.is_numeric_dtype(data[column].dtype)\n",
    "                if column_metadata.get('regex_format', False):\n",
    "                    transformers[column] = self.create_regex_generator(\n",
    "                        column,\n",
    "                        sdtype,\n",
    "                        column_metadata,\n",
    "                        is_numeric\n",
    "                    )\n",
    "                    sdtypes[column] = 'text'\n",
    "\n",
    "                elif column in self._keys:\n",
    "                    prefix = None\n",
    "                    if not is_numeric:\n",
    "                        prefix = 'sdv-id-'\n",
    "\n",
    "                    transformers[column] = IDGenerator(prefix=prefix)\n",
    "                    sdtypes[column] = 'text'\n",
    "\n",
    "                else:\n",
    "                    transformers[column] = AnonymizedFaker(\n",
    "                        provider_name=None,\n",
    "                        function_name='bothify',\n",
    "                        function_kwargs={'text': '#####'}\n",
    "                    )\n",
    "                    sdtypes[column] = 'pii'\n",
    "\n",
    "            elif sdtype == 'unknown':\n",
    "                transformers[column] = AnonymizedFaker(\n",
    "                    function_name='bothify',\n",
    "                )\n",
    "                transformers[column].function_kwargs = {\n",
    "                    'text': 'sdv-pii-?????',\n",
    "                    'letters': '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "                }\n",
    "\n",
    "            elif pii:\n",
    "                enforce_uniqueness = bool(column in self._keys)\n",
    "                transformers[column] = self.create_anonymized_transformer(\n",
    "                    sdtype,\n",
    "                    column_metadata,\n",
    "                    enforce_uniqueness,\n",
    "                    self._locales\n",
    "                )\n",
    "\n",
    "            elif sdtype in self._transformers_by_sdtype:\n",
    "                transformers[column] = self._get_transformer_instance(sdtype, column_metadata)\n",
    "\n",
    "            else:\n",
    "                sdtypes[column] = 'categorical'\n",
    "                transformers[column] = self._get_transformer_instance(\n",
    "                    'categorical',\n",
    "                    column_metadata\n",
    "                )\n",
    "\n",
    "        for columns, transformer in self.grouped_columns_to_transformers.items():\n",
    "            transformers[columns] = transformer\n",
    "\n",
    "        config = {'transformers': transformers, 'sdtypes': sdtypes}\n",
    "        config = self._update_constraint_transformers(data, columns_created_by_constraints, config)\n",
    "\n",
    "        return config\n",
    "\n",
    "    def update_transformers(self, column_name_to_transformer):\n",
    "        \"\"\"Update any of the transformers assigned to each of the column names.\n",
    "\n",
    "        Args:\n",
    "            column_name_to_transformer (dict):\n",
    "                Dict mapping column names to transformers to be used for that column.\n",
    "        \"\"\"\n",
    "        if self._hyper_transformer.field_transformers == {}:\n",
    "            raise NotFittedError(\n",
    "                'The DataProcessor must be prepared for fitting before the transformers can be '\n",
    "                'updated.'\n",
    "            )\n",
    "\n",
    "        for column, transformer in column_name_to_transformer.items():\n",
    "            if column in self._keys and not type(transformer) in (AnonymizedFaker, RegexGenerator):\n",
    "                raise SynthesizerInputError(\n",
    "                    f\"Invalid transformer '{transformer.__class__.__name__}' for a primary \"\n",
    "                    f\"or alternate key '{column}'. Please use 'AnonymizedFaker' or \"\n",
    "                    \"'RegexGenerator' instead.\"\n",
    "                )\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', module='rdt.hyper_transformer')\n",
    "            self._hyper_transformer.update_transformers(column_name_to_transformer)\n",
    "\n",
    "        self.grouped_columns_to_transformers = {\n",
    "            col_tuple: transformer\n",
    "            for col_tuple, transformer in self._hyper_transformer.field_transformers.items()\n",
    "            if isinstance(col_tuple, tuple)\n",
    "        }\n",
    "\n",
    "    def _fit_hyper_transformer(self, data):\n",
    "        \"\"\"Create and return a new ``rdt.HyperTransformer`` instance.\n",
    "\n",
    "        First get the ``dtypes`` and then use them to build a transformer dictionary\n",
    "        to be used by the ``HyperTransformer``.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Data to transform.\n",
    "\n",
    "        Returns:\n",
    "            rdt.HyperTransformer\n",
    "        \"\"\"\n",
    "        self._hyper_transformer.fit(data)\n",
    "\n",
    "    def _fit_formatters(self, data):\n",
    "        \"\"\"Fit ``NumericalFormatter`` and ``DatetimeFormatter`` for each column in the data.\"\"\"\n",
    "        for column_name in data:\n",
    "            column_metadata = self.metadata.columns.get(column_name)\n",
    "            sdtype = column_metadata.get('sdtype')\n",
    "            if sdtype == 'numerical' and column_name != self._primary_key:\n",
    "                representation = column_metadata.get('computer_representation', 'Float')\n",
    "                self.formatters[column_name] = NumericalFormatter(\n",
    "                    enforce_rounding=self._enforce_rounding,\n",
    "                    enforce_min_max_values=self._enforce_min_max_values,\n",
    "                    computer_representation=representation\n",
    "                )\n",
    "                self.formatters[column_name].learn_format(data[column_name])\n",
    "\n",
    "            elif sdtype == 'datetime' and column_name != self._primary_key:\n",
    "                datetime_format = column_metadata.get('datetime_format')\n",
    "                self.formatters[column_name] = DatetimeFormatter(datetime_format=datetime_format)\n",
    "                self.formatters[column_name].learn_format(data[column_name])\n",
    "\n",
    "    def prepare_for_fitting(self, data):\n",
    "        \"\"\"Prepare the ``DataProcessor`` for fitting.\n",
    "\n",
    "        This method will learn the ``dtypes`` of the data, fit the numerical formatters,\n",
    "        fit the constraints and create the configuration for the ``rdt.HyperTransformer``.\n",
    "        If the ``rdt.HyperTransformer`` has already been updated, this will not perform the\n",
    "        actions again.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Table data to be learnt.\n",
    "        \"\"\"\n",
    "        if not self._prepared_for_fitting:\n",
    "            LOGGER.info(f'Fitting table {self.table_name} metadata')\n",
    "            self._dtypes = data[list(data.columns)].dtypes\n",
    "\n",
    "            self.formatters = {}\n",
    "            LOGGER.info(f'Fitting formatters for table {self.table_name}')\n",
    "            self._fit_formatters(data)\n",
    "\n",
    "            LOGGER.info(f'Fitting constraints for table {self.table_name}')\n",
    "            if len(self._constraints_list) != len(self._constraints):\n",
    "                self._fit_constraints(data)\n",
    "\n",
    "            constrained = self._transform_constraints(data)\n",
    "            columns_created_by_constraints = set(constrained.columns) - set(data.columns)\n",
    "\n",
    "            config = self._hyper_transformer.get_config()\n",
    "            missing_columns = columns_created_by_constraints - config.get('sdtypes').keys()\n",
    "            if not config.get('sdtypes'):\n",
    "                LOGGER.info((\n",
    "                    'Setting the configuration for the ``HyperTransformer`` '\n",
    "                    f'for table {self.table_name}'\n",
    "                ))\n",
    "                config = self._create_config(constrained, columns_created_by_constraints)\n",
    "                self._hyper_transformer.set_config(config)\n",
    "\n",
    "            elif missing_columns:\n",
    "                config = self._update_constraint_transformers(\n",
    "                    constrained,\n",
    "                    missing_columns,\n",
    "                    config\n",
    "                )\n",
    "                self._hyper_transformer = rdt.HyperTransformer()\n",
    "                self._hyper_transformer.set_config(config)\n",
    "\n",
    "            self._prepared_for_fitting = True\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit this metadata to the given data.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Table to be analyzed.\n",
    "        \"\"\"\n",
    "        if data.empty:\n",
    "            raise ValueError('The fit dataframe is empty, synthesizer will not be fitted.')\n",
    "        self._prepared_for_fitting = False\n",
    "        self.prepare_for_fitting(data)\n",
    "        constrained = self._transform_constraints(data)\n",
    "        if constrained.empty:\n",
    "            raise ValueError(\n",
    "                'The constrained fit dataframe is empty, synthesizer will not be fitted.')\n",
    "        LOGGER.info(f'Fitting HyperTransformer for table {self.table_name}')\n",
    "        self._fit_hyper_transformer(constrained)\n",
    "        self.fitted = True\n",
    "        self.columns = list(data.columns)\n",
    "\n",
    "    def reset_sampling(self):\n",
    "        \"\"\"Reset the sampling state for the anonymized columns and primary keys.\"\"\"\n",
    "        self._hyper_transformer.reset_randomization()\n",
    "\n",
    "    def generate_keys(self, num_rows, reset_keys=False):\n",
    "        \"\"\"Generate the columns that are identified as ``keys``.\n",
    "\n",
    "        Args:\n",
    "            num_rows (int):\n",
    "                Number of rows to be created. Must be an integer greater than 0.\n",
    "            reset_keys (bool):\n",
    "                Whether or not to reset the keys generators. Defaults to ``False``.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                A dataframe with the newly generated primary keys of the size ``num_rows``.\n",
    "        \"\"\"\n",
    "        generated_keys = self._hyper_transformer.create_anonymized_columns(\n",
    "            num_rows=num_rows,\n",
    "            column_names=self._keys,\n",
    "        )\n",
    "        return generated_keys\n",
    "\n",
    "    def transform(self, data, is_condition=False):\n",
    "        \"\"\"Transform the given data.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Table data.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Transformed data.\n",
    "        \"\"\"\n",
    "        data = data.copy()\n",
    "        if not self.fitted:\n",
    "            raise NotFittedError()\n",
    "\n",
    "        # Filter columns that can be transformed\n",
    "        columns = [\n",
    "            column for column in self.get_sdtypes(primary_keys=not is_condition)\n",
    "            if column in data.columns\n",
    "        ]\n",
    "        LOGGER.debug(f'Transforming constraints for table {self.table_name}')\n",
    "        data = self._transform_constraints(data[columns], is_condition)\n",
    "\n",
    "        LOGGER.debug(f'Transforming table {self.table_name}')\n",
    "        if self._keys and not is_condition:\n",
    "            data = data.set_index(self._primary_key, drop=False)\n",
    "\n",
    "        try:\n",
    "            transformed = self._hyper_transformer.transform_subset(data)\n",
    "        except (rdt.errors.NotFittedError, rdt.errors.ConfigNotSetError):\n",
    "            transformed = data\n",
    "\n",
    "        return transformed\n",
    "\n",
    "    def reverse_transform(self, data, reset_keys=False):\n",
    "        \"\"\"Reverse the transformed data to the original format.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Data to be reverse transformed.\n",
    "            reset_keys (bool):\n",
    "                Whether or not to reset the keys generators. Defaults to ``False``.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise NotFittedError()\n",
    "\n",
    "        reversible_columns = [\n",
    "            column\n",
    "            for column in self._hyper_transformer._output_columns\n",
    "            if column in data.columns\n",
    "        ]\n",
    "\n",
    "        reversed_data = data\n",
    "        try:\n",
    "            if not data.empty:\n",
    "                reversed_data = self._hyper_transformer.reverse_transform_subset(\n",
    "                    data[reversible_columns]\n",
    "                )\n",
    "        except rdt.errors.NotFittedError:\n",
    "            LOGGER.info(f'HyperTransformer has not been fitted for table {self.table_name}')\n",
    "\n",
    "        for transformer in self.grouped_columns_to_transformers.values():\n",
    "            if not transformer.output_columns:\n",
    "                reversed_data = transformer.reverse_transform(reversed_data)\n",
    "\n",
    "        num_rows = len(reversed_data)\n",
    "        sampled_columns = list(reversed_data.columns)\n",
    "        missing_columns = [\n",
    "            column\n",
    "            for column in self.metadata.columns.keys() - set(sampled_columns + self._keys)\n",
    "            if self._hyper_transformer.field_transformers.get(column)\n",
    "        ]\n",
    "        if missing_columns and num_rows:\n",
    "            anonymized_data = self._hyper_transformer.create_anonymized_columns(\n",
    "                num_rows=num_rows,\n",
    "                column_names=missing_columns\n",
    "            )\n",
    "            sampled_columns.extend(missing_columns)\n",
    "            reversed_data[anonymized_data.columns] = anonymized_data[anonymized_data.notna()]\n",
    "\n",
    "        if self._keys and num_rows:\n",
    "            generated_keys = self.generate_keys(num_rows, reset_keys)\n",
    "            sampled_columns.extend(self._keys)\n",
    "            reversed_data[generated_keys.columns] = generated_keys[generated_keys.notna()]\n",
    "\n",
    "        for constraint in reversed(self._constraints_to_reverse):\n",
    "            reversed_data = constraint.reverse_transform(reversed_data)\n",
    "\n",
    "        # Add new columns generated by the constraint\n",
    "        new_columns = list(set(reversed_data.columns) - set(sampled_columns))\n",
    "        sampled_columns.extend(new_columns)\n",
    "\n",
    "        # Sort the sampled columns in the order of the metadata.\n",
    "        # Any extra columns not present in the metadata will be dropped.\n",
    "        # In multitable there may be missing columns in the sample such as foreign keys\n",
    "        # And alternate keys. Thats the reason of ensuring that the metadata column is within\n",
    "        # The sampled columns.\n",
    "        sampled_columns = [\n",
    "            column for column in self.metadata.columns.keys()\n",
    "            if column in sampled_columns\n",
    "        ]\n",
    "        for column_name in sampled_columns:\n",
    "            column_data = reversed_data[column_name]\n",
    "\n",
    "            dtype = self._dtypes[column_name]\n",
    "            if is_integer_dtype(dtype) and is_float_dtype(column_data.dtype):\n",
    "                column_data = column_data.round()\n",
    "\n",
    "            reversed_data[column_name] = column_data[column_data.notna()]\n",
    "            try:\n",
    "                reversed_data[column_name] = reversed_data[column_name].astype(dtype)\n",
    "            except ValueError as e:\n",
    "                column_metadata = self.metadata.columns.get(column_name)\n",
    "                sdtype = column_metadata.get('sdtype')\n",
    "                if sdtype not in self._DTYPE_TO_SDTYPE.values():\n",
    "                    LOGGER.info(\n",
    "                        f\"The real data in '{column_name}' was stored as '{dtype}' but the \"\n",
    "                        'synthetic data could not be cast back to this type. If this is a '\n",
    "                        'problem, please check your input data and metadata settings.'\n",
    "                    )\n",
    "                    if column_name in self.formatters:\n",
    "                        self.formatters.pop(column_name)\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(e)\n",
    "\n",
    "        # reformat columns using the formatters\n",
    "        for column in sampled_columns:\n",
    "            if column in self.formatters:\n",
    "                data_to_format = reversed_data[column]\n",
    "                reversed_data[column] = self.formatters[column].format_data(data_to_format)\n",
    "        d = reversed_data[sampled_columns]\n",
    "        new_column_order = self.columns\n",
    "        df_syn = d[new_column_order]\n",
    "        return df_syn\n",
    "\n",
    "    def filter_valid(self, data):\n",
    "        \"\"\"Filter the data using the constraints and return only the valid rows.\n",
    "\n",
    "        Args:\n",
    "            data (pandas.DataFrame):\n",
    "                Table data.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame:\n",
    "                Table containing only the valid rows.\n",
    "        \"\"\"\n",
    "        for constraint in self._constraints:\n",
    "            data = constraint.filter_valid(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Get a dict representation of this DataProcessor.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                Dict representation of this DataProcessor.\n",
    "        \"\"\"\n",
    "        constraints_to_reverse = [cnt.to_dict() for cnt in self._constraints_to_reverse]\n",
    "        return {\n",
    "            'metadata': deepcopy(self.metadata.to_dict()),\n",
    "            'constraints_list': self.get_constraints(),\n",
    "            'constraints_to_reverse': constraints_to_reverse,\n",
    "            'model_kwargs': deepcopy(self._model_kwargs)\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, metadata_dict, enforce_rounding=True, enforce_min_max_values=True):\n",
    "        \"\"\"Load a DataProcessor from a metadata dict.\n",
    "\n",
    "        Args:\n",
    "            metadata_dict (dict):\n",
    "                Dict metadata to load.\n",
    "            enforce_rounding (bool):\n",
    "                If passed, set the ``enforce_rounding`` on the new instance.\n",
    "            enforce_min_max_values (bool):\n",
    "                If passed, set the ``enforce_min_max_values`` on the new instance.\n",
    "        \"\"\"\n",
    "        instance = cls(\n",
    "            metadata=SingleTableMetadata.load_from_dict(metadata_dict['metadata']),\n",
    "            enforce_rounding=enforce_rounding,\n",
    "            enforce_min_max_values=enforce_min_max_values,\n",
    "            model_kwargs=metadata_dict.get('model_kwargs')\n",
    "        )\n",
    "\n",
    "        instance._constraints_to_reverse = [\n",
    "            Constraint.from_dict(cnt) for cnt in metadata_dict.get('constraints_to_reverse', [])\n",
    "        ]\n",
    "        instance._constraints_list = metadata_dict.get('constraints_list', [])\n",
    "\n",
    "        return instance\n",
    "\n",
    "    def to_json(self, filepath):\n",
    "        \"\"\"Dump this DataProcessor into a JSON file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str):\n",
    "                Path of the JSON file where this metadata will be stored.\n",
    "        \"\"\"\n",
    "        with open(filepath, 'w') as out_file:\n",
    "            json.dump(self.to_dict(), out_file, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, filepath):\n",
    "        \"\"\"Load a DataProcessor from a JSON.\n",
    "\n",
    "        Args:\n",
    "            filepath (str):\n",
    "                Path of the JSON file to load\n",
    "        \"\"\"\n",
    "        with open(filepath, 'r') as in_file:\n",
    "            return cls.from_dict(json.load(in_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "MAX_DECIMALS = sys.float_info.dig - 1\n",
    "INTEGER_BOUNDS = {\n",
    "    'Int8': (-2**7, 2**7 - 1),\n",
    "    'Int16': (-2**15, 2**15 - 1),\n",
    "    'Int32': (-2**31, 2**31 - 1),\n",
    "    'Int64': (-2**63, 2**63 - 1),\n",
    "    'UInt8': (0, 2**8 - 1),\n",
    "    'UInt16': (0, 2**16 - 1),\n",
    "    'UInt32': (0, 2**32 - 1),\n",
    "    'UInt64': (0, 2**64 - 1),\n",
    "}\n",
    "\n",
    "\n",
    "class NumericalFormatter:\n",
    "    \"\"\"Formatter for numerical data.\n",
    "\n",
    "    Args:\n",
    "        enforce_rounding (bool):\n",
    "            Whether or not to learn what place to round to based on the data seen during ``fit``.\n",
    "            If ``True``, the data returned by ``reverse_transform`` will be rounded to that place.\n",
    "            Defaults to ``False``.\n",
    "        enforce_min_max_values (bool):\n",
    "            Whether or not to clip the data returned by ``reverse_transform`` to the min and\n",
    "            max values seen during ``fit``.\n",
    "            Defaults to ``False``.\n",
    "        computer_representation (dtype):\n",
    "            Accepts ``'Int8'``, ``'Int16'``, ``'Int32'``, ``'Int64'``, ``'UInt8'``, ``'UInt16'``,\n",
    "            ``'UInt32'``, ``'UInt64'``, ``'Float'``.\n",
    "            Defaults to ``'Float'``.\n",
    "    \"\"\"\n",
    "\n",
    "    _dtype = None\n",
    "    _min_value = None\n",
    "    _max_value = None\n",
    "    _rounding_digits = None\n",
    "\n",
    "    def __init__(self, enforce_rounding=False, enforce_min_max_values=False,\n",
    "                 computer_representation='Float'):\n",
    "        self.enforce_rounding = enforce_rounding\n",
    "        self.enforce_min_max_values = enforce_min_max_values\n",
    "        self.computer_representation = computer_representation\n",
    "\n",
    "    @staticmethod\n",
    "    def _learn_rounding_digits(data):\n",
    "        \"\"\"Check if data has any decimals.\"\"\"\n",
    "        name = data.name\n",
    "        data = np.array(data)\n",
    "        roundable_data = data[~(np.isinf(data) | pd.isna(data))]\n",
    "\n",
    "        # Doesn't contain numbers\n",
    "        if len(roundable_data) == 0:\n",
    "            return None\n",
    "\n",
    "        # Doesn't contain decimal digits\n",
    "        if ((roundable_data % 1) == 0).all():\n",
    "            return 0\n",
    "\n",
    "        # Try to round to fewer digits\n",
    "        if (roundable_data == roundable_data.round(MAX_DECIMALS)).all():\n",
    "            for decimal in range(MAX_DECIMALS + 1):\n",
    "                if (roundable_data == roundable_data.round(decimal)).all():\n",
    "                    return decimal\n",
    "\n",
    "        # Can't round, not equal after MAX_DECIMALS digits of precision\n",
    "        LOGGER.info(\n",
    "            f\"No rounding scheme detected for column '{name}'.\"\n",
    "            ' Synthetic data will not be rounded.'\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    def learn_format(self, column):\n",
    "        \"\"\"Learn the format of a column.\n",
    "\n",
    "        Args:\n",
    "            column (pandas.Series):\n",
    "                Data to learn the format.\n",
    "        \"\"\"\n",
    "        self._dtype = column.dtype\n",
    "        if self.enforce_min_max_values:\n",
    "            self._min_value = column.min()\n",
    "            self._max_value = column.max()\n",
    "\n",
    "        if self.enforce_rounding:\n",
    "            self._rounding_digits = self._learn_rounding_digits(column)\n",
    "\n",
    "    def format_data(self, column):\n",
    "        \"\"\"Format a column according to the learned format.\n",
    "\n",
    "        Args:\n",
    "            column (pd.Series):\n",
    "                Data to format.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray:\n",
    "                containing the formatted data.\n",
    "        \"\"\"\n",
    "        column = column.copy().to_numpy()\n",
    "        if self.enforce_min_max_values:\n",
    "            column = column.clip(self._min_value, self._max_value)\n",
    "        elif self.computer_representation != 'Float':\n",
    "            min_bound, max_bound = INTEGER_BOUNDS[self.computer_representation]\n",
    "            column = column.clip(min_bound, max_bound)\n",
    "\n",
    "        is_integer = np.dtype(self._dtype).kind == 'i'\n",
    "        if self.enforce_rounding and self._rounding_digits is not None:\n",
    "            column = column.round(self._rounding_digits)\n",
    "        elif is_integer:\n",
    "            column = column.round(0)\n",
    "\n",
    "        if pd.isna(column).any() and is_integer:\n",
    "            return column\n",
    "\n",
    "        return column.astype(self._dtype)\n",
    "def get_datetime_format(value):\n",
    "    \"\"\"Get the ``strftime`` format for a given ``value``.\n",
    "\n",
    "    This function returns the ``strftime`` format of a given ``value`` when possible.\n",
    "    If the ``_guess_datetime_format_for_array`` from ``pandas.core.tools.datetimes`` is\n",
    "    able to detect the ``strftime`` it will return it as a ``string`` if not, a ``None``\n",
    "    will be returned.\n",
    "\n",
    "    Args:\n",
    "        value (pandas.Series, np.ndarray, list, or str):\n",
    "            Input to attempt detecting the format.\n",
    "\n",
    "    Return:\n",
    "        String representing the datetime format in ``strftime`` format or ``None`` if not detected.\n",
    "    \"\"\"\n",
    "    if not isinstance(value, pd.Series):\n",
    "        value = pd.Series(value)\n",
    "\n",
    "    value = value[~value.isna()]\n",
    "    value = value.astype(str).to_numpy()\n",
    "\n",
    "    return _guess_datetime_format_for_array(value)\n",
    "class DatetimeFormatter:\n",
    "    \"\"\"Formatter for datetime data.\n",
    "\n",
    "    Args:\n",
    "        datetime_format (str):\n",
    "            The strftime to use for parsing time. For more information, see\n",
    "            https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n",
    "            If ``None`` it will attempt to learn it by itself. Defaults to ``None``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datetime_format=None):\n",
    "        self.datetime_format = datetime_format\n",
    "\n",
    "    def learn_format(self, column):\n",
    "        \"\"\"Learn the format of a column.\n",
    "\n",
    "        Args:\n",
    "            column (pandas.Series):\n",
    "                Data to learn the format.\n",
    "        \"\"\"\n",
    "        self._dtype = column.dtype\n",
    "        if self.datetime_format is None:\n",
    "            self.datetime_format = get_datetime_format(column)\n",
    "\n",
    "    def format_data(self, column):\n",
    "        \"\"\"Format a column according to the learned format.\n",
    "\n",
    "        Args:\n",
    "            column (pd.Series):\n",
    "                Data to format.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray:\n",
    "                containing the formatted data.\n",
    "        \"\"\"\n",
    "        if self.datetime_format:\n",
    "            try:\n",
    "                datetime_column = pd.to_datetime(column, format=self.datetime_format)\n",
    "                column = datetime_column.dt.strftime(self.datetime_format)\n",
    "            except ValueError:\n",
    "                column = pd.to_datetime(column).dt.strftime(self.datetime_format)\n",
    "\n",
    "        return column.astype(self._dtype)\n",
    "TMP_FILE_NAME = '.sample.csv.temp'\n",
    "DISABLE_TMP_FILE = 'disable'\n",
    "IGNORED_DICT_KEYS = ['fitted', 'distribution', 'type']\n",
    "def validate_file_path(output_file_path):\n",
    "    \"\"\"Validate the user-passed output file arg, and create the file.\"\"\"\n",
    "    output_path = None\n",
    "    if output_file_path == DISABLE_TMP_FILE:\n",
    "        # Temporary way of disabling the output file feature, used by HMA1.\n",
    "        return output_path\n",
    "\n",
    "    elif output_file_path:\n",
    "        output_path = os.path.abspath(output_file_path)\n",
    "        if os.path.exists(output_path):\n",
    "            raise AssertionError(f'{output_path} already exists.')\n",
    "\n",
    "    else:\n",
    "        if os.path.exists(TMP_FILE_NAME):\n",
    "            os.remove(TMP_FILE_NAME)\n",
    "\n",
    "        output_path = TMP_FILE_NAME\n",
    "\n",
    "    # Create the file.\n",
    "    with open(output_path, 'w+'):\n",
    "        pass\n",
    "\n",
    "    return output_path\n",
    "def groupby_list(list_to_check):\n",
    "    \"\"\"Return the first element of the list if the length is 1 else the entire list.\"\"\"\n",
    "    return list_to_check[0] if len(list_to_check) == 1 else list_to_check\n",
    "def check_num_rows(num_rows, expected_num_rows, is_reject_sampling, max_tries_per_batch):\n",
    "    \"\"\"Check the number of sampled rows against the expected number of rows.\n",
    "\n",
    "    If the number of sampled rows is zero, throw a ValueError.\n",
    "    If the number of sampled rows is less than the expected number of rows,\n",
    "    raise a warning.\n",
    "\n",
    "    Args:\n",
    "        num_rows (int):\n",
    "            The number of sampled rows.\n",
    "        expected_num_rows (int):\n",
    "            The expected number of rows.\n",
    "        is_reject_sampling (bool):\n",
    "            If reject sampling is used or not.\n",
    "        max_tries_per_batch (int):\n",
    "            Number of times to retry sampling until the batch size is met.\n",
    "\n",
    "    Side Effects:\n",
    "        ValueError or warning.\n",
    "    \"\"\"\n",
    "    if num_rows < expected_num_rows:\n",
    "        if num_rows == 0:\n",
    "            user_msg = ('Unable to sample any rows for the given conditions. ')\n",
    "            if is_reject_sampling:\n",
    "                user_msg = user_msg + (\n",
    "                    f'Try increasing `max_tries_per_batch` (currently: {max_tries_per_batch}). '\n",
    "                    'Note that increasing this value will also increase the sampling time.'\n",
    "                )\n",
    "            else:\n",
    "                user_msg = user_msg + (\n",
    "                    'This may be because the provided values are out-of-bounds in the '\n",
    "                    'current model. \\nPlease try again with a different set of values.'\n",
    "                )\n",
    "            raise ValueError(user_msg)\n",
    "\n",
    "        else:\n",
    "            # This case should only happen with reject sampling.\n",
    "            user_msg = (\n",
    "                f'Only able to sample {num_rows} rows for the given conditions. '\n",
    "                'To sample more rows, try increasing `max_tries_per_batch` '\n",
    "                f'(currently: {max_tries_per_batch}). Note that increasing this value '\n",
    "                'will also increase the sampling time.'\n",
    "            )\n",
    "            warnings.warn(user_msg)\n",
    "def detect_discrete_columns(metadata, data, transformers):\n",
    "    \"\"\"Detect the discrete columns in a dataset.\n",
    "\n",
    "    Because the metadata doesn't necessarily match the data (we only preprocess the data,\n",
    "    while the metadata stays static), this method tries to infer whether the data is\n",
    "    discrete.\n",
    "\n",
    "    Args:\n",
    "        metadata (sdv.metadata.SingleTableMetadata):\n",
    "            Metadata that belongs to the given ``data``.\n",
    "\n",
    "        data (pandas.DataFrame):\n",
    "            ``pandas.DataFrame`` that matches the ``metadata``.\n",
    "\n",
    "        transformers (dict[str: rdt.transformers.BaseTransformer]):\n",
    "            A dictionary mapping between column names and the transformers assigned\n",
    "            for it.\n",
    "\n",
    "    Returns:\n",
    "        discrete_columns (list):\n",
    "            A list of discrete columns to be used with some of ``sdv`` synthesizers.\n",
    "    \"\"\"\n",
    "    discrete_columns = []\n",
    "    for column in data.columns:\n",
    "        if column in metadata.columns:\n",
    "            sdtype = metadata.columns[column]['sdtype']\n",
    "            # Numerical and datetime columns never get preprocessed into categorical ones\n",
    "            if sdtype in ['numerical', 'datetime']:\n",
    "                continue\n",
    "\n",
    "            elif sdtype in ['categorical', 'boolean']:\n",
    "                transformer = transformers.get(column)\n",
    "                if transformer and transformer.get_output_sdtypes().get(column) == 'float':\n",
    "                    continue\n",
    "\n",
    "                discrete_columns.append(column)\n",
    "                continue\n",
    "\n",
    "        # Logic to detect columns produced by transformers outside of the metadata scope\n",
    "        # or columns created by constraints.\n",
    "        column_data = data[column].dropna()\n",
    "\n",
    "        # Ignore columns with only nans and empty datasets\n",
    "        if column_data.empty:\n",
    "            continue\n",
    "\n",
    "        # Non-integer floats and integers with too many unique values are not categorical\n",
    "        try:\n",
    "            column_data = column_data.astype('float')\n",
    "            is_int = column_data.equals(column_data.round())\n",
    "            is_float = not is_int\n",
    "            num_values = len(column_data)\n",
    "            num_categories = column_data.nunique()\n",
    "            threshold = max(10, num_values * .1)\n",
    "            has_many_categories = num_categories > threshold\n",
    "            if is_float or (is_int and has_many_categories):\n",
    "                continue\n",
    "\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "        # Everything else is presumed categorical\n",
    "        discrete_columns.append(column)\n",
    "\n",
    "    return discrete_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../original data/adult/train.csv') # load the original training data as data_1\n",
    "test = pd.read_csv('../original data/adult/train.csv') # load the original test data as test\n",
    "result_1 = pd.read_csv('../synthetic data/adult/result_CTGAN.csv') # load the CTGAN augmented data as result_1\n",
    "result_2 = pd.read_csv('../synthetic data/adult/result_BCTGAN.csv') # load the BCTGAN augmented data as result_2\n",
    "result_3 = pd.read_csv('../synthetic data/adult/result_Cholesky.csv') # load the Cholesky generated data as result_3\n",
    "result_4 = pd.read_csv('../synthetic data/adult/result_BPCA.csv') # load the BPCA generated data as result_4\n",
    "\n",
    "with open('../original data/adult/metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)  # load the metadata object\n",
    "metadata_obj = SingleTableMetadata.load_from_dict(metadata)\n",
    "label = 'label' # label indicates the column to be classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data to numerical form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real = train.drop(label, axis=1)\n",
    "y_real = train[label]\n",
    "processor = DataProcessor(metadata_obj)\n",
    "processor.fit(X_real)\n",
    "num_X_real = processor.transform(X_real)\n",
    "label_encoder = LabelEncoder()\n",
    "num_y_real = label_encoder.fit_transform(y_real)\n",
    "column_label = pd.DataFrame(num_y_real, columns=[label])\n",
    "real = pd.concat([num_X_real, column_label], axis=1, ignore_index=True)\n",
    "\n",
    "X_syn_1 = result_1.drop(label, axis=1)\n",
    "y_syn_1 = result_1[label]\n",
    "num_X_syn_1 = processor.transform(X_syn_1).reset_index(drop=True)\n",
    "num_y_syn_1 = label_encoder.fit_transform(y_syn_1)\n",
    "column_label_1 = pd.DataFrame(num_y_syn_1, columns=[label])\n",
    "syn_1 = pd.concat([num_X_syn_1, column_label_1], axis=1)\n",
    "\n",
    "X_syn_2 = result_2.drop(label, axis=1)\n",
    "y_syn_2 = result_2[label]\n",
    "num_X_syn_2 = processor.transform(X_syn_2).reset_index(drop=True)\n",
    "num_y_syn_2 = label_encoder.fit_transform(y_syn_2)\n",
    "column_label_2 = pd.DataFrame(num_y_syn_2, columns=[label])\n",
    "syn_2 = pd.concat([num_X_syn_2, column_label_2], axis=1, ignore_index=True)\n",
    "\n",
    "X_syn_3 = result_3.drop(label, axis=1)\n",
    "y_syn_3 = result_3[label]\n",
    "num_X_syn_3 = processor.transform(X_syn_3).reset_index(drop=True)\n",
    "num_y_syn_3 = label_encoder.fit_transform(y_syn_3)\n",
    "column_label_3 = pd.DataFrame(num_y_syn_3, columns=[label]).reset_index(drop=True)\n",
    "syn_3 = pd.concat([num_X_syn_3, column_label_3], axis=1)\n",
    "\n",
    "X_syn_4 = result_4.drop(label, axis=1)\n",
    "y_syn_4 = result_4[label]\n",
    "num_X_syn_4 = processor.transform(X_syn_4).reset_index(drop=True)\n",
    "num_y_syn_4 = label_encoder.fit_transform(y_syn_4)\n",
    "column_label_4 = pd.DataFrame(num_y_syn_4, columns=[label]).reset_index(drop=True)\n",
    "syn_4 = pd.concat([num_X_syn_4, column_label_4], axis=1)\n",
    "\n",
    "X_test = test.drop(label, axis=1)\n",
    "y_test = test[label]\n",
    "num_X_test = processor.transform(X_test)\n",
    "num_y_test = label_encoder.fit_transform(y_test)\n",
    "column_label_test = pd.DataFrame(num_y_test, columns=[label])\n",
    "test_1 = pd.concat([num_X_test, column_label_test], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the distance between original data and synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate DCR distance\n",
    "def calculate_distance(dataset_a, dataset_b):\n",
    "    distances = 0\n",
    "    rows = dataset_a.shape[0]\n",
    "    for _, row_a in dataset_a.iterrows():\n",
    "        row_a_np = row_a.to_numpy().reshape(1, -1)\n",
    "        euclidean_distances = cdist(row_a_np, dataset_b, 'euclidean')\n",
    "        min_distance_index = np.argmin(euclidean_distances)\n",
    "        min_distance = euclidean_distances[0, min_distance_index]\n",
    "        distances += min_distance\n",
    "    average_distance = distances/rows\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = calculate_distance(real, syn_1)\n",
    "print(dist1)\n",
    "dist2 = calculate_distance(real, syn_2)\n",
    "print(dist2)\n",
    "dist3 = calculate_distance(real, syn_3)\n",
    "print(dist3)\n",
    "dist4 = calculate_distance(real, syn_4)\n",
    "print(dist4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the F1 score of svm and rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(num_X_real, num_y_real)\n",
    "predictions = svm_classifier.predict(num_X_test)\n",
    "\n",
    "accuracy = accuracy_score(num_y_test, predictions)\n",
    "classification_report_result = classification_report(num_y_test, predictions)\n",
    "print(\"Original\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)\n",
    "\n",
    "svm_classifier1 = SVC(kernel='linear')\n",
    "svm_classifier1.fit(num_X_syn_1, num_y_syn_1)\n",
    "predictions1 = svm_classifier1.predict(num_X_test)\n",
    "\n",
    "accuracy = accuracy_score(num_y_test, predictions1)\n",
    "classification_report_result = classification_report(num_y_test, predictions1)\n",
    "print(\"CTGAN\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)\n",
    "\n",
    "svm_classifier2 = SVC(kernel='linear')\n",
    "svm_classifier2.fit(num_X_syn_2, num_y_syn_2)\n",
    "predictions2 = svm_classifier2.predict(num_X_test)\n",
    "\n",
    "accuracy = accuracy_score(num_y_test, predictions2)\n",
    "classification_report_result = classification_report(num_y_test, predictions2)\n",
    "print(\"BCTGAN\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)\n",
    "\n",
    "int_cols = [column_name for column_name, properties in metadata[\"columns\"].items() if properties.get(\"computer_representation\") == \"Int64\"]\n",
    "result_3[int_cols] = result_3[int_cols].round().astype(int)\n",
    "result_4[int_cols] = result_4[int_cols].round().astype(int)\n",
    "\n",
    "svm_classifier3 = SVC(kernel='linear')\n",
    "svm_classifier3.fit(num_X_syn_3, num_y_syn_3)\n",
    "predictions3 = svm_classifier3.predict(num_X_test)\n",
    "\n",
    "accuracy = accuracy_score(num_y_test, predictions3)\n",
    "classification_report_result = classification_report(num_y_test, predictions3)\n",
    "print(\"balanced Cholesky\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)\n",
    "\n",
    "svm_classifier4 = SVC(kernel='linear')\n",
    "svm_classifier4.fit(num_X_syn_4, num_y_syn_4)\n",
    "predictions4 = svm_classifier4.predict(num_X_test)\n",
    "\n",
    "accuracy = accuracy_score(num_y_test, predictions4)\n",
    "classification_report_result = classification_report(num_y_test, predictions4)\n",
    "print(\"balanced PCA\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(num_X_real, num_y_real)\n",
    "y_pred = rf_classifier.predict(num_X_test)\n",
    "accuracy = accuracy_score(num_y_test, y_pred)\n",
    "classification_report_result = classification_report(num_y_test, y_pred)\n",
    "print(\"Original\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)\n",
    "\n",
    "rf_classifier1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier1.fit(num_X_syn_1, num_y_syn_1)\n",
    "y_pred1 = rf_classifier1.predict(num_X_test)\n",
    "accuracy = accuracy_score(num_y_test, y_pred1)\n",
    "classification_report_result = classification_report(num_y_test, y_pred1)\n",
    "print(\"BCTGAN\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)\n",
    "\n",
    "rf_classifier2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier2.fit(num_X_syn_2, num_y_syn_2)\n",
    "y_pred2 = rf_classifier2.predict(num_X_test)\n",
    "accuracy = accuracy_score(num_y_test, y_pred2)\n",
    "classification_report_result = classification_report(num_y_test, y_pred2)\n",
    "print(\"CTGAN\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)\n",
    "\n",
    "rf_classifier3 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier3.fit(num_X_syn_3, num_y_syn_3)\n",
    "y_pred3 = rf_classifier3.predict(num_X_test)\n",
    "accuracy = accuracy_score(num_y_test, y_pred3)\n",
    "classification_report_result = classification_report(num_y_test, y_pred3)\n",
    "print(\"balanced Cholesky\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)\n",
    "\n",
    "rf_classifier4 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier4.fit(num_X_syn_4, num_y_syn_4)\n",
    "y_pred4 = rf_classifier4.predict(num_X_test)\n",
    "accuracy = accuracy_score(num_y_test, y_pred4)\n",
    "classification_report_result = classification_report(num_y_test, y_pred4)\n",
    "print(\"balanced PCA\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
